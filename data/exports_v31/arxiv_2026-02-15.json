{
  "database": "arXiv",
  "search_date": "2026-02-15",
  "queries": {
    "primary": "(ti:\"single cell\" OR ti:scRNA OR ti:\"gene expression\" OR ti:transcriptomics OR ti:genomics OR abs:\"single-cell\" OR abs:\"RNA-seq\" OR abs:\"RNA seq\" OR abs:\"spatial transcriptomics\" OR abs:\"multi-omics\" OR abs:\"multi omics\") AND (ti:\"language model\" OR ti:GPT OR ti:LLM OR ti:CLIP OR ti:multimodal OR abs:\"language model\" OR abs:\"large language model\" OR abs:\"natural language\" OR abs:\"cross-modal\" OR abs:\"multi-modal\" OR abs:tokeniz OR abs:prompt) AND (abs:\"foundation model\" OR abs:pretrain OR abs:\"self-supervised\" OR abs:transformer OR abs:generative OR abs:decoder OR abs:autoregressive OR abs:\"transfer learning\")",
    "cell_language": "abs:\"foundation model\" AND abs:\"single cell\" AND abs:\"language model\"",
    "model_names": "(ti:scGPT OR ti:tGPT OR ti:LangCell OR ti:ChatCell OR ti:GenePT OR ti:GeneGPT OR ti:CellPLM OR ti:Nicheformer OR ti:EpiAgent)",
    "bio_llm": "abs:\"gene expression\" AND abs:LLM AND (abs:generative OR abs:decoder OR abs:autoregressive)"
  },
  "filters": "categories: cat:cs.LG OR cat:cs.AI OR cat:cs.CL OR cat:q-bio.GN OR cat:q-bio.QM OR cat:stat.ML, date: submittedDate:[20180101 TO 20260228]",
  "total_unique_results": 187,
  "records": [
    {
      "arxiv_id": "2602.11609v1",
      "doi": "",
      "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
      "abstract": "We present scPilot, the first systematic framework to practice omics-native reasoning: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence.   To measure progress, we release scBench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that iterative omics-native reasoning lifts average accuracy by 11% for cell-type annotation and Gemini-2.5-Pro cuts trajectory graph-edit distance by 30% versus one-shot prompting, while generating transparent reasoning traces explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses.   Code, data, and package are available at https://github.com/maitrix-org/scPilot",
      "authors": "Yiming Gao; Zhen Wang; Jefferson Chen; Mark Antkowiak; Mengzhou Hu; JungHo Kong; Dexter Pratt; Jieyuan Liu; Enze Ma; Zhiting Hu; Eric P. Xing",
      "date": "2026-02-12",
      "year": "2026",
      "categories": [
        "cs.AI",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2602.02014v1",
      "doi": "",
      "title": "Rethinking Genomic Modeling Through Optical Character Recognition",
      "abstract": "Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \\emph{visual DNA encoder} and a \\emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\\times$ fewer effective tokens, and surpasses models with up to $985\\times$ more activated parameters while tuning only 256k \\emph{trainable} parameters.",
      "authors": "Hongxin Xiang; Pengsen Ma; Yunkang Cao; Di Yu; Haowen Chen; Xinyu Yang; Xiangxiang Zeng",
      "date": "2026-02-02",
      "year": "2026",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2602.01772v1",
      "doi": "",
      "title": "DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics",
      "abstract": "Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.",
      "authors": "Yucheng Liao; Han Wen; Weinan E; Weijie Zhang",
      "date": "2026-02-02",
      "year": "2026",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2601.16175v2",
      "doi": "",
      "title": "Learning to Discover at Test Time",
      "abstract": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
      "authors": "Mert Yuksekgonul; Daniel Koceja; Xinhao Li; Federico Bianchi; Jed McCaleb; Xiaolong Wang; Jan Kautz; Yejin Choi; James Zou; Carlos Guestrin; Yu Sun",
      "date": "2026-01-22",
      "year": "2026",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2601.15392v1",
      "doi": "https://doi.org/10.1007/978-3-032-11317-7_33",
      "title": "GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology Images and Clinical Descriptions for Gene Expression Profile Generation",
      "abstract": "Biomedical research increasingly relies on integrating diverse data modalities, including gene expression profiles, medical images, and clinical metadata. While medical images and clinical metadata are routinely collected in clinical practice, gene expression data presents unique challenges for widespread research use, mainly due to stringent privacy regulations and costly laboratory experiments. To address these limitations, we present GeMM-GAN, a novel Generative Adversarial Network conditioned on histopathology tissue slides and clinical metadata, designed to synthesize realistic gene expression profiles. GeMM-GAN combines a Transformer Encoder for image patches with a final Cross Attention mechanism between patches and text tokens, producing a conditioning vector to guide a generative model in generating biologically coherent gene expression profiles. We evaluate our approach on the TCGA dataset and demonstrate that our framework outperforms standard generative models and generates more realistic and functionally meaningful gene expression profiles, improving by more than 11\\% the accuracy on downstream disease type prediction compared to current state-of-the-art generative models. Code will be available at: https://github.com/francescapia/GeMM-GAN",
      "authors": "Francesca Pia Panaccione; Carlo Sgaravatti; Pietro Pinoli",
      "date": "2026-01-21",
      "year": "2026",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2601.05648v1",
      "doi": "",
      "title": "Open World Knowledge Aided Single-Cell Foundation Model with Robust Cross-Modal Cell-Language Pre-training",
      "abstract": "Recent advancements in single-cell multi-omics, particularly RNA-seq, have provided profound insights into cellular heterogeneity and gene regulation. While pre-trained language model (PLM) paradigm based single-cell foundation models have shown promise, they remain constrained by insufficient integration of in-depth individual profiles and neglecting the influence of noise within multi-modal data. To address both issues, we propose an Open-world Language Knowledge-Aided Robust Single-Cell Foundation Model (OKR-CELL). It is built based on a cross-modal Cell-Language pre-training framework, which comprises two key innovations: (1) leveraging Large Language Models (LLMs) based workflow with retrieval-augmented generation (RAG) enriches cell textual descriptions using open-world knowledge; (2) devising a Cross-modal Robust Alignment (CRA) objective that incorporates sample reliability assessment, curriculum learning, and coupled momentum contrastive learning to strengthen the model's resistance to noisy data. After pretraining on 32M cell-text pairs, OKR-CELL obtains cutting-edge results across 6 evaluation tasks. Beyond standard benchmarks such as cell clustering, cell-type annotation, batch-effect correction, and few-shot annotation, the model also demonstrates superior performance in broader multi-modal applications, including zero-shot cell-type annotation and bidirectional cell-text retrieval.",
      "authors": "Haoran Wang; Xuanyi Zhang; Shuangsang Fang; Longke Ran; Ziqing Deng; Yong Zhang; Yuxiang Li; Shaoshuai Li",
      "date": "2026-01-09",
      "year": "2026",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2601.03295v1",
      "doi": "",
      "title": "MetagenBERT: a Transformer-based Architecture using Foundational genomic Large Language Models for novel Metagenome Representation",
      "abstract": "Metagenomic disease prediction commonly relies on species abundance tables derived from large, incomplete reference catalogs, constraining resolution and discarding valuable information contained in DNA reads. To overcome these limitations, we introduce MetagenBERT, a Transformer based framework that produces end to end metagenome embeddings directly from raw DNA sequences, without taxonomic or functional annotations. Reads are embedded using foundational genomic language models (DNABERT2 and the microbiome specialized DNABERTMS), then aggregated through a scalable clustering strategy based on FAISS accelerated KMeans. Each metagenome is represented as a cluster abundance vector summarizing the distribution of its embedded reads. We evaluate this approach on five benchmark gut microbiome datasets (Cirrhosis, T2D, Obesity, IBD, CRC). MetagenBERT achieves competitive or superior AUC performance relative to species abundance baselines across most tasks. Concatenating both representations further improves prediction, demonstrating complementarity between taxonomic and embedding derived signals. Clustering remains robust when applied to as little as 10% of reads, highlighting substantial redundancy in metagenomes and enabling major computational gains. We additionally introduce MetagenBERT Glob Mcardis, a cross cohort variant trained on the large, phenotypically diverse MetaCardis cohort and transferred to other datasets, retaining predictive signal including for unseen phenotypes, indicating the feasibility of a foundation model for metagenome representation. Robustness analyses (PERMANOVA, PERMDISP, entropy) show consistent separation of different states across subsamples. Overall, MetagenBERT provides a scalable, annotation free representation of metagenomes pointing toward future phenotype aware generalization across heterogeneous cohorts and sequencing technologies.",
      "authors": "Gaspar Roy; Eugeni Belda; Baptiste Hennecart; Yann Chevaleyre; Edi Prifti; Jean-Daniel Zucker",
      "date": "2026-01-05",
      "year": "2026",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2512.24733v1",
      "doi": "",
      "title": "BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics Pathway Mechanism Elucidation from Scientific Literature",
      "abstract": "Multi-omics studies often rely on pathway enrichment to interpret heterogeneous molecular changes, but pathway enrichment (PE)-based workflows inherit structural limitations of pathway resources, including curation lag, functional redundancy, and limited sensitivity to molecular states and interventions. Although recent work has explored using large language models (LLMs) to improve PE-based interpretation, the lack of a standardized benchmark for end-to-end multi-omics pathway mechanism elucidation has largely confined evaluation to small, manually curated datasets or ad hoc case studies, hindering reproducible progress. To address this issue, we introduce BIOME-Bench, constructed via a rigorous four-stage workflow, to evaluate two core capabilities of LLMs in multi-omics analysis: Biomolecular Interaction Inference and end-to-end Multi-Omics Pathway Mechanism Elucidation. We develop evaluation protocols for both tasks and conduct comprehensive experiments across multiple strong contemporary models. Experimental results demonstrate that existing models still exhibit substantial deficiencies in multi-omics analysis, struggling to reliably distinguish fine-grained biomolecular relation types and to generate faithful, robust pathway-level mechanistic explanations.",
      "authors": "Sibo Wei; Peng Chen; Lifeng Dong; Yin Luo; Lei Wang; Peng Zhang; Wenpeng Lu; Jianbin Guo; Hongjun Yang; Dajun Zeng",
      "date": "2025-12-31",
      "year": "2025",
      "categories": [
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2512.21907v2",
      "doi": "",
      "title": "SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?",
      "abstract": "Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.",
      "authors": "Kenny Workman; Zhen Yang; Harihara Muralidharan; Hannah Le",
      "date": "2025-12-26",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2512.17126v2",
      "doi": "",
      "title": "DNAMotifTokenizer: Towards Biologically Informed Tokenization of Genomic Sequences",
      "abstract": "DNA language models have advanced genomics, but their downstream performance varies widely due to differences in tokenization, pretraining data, and architecture. We argue that a major bottleneck lies in tokenizing sparse and unevenly distributed DNA sequence motifs, which are critical for accurate and interpretable models. To investigate, we systematically benchmark k-mer and Byte-Pair Encoding (BPE) tokenizers under controlled pretraining budget, evaluating across multiple downstream tasks from five datasets. We find that tokenizer choice induces task-specific trade-offs, and that vocabulary size and tokenizer training data strongly influence the biological knowledge captured. Notably, BPE tokenizers achieve strong performance when trained on smaller but biologically significant data. Building on these insights, we introduce DNAMotifTokenizer, which directly incorporates domain knowledge of DNA sequence motifs into the tokenization process. DNAMotifTokenizer consistently outperforms BPE across diverse benchmarks, demonstrating that knowledge-infused tokenization is crucial for learning powerful, interpretable, and generalizable genomic representations.",
      "authors": "Xiaoxiao Zhou; Zihan Wang; Jingbo Shang; Yang E. Li",
      "date": "2025-12-18",
      "year": "2025",
      "categories": [
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2512.12888v1",
      "doi": "",
      "title": "Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence",
      "abstract": "Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.",
      "authors": "David Dang; Stuart Love; Meena Salib; Quynh Dang; Samuel Rothfarb; Mysk Alnatour; Andrew Salij; Hou-Tong Chen; Ho Wai;  Lee; Wilton J. M. Kort-Kamp",
      "date": "2025-12-15",
      "year": "2025",
      "categories": [
        "physics.optics",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2512.09964v1",
      "doi": "",
      "title": "Development of an Agentic AI Model for NGS Downstream Analysis Targeting Researchers with Limited Biological Background",
      "abstract": "Next-Generation Sequencing (NGS) has become a cornerstone of genomic research, yet the complexity of downstream analysis-ranging from differential expression gene (DEG) identification to biological interpretations-remains a significant barrier for researchers lacking specialized computational and biological expertise. While recent studies have introduced AI agents for RNA-seq analysis, most focus on general workflows without offering tailored interpretations or guidance for novices. To address this gap, we developed an Agentic AI model designed to automate NGS downstream analysis, provide literature-backed interpretations, and autonomously recommend advanced analytical methods. Built on the Llama 3 70B Large Language Model (LLM) and a Retrieval-Augmented Generation (RAG) framework, the model is deployed as an interactive Streamlit web application. The system integrates standard bioinformatics tools (Biopython, GSEApy, gProfiler) to execute core analyses, including DEG identification, clustering, and pathway enrichment. Uniquely, the agent utilizes RAG to query PubMed via Entrez, synthesizing biological insights and validating hypotheses with current literature. In a case study using cancer-related dataset, the model successfully identified significant DEGs, visualized clinical correlations, and derived evidence-based insights (e.g., linking BRAF mutations to prognosis), subsequently executing advanced survival modeling upon user selection. This framework democratizes bioinformatics by enabling researchers with limited backgrounds to seamlessly transition from basic data processing to advanced hypothesis testing and validation.",
      "authors": "Donghyeon Lee; Dongseok Kim; Seokhwan Ko; Seo-Young Park; Junghwan Cho",
      "date": "2025-12-10",
      "year": "2025",
      "categories": [
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2512.00306v1",
      "doi": "",
      "title": "VCWorld: A Biological World Model for Virtual Cell Simulation",
      "abstract": "Virtual cell modeling aims to predict cellular responses to perturbations. Existing virtual cell models rely heavily on large-scale single-cell datasets, learning explicit mappings between gene expression and perturbations. Although recent models attempt to incorporate multi-source biological information, their generalization remains constrained by data quality, coverage, and batch effects. More critically, these models often function as black boxes, offering predictions without interpretability or consistency with biological principles, which undermines their credibility in scientific research. To address these challenges, we present VCWorld, a cell-level white-box simulator that integrates structured biological knowledge with the iterative reasoning capabilities of large language models to instantiate a biological world model. VCWorld operates in a data-efficient manner to reproduce perturbation-induced signaling cascades and generates interpretable, stepwise predictions alongside explicit mechanistic hypotheses. In drug perturbation benchmarks, VCWorld achieves state-of-the-art predictive performance, and the inferred mechanistic pathways are consistent with publicly available biological evidence.",
      "authors": "Zhijian Wei; Runze Ma; Zichen Wang; Zhongmin Li; Shuotong Song; Shuangjia Zheng",
      "date": "2025-11-29",
      "year": "2025",
      "categories": [
        "q-bio.CB",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.21075v1",
      "doi": "",
      "title": "Aligning LLMs with Biomedical Knowledge using Balanced Fine-Tuning",
      "abstract": "Effective post-training is essential to align Large Language Models (LLMs) with specialized biomedical knowledge to accelerate life science research. However, current approaches face significant limitations. First, biomedical reasoning involves intricate mechanisms often represented by sparse textual data. Standard Supervised Fine-Tuning (SFT) tends to overfit to surface-level instruction patterns without effectively internalizing this fragmented scientific knowledge. Second, Reinforcement Learning (RL) is impractical for this domain, as defining meaningful rewards often necessitates prohibitive experimental validation (e.g., wet-lab verification of drug responses), rendering real-time feedback unfeasible. We propose Balanced Fine-Tuning (BFT), an efficient post-training method designed to learn complex reasoning from sparse data without external reward signals. BFT operates through a two-layer weighting mechanism: 1. At the token level, it scales loss via prediction probabilities to stabilize gradients and prevent overfitting; 2. At the sample level, it uses \"minimum group confidence\" to adaptively enhance the learning of hard samples. Experiments demonstrate that BFT significantly outperforms SFT. In medical tasks, it enables LLMs to acquire knowledge that SFT misses. In biological tasks, BFT-based LLMs surpass GeneAgent (an accurate agent for biology analysis) in biological process reasoning. Moreover, the text embeddings generated by BFT can be directly applied to downstream tasks, such as gene interaction and single-cell perturbation response prediction. These results indicate that BFT facilitates broad applications of LLMs in biomedical research.",
      "authors": "Zhenchao Tang; Fang Wang; Haohuai He; Jiale Zhou; Tianxu Lv; Jun Zhu; Shouzhi Chen; Minghao Yang; Yu Wang; Jiayang Wu; Yidong Song; Jianhua Yao",
      "date": "2025-11-26",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.20382v2",
      "doi": "",
      "title": "MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers",
      "abstract": "Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with Scrublet, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.",
      "authors": "Audrey Pei-Hsuan Chen",
      "date": "2025-11-25",
      "year": "2025",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.19299v1",
      "doi": "",
      "title": "Open-weight genome language model safeguards: Assessing robustness via adversarial fine-tuning",
      "abstract": "Novel deep learning architectures are increasingly being applied to biological data, including genetic sequences. These models, referred to as genomic language mod- els (gLMs), have demonstrated impressive predictive and generative capabilities, raising concerns that such models may also enable misuse, for instance via the generation of genomes for human-infecting viruses. These concerns have catalyzed calls for risk mitigation measures. The de facto mitigation of choice is filtering of pretraining data (i.e., removing viral genomic sequences from training datasets) in order to limit gLM performance on virus-related tasks. However, it is not currently known how robust this approach is for securing open-source models that can be fine-tuned using sensitive pathogen data. Here, we evaluate a state-of-the-art gLM, Evo 2, and perform fine-tuning using sequences from 110 harmful human-infecting viruses to assess the rescue of misuse-relevant predictive capabilities. The fine- tuned model exhibited reduced perplexity on unseen viral sequences relative to 1) the pretrained model and 2) a version fine-tuned on bacteriophage sequences. The model fine-tuned on human-infecting viruses also identified immune escape variants from SARS-CoV-2 (achieving an AUROC of 0.6), despite having no expo- sure to SARS-CoV-2 sequences during fine-tuning. This work demonstrates that data exclusion might be circumvented by fine-tuning approaches that can, to some degree, rescue misuse-relevant capabilities of gLMs. We highlight the need for safety frameworks for gLMs and outline further work needed on evaluations and mitigation measures to enable the safe deployment of gLMs.",
      "authors": "James R. M. Black; Moritz S. Hanke; Aaron Maiwald; Tina Hernandez-Boussard; Oliver M. Crook; Jaspreet Pannu",
      "date": "2025-11-24",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.15061v1",
      "doi": "https://doi.org/10.1145/3767695.3769488",
      "title": "Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering",
      "abstract": "Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.   In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.   OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.",
      "authors": "Haodong Chen; Guido Zuccon; Teerapong Leelanupab",
      "date": "2025-11-19",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.IR",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.15728v1",
      "doi": "",
      "title": "The Future of Food: How Artificial Intelligence is Transforming Food Manufacturing",
      "abstract": "Artificial intelligence is accelerating a new era of food innovation, connecting data from farm to consumer to improve formulation, processing, and health outcomes. Recent advances in deep learning, natural language processing, and multi-omics integration make it possible to understand and optimize food systems with unprecedented depth. However, AI adoption across the food sector remains uneven due to heterogeneous datasets, limited model and system interoperability, and a persistent skills gap between data scientists and food domain experts. To address these challenges and advance responsible innovation, the AI Institute for Next Generation Food Systems (AIFS) convened the inaugural AI for Food Product Development Symposium at University of California, Davis, in October 2025. This white paper synthesizes insights from the symposium, organized around five domains where AI can have the greatest near-term impact: supply chain; formulation and processing; consumer insights and sensory prediction; nutrition and health; and education and workforce development. Across the areas, participants emphasized the importance of interoperable data standards, transparent and interpretable models, and cross-sector collaboration to accelerate the translation of AI research into practice. The discussions further highlighted the need for robust digital infrastructure, privacy-preserving data-sharing mechanisms, and interdisciplinary training pathways that integrate AI literacy with domain expertise. Collectively, the priorities outline a roadmap for integrating AI into food manufacturing in ways that enhance innovation, sustainability, and human well-being while ensuring that technological progress remains grounded in ethics, scientific rigor, and societal benefit.",
      "authors": "Xu Zhou; Ivor Prado; AIFPDS participants; Ilias Tagkopoulos",
      "date": "2025-11-17",
      "year": "2025",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.14806v1",
      "doi": "",
      "title": "MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging",
      "abstract": "Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.",
      "authors": "Siyuan Li; Kai Yu; Anna Wang; Zicheng Liu; Chang Yu; Jingbo Zhou; Qirong Yang; Yucheng Guo; Xiaoming Zhang; Stan Z. Li",
      "date": "2025-11-17",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.11380v1",
      "doi": "",
      "title": "When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering",
      "abstract": "Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to \"speak\" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.",
      "authors": "Jiangkai Long; Yanran Zhu; Chang Tang; Kun Sun; Yuanyuan Liu; Xuesong Yan",
      "date": "2025-11-14",
      "year": "2025",
      "categories": [
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.07503v3",
      "doi": "",
      "title": "Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models",
      "abstract": "The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.",
      "authors": "Asia Belfiore; Jonathan Passerat-Palmbach; Dmitrii Usynin",
      "date": "2025-11-10",
      "year": "2025",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.07481v1",
      "doi": "",
      "title": "Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data",
      "abstract": "This study investigates embedding reconstruction attacks in large language models (LLMs) applied to genomic sequences, with a specific focus on how fine-tuning affects vulnerability to these attacks. Building upon Pan et al.'s seminal work demonstrating that embeddings from pretrained language models can leak sensitive information, we conduct a comprehensive analysis using the HS3D genomic dataset to determine whether task-specific optimization strengthens or weakens privacy protections. Our research extends Pan et al.'s work in three significant dimensions. First, we apply their reconstruction attack pipeline to pretrained and fine-tuned model embeddings, addressing a critical gap in their methodology that did not specify embedding types. Second, we implement specialized tokenization mechanisms tailored specifically for DNA sequences, enhancing the model's ability to process genomic data, as these models are pretrained on natural language and not DNA. Third, we perform a detailed comparative analysis examining position-specific, nucleotide-type, and privacy changes between pretrained and fine-tuned embeddings. We assess embeddings vulnerabilities across different types and dimensions, providing deeper insights into how task adaptation shifts privacy risks throughout genomic sequences. Our findings show a clear distinction in reconstruction vulnerability between pretrained and fine-tuned embeddings. Notably, fine-tuning strengthens resistance to reconstruction attacks in multiple architectures -- XLNet (+19.8\\%), GPT-2 (+9.8\\%), and BERT (+7.8\\%) -- pointing to task-specific optimization as a potential privacy enhancement mechanism. These results highlight the need for advanced protective mechanisms for language models processing sensitive genomic data, while highlighting fine-tuning as a potential privacy-enhancing technique worth further exploration.",
      "authors": "Reem Al-Saidi; Erman Ayday; Ziad Kobti",
      "date": "2025-11-09",
      "year": "2025",
      "categories": [
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2511.02531v4",
      "doi": "",
      "title": "Causal Graph Neural Networks for Healthcare",
      "abstract": "Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.",
      "authors": "Munib Mesinovic; Max Buhlan; Tingting Zhu",
      "date": "2025-11-04",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2510.24987v2",
      "doi": "",
      "title": "scMRDR: A scalable and flexible framework for unpaired single-cell multi-omics data integration",
      "abstract": "Advances in single-cell sequencing have enabled high-resolution profiling of diverse molecular modalities, while integrating unpaired multi-omics single-cell data remains challenging. Existing approaches either rely on pair information or prior correspondences, or require computing a global pairwise coupling matrix, limiting their scalability and flexibility. In this paper, we introduce a scalable and flexible generative framework called single-cell Multi-omics Regularized Disentangled Representations (scMRDR) for unpaired multi-omics integration. Specifically, we disentangle each cell's latent representations into modality-shared and modality-specific components using a well-designed $β$-VAE architecture, which are augmented with isometric regularization to preserve intra-omics biological heterogeneity, adversarial objective to encourage cross-modal alignment, and masked reconstruction loss strategy to address the issue of missing features across modalities. Our method achieves excellent performance on benchmark datasets in terms of batch correction, modality alignment, and biological signal preservation. Crucially, it scales effectively to large-scale datasets and supports integration of more than two omics, offering a powerful and flexible solution for large-scale multi-omics data integration and downstream biological discovery.",
      "authors": "Jianle Sun; Chaoqi Liang; Ran Wei; Peng Zheng; Lei Bai; Wanli Ouyang; Hongliang Yan; Peng Ye",
      "date": "2025-10-28",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2510.23639v3",
      "doi": "",
      "title": "Integrating Genomics into Multimodal EHR Foundation Models",
      "abstract": "This paper introduces an innovative Electronic Health Record (EHR) foundation model that integrates Polygenic Risk Scores (PRS) as a foundational data modality, moving beyond traditional EHR-only approaches to build more holistic health profiles. Leveraging the extensive and diverse data from the All of Us (AoU) Research Program, this multimodal framework aims to learn complex relationships between clinical data and genetic predispositions. The methodology extends advancements in generative AI to the EHR foundation model space, enhancing predictive capabilities and interpretability. Evaluation on AoU data demonstrates the model's predictive value for the onset of various conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay between PRS and EHR data. The work also explores transfer learning for custom classification tasks, showcasing the architecture's versatility and efficiency. This approach is pivotal for unlocking new insights into disease prediction, proactive health management, risk stratification, and personalized treatment strategies, laying the groundwork for more personalized, equitable, and actionable real-world evidence generation in healthcare.",
      "authors": "Jonathan Amar; Edward Liu; Alessandra Breschi; Liangliang Zhang; Pouya Kheradpour; Sylvia Li; Lisa Soleymani Lehmann; Alessandro Giulianelli; Matt Edwards; Yugang Jia; David Nola; Raghav Mani; Pankaj Vats; Jesse Tetreault; T. J. Chen; Cory Y. McLean",
      "date": "2025-10-24",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2510.17064v3",
      "doi": "",
      "title": "A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation",
      "abstract": "Single-cell RNA sequencing has transformed our ability to identify diverse cell types and their transcriptomic signatures. However, annotating these signatures-especially those involving poorly characterized genes-remains a major challenge. Traditional methods, such as Gene Set Enrichment Analysis (GSEA), depend on well-curated annotations and often perform poorly in these contexts. Large Language Models (LLMs) offer a promising alternative but struggle to represent complex biological knowledge within structured ontologies. To address this, we present BRAINCELL-AID (BRAINCELL-AID: https://biodataai.uth.edu/BRAINCELL-AID), a novel multi-agent AI system that integrates free-text descriptions with ontology labels to enable more accurate and robust gene set annotation. By incorporating retrieval-augmented generation (RAG), we developed a robust agentic workflow that refines predictions using relevant PubMed literature, reducing hallucinations and enhancing interpretability. Using this workflow, we achieved correct annotations for 77% of mouse gene sets among their top predictions. Applying this approach, we annotated 5,322 brain cell clusters from the comprehensive mouse brain cell atlas generated by the BRAIN Initiative Cell Census Network, enabling novel insights into brain cell function by identifying region-specific gene co-expression patterns and inferring functional roles of gene ensembles. BRAINCELL-AID also identifies Basal Ganglia-related cell types with neurologically meaningful descriptions. Hence, we create a valuable resource to support community-driven cell type annotation.",
      "authors": "Rongbin Li; Wenbo Chen; Zhao Li; Rodrigo Munoz-Castaneda; Jinbo Li; Neha S. Maurya; Arnav Solanki; Huan He; Hanwen Xing; Meaghan Ramlakhan; Zachary Wise; Nelson Johansen; Zhuhao Wu; Hua Xu; Michael Hawrylycz; W. Jim Zheng",
      "date": "2025-10-20",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2510.12498v2",
      "doi": "",
      "title": "Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation",
      "abstract": "Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable, decision-relevant models of cell state from multimodal, multiscale measurements. Recent studies have introduced single-cell and spatial foundation models, improved cross-modality alignment, scaled perturbation atlases, and explored pathway-level readouts. Nevertheless, although held-out validation is standard practice, evaluations remain predominantly within single datasets and settings; evidence indicates that transport across laboratories and platforms is often limited, that some data splits are vulnerable to leakage and coverage bias, and that dose, time and combination effects are not yet systematically handled. Cross-scale coupling also remains constrained, as anchors linking molecular, cellular and tissue levels are sparse, and alignment to scientific or clinical readouts varies across studies. We propose a model-agnostic Cell-State Latent (CSL) perspective that organizes learning via an operator grammar: measurement, lift/project for cross-scale coupling, and intervention for dosing and scheduling. This view motivates a decision-aligned evaluation blueprint across modality, scale, context and intervention, and emphasizes function-space readouts such as pathway activity, spatial neighborhoods and clinically relevant endpoints. We recommend operator-aware data design, leakage-resistant partitions, and transparent calibration and reporting to enable reproducible, like-for-like comparisons.",
      "authors": "Chengpeng Hu; Calvin Yu-Chian Chen",
      "date": "2025-10-14",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2510.07793v3",
      "doi": "",
      "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology",
      "abstract": "Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.",
      "authors": "Sajib Acharjee Dip; Adrika Zafor; Bikash Kumar Paul; Uddip Acharjee Shuvo; Muhit Islam Emon; Xuan Wang; Liqing Zhang",
      "date": "2025-10-09",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2510.01428v1",
      "doi": "",
      "title": "BioVERSE: Representation Alignment of Biomedical Modalities to LLMs for Multi-Modal Reasoning",
      "abstract": "Recent advances in large language models (LLMs) and biomedical foundation models (BioFMs) have achieved strong results in biological text reasoning, molecular modeling, and single-cell analysis, yet they remain siloed in disjoint embedding spaces, limiting cross-modal reasoning. We present BIOVERSE (Biomedical Vector Embedding Realignment for Semantic Engagement), a two-stage approach that adapts pretrained BioFMs as modality encoders and aligns them with LLMs through lightweight, modality-specific projection layers. The approach first aligns each modality to a shared LLM space through independently trained projections, allowing them to interoperate naturally, and then applies standard instruction tuning with multi-modal data to bring them together for downstream reasoning. By unifying raw biomedical data with knowledge embedded in LLMs, the approach enables zero-shot annotation, cross-modal question answering, and interactive, explainable dialogue. Across tasks spanning cell-type annotation, molecular description, and protein function reasoning, compact BIOVERSE configurations surpass larger LLM baselines while enabling richer, generative outputs than existing BioFMs, establishing a foundation for principled multi-modal biomedical reasoning.",
      "authors": "Ching-Huei Tsou; Michal Ozery-Flato; Ella Barkan; Diwakar Mahajan; Ben Shapira",
      "date": "2025-10-01",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2510.00392v1",
      "doi": "",
      "title": "A Deep Learning Pipeline for Epilepsy Genomic Analysis Using GPT-2 XL and NVIDIA H100",
      "abstract": "Epilepsy is a chronic neurological condition characterized by recurrent seizures, with global prevalence estimated at 50 million people worldwide. While progress in high-throughput sequencing has allowed for broad-based transcriptomic profiling of brain tissues, the deciphering of these highly complex datasets remains one of the challenges. To address this issue, in this paper we propose a new analysis pipeline that integrates the power of deep learning strategies with GPU-acceleration computation for investigating Gene expression patterns in epilepsy. Specifically, our proposed approach employs GPT-2 XL, a transformer-based Large Language Model (LLM) with 1.5 billion parameters for genomic sequence analysis over the latest NVIDIA H100 Tensor Core GPUs based on Hopper architecture. Our proposed method enables efficient preprocessing of RNA sequence data, gene sequence encoding, and subsequent pattern identification. We conducted experiments on two epilepsy datasets including GEO accession GSE264537 and GSE275235. The obtained results reveal several significant transcriptomic modifications, including reduced hippocampal astrogliosis after ketogenic diet treatment as well as restored excitatory-inhibitory signaling equilibrium in zebrafish epilepsy model. Moreover, our results highlight the effectiveness of leveraging LLMs in combination with advanced hardware acceleration for transcriptomic characterization in neurological diseases.",
      "authors": "Muhammad Omer Latif; Hayat Ullah; Muhammad Ali Shafique; Zhihua Dong",
      "date": "2025-10-01",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.CV",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.26223v1",
      "doi": "",
      "title": "Nephrobase Cell+: Multimodal Single-Cell Foundation Model for Decoding Kidney Biology",
      "abstract": "Background: Large foundation models have revolutionized single-cell analysis, yet no kidney-specific model currently exists, and it remains unclear whether organ-focused models can outperform generalized models. The kidney's complex cellular architecture further complicate integration of large-scale omics data, where current frameworks trained on limited datasets struggle to correct batch effects, capture cross-modality variation, and generalize across species. Methods: We developed Nephrobase Cell+, the first kidney-focused large foundation model, pretrained on ~100 billion tokens from ~39.5 million single-cell and single-nucleus profiles across 4,319 samples. Nephrobase Cell+ uses a transformer-based encoder-decoder architecture with gene-token cross-attention and a mixture-of-experts module for scalable representation learning. Results: Nephrobase Cell+ sets a new benchmark for kidney single-cell analysis. It produces tightly clustered, biologically coherent embeddings in human and mouse kidneys, far surpassing previous foundation models such as Geneformer, scGPT, and UCE, as well as traditional methods such as PCA and autoencoders. It achieves the highest cluster concordance and batch-mixing scores, effectively removing donor/assay batch effects while preserving cell-type structure. Cross-species evaluation shows superior alignment of homologous cell types and >90% zero-shot annotation accuracy for major kidney lineages in both human and mouse. Even its 1B-parameter and 500M variants consistently outperform all existing models. Conclusions: Nephrobase Cell+ delivers a unified, high-fidelity representation of kidney biology that is robust, cross-species transferable, and unmatched by current single-cell foundation models, offering a powerful resource for kidney genomics and disease research.",
      "authors": "Chenyu Li; Elias Ziyadeh; Yash Sharma; Bernhard Dumoulin; Jonathan Levinsohn; Eunji Ha; Siyu Pan; Vishwanatha Rao; Madhav Subramaniyam; Mario Szegedy; Nancy Zhang; Katalin Susztak",
      "date": "2025-09-30",
      "year": "2025",
      "categories": [
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.24840v2",
      "doi": "",
      "title": "Cell2Text: Multimodal LLM for Generating Single-Cell Descriptions from RNA-Seq Data",
      "abstract": "Single-cell RNA sequencing has transformed biology by enabling the measurement of gene expression at cellular resolution, providing information for cell types, states, and disease contexts. Recently, single-cell foundation models have emerged as powerful tools for learning transferable representations directly from expression profiles, improving performance on classification and clustering tasks. However, these models are limited to discrete prediction heads, which collapse cellular complexity into predefined labels that fail to capture the richer, contextual explanations biologists need. We introduce Cell2Text, a multimodal generative framework that translates scRNA-seq profiles into structured natural language descriptions. By integrating gene-level embeddings from single-cell foundation models with pretrained large language models, Cell2Text generates coherent summaries that capture cellular identity, tissue origin, disease associations, and pathway activity, generalizing to unseen cells. Empirically, Cell2Text outperforms baselines on classification accuracy, demonstrates strong ontological consistency using PageRank-based similarity metrics, and achieves high semantic fidelity in text generation. These results demonstrate that coupling expression data with natural language offers both stronger predictive performance and inherently interpretable outputs, pointing to a scalable path for label-efficient characterization of unseen cells.",
      "authors": "Oussama Kharouiche; Aris Markogiannakis; Xiao Fei; Michail Chatzianastasis; Michalis Vazirgiannis",
      "date": "2025-09-29",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.CE"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.23986v1",
      "doi": "",
      "title": "TusoAI: Agentic Optimization for Scientific Methods",
      "abstract": "Scientific discovery is often slowed by the manual development of computational tools needed to analyze complex experimental data. Building such tools is costly and time-consuming because scientists must iteratively review literature, test modeling and scientific assumptions against empirical data, and implement these insights into efficient software. Large language models (LLMs) have demonstrated strong capabilities in synthesizing literature, reasoning with empirical data, and generating domain-specific code, offering new opportunities to accelerate computational method development. Existing LLM-based systems either focus on performing scientific analyses using existing computational methods or on developing computational methods or models for general machine learning without effectively integrating the often unstructured knowledge specific to scientific domains. Here, we introduce TusoAI , an agentic AI system that takes a scientific task description with an evaluation function and autonomously develops and optimizes computational methods for the application. TusoAI integrates domain knowledge into a knowledge tree representation and performs iterative, domain-specific optimization and model diagnosis, improving performance over a pool of candidate solutions. We conducted comprehensive benchmark evaluations demonstrating that TusoAI outperforms state-of-the-art expert methods, MLE agents, and scientific AI agents across diverse tasks, such as single-cell RNA-seq data denoising and satellite-based earth monitoring. Applying TusoAI to two key open problems in genetics improved existing computational methods and uncovered novel biology, including 9 new associations between autoimmune diseases and T cell subtypes and 7 previously unreported links between disease variants linked to their target genes. Our code is publicly available at https://github.com/Alistair-Turcan/TusoAI.",
      "authors": "Alistair Turcan; Kexin Huang; Lei Li; Martin Jinye Zhang",
      "date": "2025-09-28",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.25274v1",
      "doi": "",
      "title": "DNABERT-2: Fine-Tuning a Genomic Language Model for Colorectal Gene Enhancer Classification",
      "abstract": "Gene enhancers control when and where genes switch on, yet their sequence diversity and tissue specificity make them hard to pinpoint in colorectal cancer. We take a sequence-only route and fine-tune DNABERT-2, a transformer genomic language model that uses byte-pair encoding to learn variable-length tokens from DNA. Using assays curated via the Johnston Cancer Research Centre at Queen's University Belfast, we assembled a balanced corpus of 2.34 million 1 kb enhancer sequences, applied summit-centered extraction and rigorous de-duplication including reverse-complement collapse, and split the data stratified by class. With a 4096-term vocabulary and a 232-token context chosen empirically, the DNABERT-2-117M classifier was trained with Optuna-tuned hyperparameters and evaluated on 350742 held-out sequences. The model reached PR-AUC 0.759, ROC-AUC 0.743, and best F1 0.704 at an optimized threshold (0.359), with recall 0.835 and precision 0.609. Against a CNN-based EnhancerNet trained on the same data, DNABERT-2 delivered stronger threshold-independent ranking and higher recall, although point accuracy was lower. To our knowledge, this is the first study to apply a second-generation genomic language model with BPE tokenization to enhancer classification in colorectal cancer, demonstrating the feasibility of capturing tumor-associated regulatory signals directly from DNA sequence alone. Overall, our results show that transformer-based genomic models can move beyond motif-level encodings toward holistic classification of regulatory elements, offering a novel path for cancer genomics. Next steps will focus on improving precision, exploring hybrid CNN-transformer designs, and validating across independent datasets to strengthen real-world utility.",
      "authors": "Darren King; Yaser Atlasi; Gholamreza Rafiee",
      "date": "2025-09-28",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.23543v1",
      "doi": "",
      "title": "Contrastive Learning Enhances Language Model Based Cell Embeddings for Low-Sample Single Cell Transcriptomics",
      "abstract": "Large language models (LLMs) have shown strong ability in generating rich representations across domains such as natural language processing and generation, computer vision, and multimodal learning. However, their application in biomedical data analysis remains nascent. Single-cell transcriptomic profiling is essential for dissecting cell subtype diversity in development and disease, but rare subtypes pose challenges for scaling laws. We present a computational framework that integrates single-cell RNA sequencing (scRNA-seq) with LLMs to derive knowledge-informed gene embeddings. Highly expressed genes for each cell are mapped to NCBI Gene descriptions and embedded using models such as text-embedding-ada-002, BioBERT, and SciBERT. Applied to retinal ganglion cells (RGCs), which differ in vulnerability to glaucoma-related neurodegeneration, this strategy improves subtype classification, highlights biologically significant features, and reveals pathways underlying selective neuronal vulnerability. More broadly, it illustrates how LLM-derived embeddings can augment biological analysis under data-limited conditions and lay the groundwork for future foundation models in single-cell biology.",
      "authors": "Luxuan Zhang; Douglas Jiang; Qinglong Wang; Haoqi Sun; Feng Tian",
      "date": "2025-09-28",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.NE",
        "q-bio.MN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.20935v2",
      "doi": "",
      "title": "GALAX: Graph-Augmented Language Model for Explainable Reinforcement-Guided Subgraph Reasoning in Precision Medicine",
      "abstract": "In precision medicine, quantitative multi-omic features, topological context, and textual biological knowledge play vital roles in identifying disease-critical signaling pathways and targets. Existing pipelines capture only part of these-numerical omics ignore topological context, text-centric LLMs lack quantitative grounded reasoning, and graph-only models underuse node semantics and the generalization of LLMs-limiting mechanistic interpretability. Although Process Reward Models (PRMs) aim to guide reasoning in LLMs, they remain limited by unreliable intermediate evaluation, and vulnerability to reward hacking with computational cost. These gaps motivate integrating quantitative multi-omic signals, topological structure with node annotations, and literature-scale text via LLMs, using subgraph reasoning as the principle bridge linking numeric evidence, topological knowledge and language context. Therefore, we propose GALAX (Graph Augmented LAnguage model with eXplainability), an innovative framework that integrates pretrained Graph Neural Networks (GNNs) into Large Language Models (LLMs) via reinforcement learning guided by a Graph Process Reward Model (GPRM), which generates disease-relevant subgraphs in a step-wise manner initiated by an LLM and iteratively evaluated by a pretrained GNN and schema-based rule check, enabling process-level supervision without explicit labels. As an application, we also introduced Target-QA, a benchmark combining CRISPR-identified targets, multi-omic profiles, and biomedical graph knowledge across diverse cancer cell lines, which enables GNN pretraining for supervising step-wise graph construction and supports long-context reasoning over text-numeric graphs (TNGs), providing a scalable and biologically grounded framework for explainable, reinforcement-guided subgraph reasoning toward reliable and interpretable target discovery in precision medicine.",
      "authors": "Heming Zhang; Di Huang; Wenyu Li; Michael Province; Yixin Chen; Philip Payne; Fuhai Li",
      "date": "2025-09-25",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.20702v1",
      "doi": "",
      "title": "Incorporating LLM Embeddings for Variation Across the Human Genome",
      "abstract": "Recent advances in large language model (LLM) embeddings have enabled powerful representations for biological data, but most applications to date focus only on gene-level information. We present one of the first systematic frameworks to generate variant-level embeddings across the entire human genome. Using curated annotations from FAVOR, ClinVar, and the GWAS Catalog, we constructed semantic text descriptions for 8.9 billion possible variants and generated embeddings at three scales: 1.5 million HapMap3+MEGA variants, ~90 million imputed UK Biobank variants, and ~9 billion all possible variants. Embeddings were produced with both OpenAI's text-embedding-3-large and the open-source Qwen3-Embedding-0.6B models. Baseline experiments demonstrate high predictive accuracy for variant properties, validating the embeddings as structured representations of genomic variation. We outline two downstream applications: embedding-informed hypothesis testing by extending the Frequentist And Bayesian framework to genome-wide association studies, and embedding-augmented genetic risk prediction that enhances standard polygenic risk scores. These resources, publicly available on Hugging Face, provide a foundation for advancing large-scale genomic discovery and precision medicine.",
      "authors": "Hongqian Niu; Jordan Bryan; Xihao Li; Didong Li",
      "date": "2025-09-25",
      "year": "2025",
      "categories": [
        "stat.AP",
        "cs.AI",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.16892v1",
      "doi": "",
      "title": "Learning from Gene Names, Expression Values and Images: Contrastive Masked Text-Image Pretraining for Spatial Transcriptomics Representation Learning",
      "abstract": "Spatial transcriptomics aims to connect high-resolution histology images with spatially resolved gene expression. To achieve better performance on downstream tasks such as gene expression prediction, large-scale pre-training is required to obtain generalisable representations that can bridge histology and transcriptomics across tissues, protocols, and laboratories. Existing cross-modal pre-training approaches for spatial transcriptomics rely on either gene names or expression values in isolation, which strips the gene branch of essential semantics and breaks the association between each gene and its quantitative magnitude. In addition, by restricting supervision to image-text alignment, these methods ignore intrinsic visual cues that are critical for learning robust image features. We present CoMTIP, the first Contrastive Masked Text-Image Pretraining framework that jointly learns from images, gene names, and expression values while capturing fine-grained visual context for spatial transcriptomics. The vision branch uses Masked Feature Modeling to reconstruct occluded patches and learn context-aware image embeddings. The text branch applies a scalable Gene-Text Encoder that processes all gene sentences in parallel, enriches each gene and its numerical value with dedicated embeddings, and employs Pair-aware Adversarial Training (PAAT) to preserve correct gene-value associations. Image and text representations are aligned in a shared InfoNCE-optimised space. Experiments on public spatial transcriptomics datasets show that CoMTIP not only surpasses previous methods on diverse downstream tasks but also achieves zero-shot gene expression prediction, a capability that existing approaches do not provide.",
      "authors": "Jiahe Qian; Yaoyu Fang; Ziqiao Weng; Xinkun Wang; Lee A. Cooper; Bo Zhou",
      "date": "2025-09-21",
      "year": "2025",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.14723v1",
      "doi": "",
      "title": "Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models",
      "abstract": "Single-cell foundation models (scFMs) have demonstrated state-of-the-art performance on various tasks, such as cell-type annotation and perturbation response prediction, by learning gene regulatory networks from large-scale transcriptome data. However, a significant challenge remains: the decision-making processes of these models are less interpretable compared to traditional methods like differential gene expression analysis. Recently, transcoders have emerged as a promising approach for extracting interpretable decision circuits from large language models (LLMs). In this work, we train a transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By leveraging the trained transcoder, we extract internal decision-making circuits from the C2S model. We demonstrate that the discovered circuits correspond to real-world biological mechanisms, confirming the potential of transcoders to uncover biologically plausible pathways within complex single-cell models.",
      "authors": "Sosuke Hosokawa; Toshiharu Kawakami; Satoshi Kodera; Masamichi Ito; Norihiko Takeda",
      "date": "2025-09-18",
      "year": "2025",
      "categories": [
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.09740v1",
      "doi": "",
      "title": "HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets",
      "abstract": "Large-scale single-cell and Perturb-seq investigations routinely involve clustering cells and subsequently annotating each cluster with Gene-Ontology (GO) terms to elucidate the underlying biological programs. However, both stages, resolution selection and functional annotation, are inherently subjective, relying on heuristics and expert curation. We present HYPOGENEAGENT, a large language model (LLM)-driven framework, transforming cluster annotation into a quantitatively optimizable task. Initially, an LLM functioning as a gene-set analyst analyzes the content of each gene program or perturbation module and generates a ranked list of GO-based hypotheses, accompanied by calibrated confidence scores. Subsequently, we embed every predicted description with a sentence-embedding model, compute pair-wise cosine similarities, and let the agent referee panel score (i) the internal consistency of the predictions, high average similarity within the same cluster, termed intra-cluster agreement (ii) their external distinctiveness, low similarity between clusters, termed inter-cluster separation. These two quantities are combined to produce an agent-derived resolution score, which is maximized when clusters exhibit simultaneous coherence and mutual exclusivity. When applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary test, our Resolution Score selects clustering granularities that exhibit alignment with known pathway compared to classical metrics such silhouette score, modularity score for gene functional enrichment summary. These findings establish LLM agents as objective adjudicators of cluster resolution and functional annotation, thereby paving the way for fully automated, context-aware interpretation pipelines in single-cell multi-omics studies.",
      "authors": "Ying Yuan; Xing-Yue Monica Ge; Aaron Archer Waterman; Tommaso Biancalani; David Richmond; Yogesh Pandit; Avtar Singh; Russell Littman; Jin Liu; Jan-Christian Huetter; Vladimir Ermakov",
      "date": "2025-09-10",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.06917v2",
      "doi": "",
      "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents",
      "abstract": "We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. Paper2Agent automatically created AI co-scientist that identified new splicing variant associated with ADHD risk. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.",
      "authors": "Jiacheng Miao; Joe R. Davis; Yaohui Zhang; Jonathan K. Pritchard; James Zou",
      "date": "2025-09-08",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2509.06503v1",
      "doi": "",
      "title": "An AI system to help scientists write expert-level empirical software",
      "abstract": "The cycle of scientific discovery is frequently bottlenecked by the slow, manual creation of software to support computational experiments. To address this, we present an AI system that creates expert-level scientific software whose goal is to maximize a quality metric. The system uses a Large Language Model (LLM) and Tree Search (TS) to systematically improve the quality metric and intelligently navigate the large space of possible solutions. The system achieves expert-level results when it explores and integrates complex research ideas from external sources. The effectiveness of tree search is demonstrated across a wide range of benchmarks. In bioinformatics, it discovered 40 novel methods for single-cell data analysis that outperformed the top human-developed methods on a public leaderboard. In epidemiology, it generated 14 models that outperformed the CDC ensemble and all other individual models for forecasting COVID-19 hospitalizations. Our method also produced state-of-the-art software for geospatial analysis, neural activity prediction in zebrafish, time series forecasting and numerical solution of integrals. By devising and implementing novel solutions to diverse tasks, the system represents a significant step towards accelerating scientific progress.",
      "authors": "Eser Aygün; Anastasiya Belyaeva; Gheorghe Comanici; Marc Coram; Hao Cui; Jake Garrison; Renee Johnston Anton Kast; Cory Y. McLean; Peter Norgaard; Zahra Shamsi; David Smalling; James Thompson; Subhashini Venugopalan; Brian P. Williams; Chujun He; Sarah Martinson; Martyna Plomecka; Lai Wei; Yuchen Zhou; Qian-Ze Zhu; Matthew Abraham; Erica Brand; Anna Bulanova; Jeffrey A. Cardille; Chris Co; Scott Ellsworth; Grace Joseph; Malcolm Kane; Ryan Krueger; Johan Kartiwa; Dan Liebling; Jan-Matthis Lueckmann; Paul Raccuglia;  Xuefei;  Wang; Katherine Chou; James Manyika; Yossi Matias; John C. Platt; Lizzie Dorfman; Shibl Mourad; Michael P. Brenner",
      "date": "2025-09-08",
      "year": "2025",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2508.20275v1",
      "doi": "",
      "title": "A Systematic Review on the Generative AI Applications in Human Medical Genomics",
      "abstract": "Although traditional statistical techniques and machine learning methods have contributed significantly to genetics and, in particular, inherited disease diagnosis, they often struggle with complex, high-dimensional data, a challenge now addressed by state-of-the-art deep learning models. Large language models (LLMs), based on transformer architectures, have excelled in tasks requiring contextual comprehension of unstructured medical data. This systematic review examines the role of LLMs in the genetic research and diagnostics of both rare and common diseases. Automated keyword-based search in PubMed, bioRxiv, medRxiv, and arXiv was conducted, targeting studies on LLM applications in diagnostics and education within genetics and removing irrelevant or outdated models. A total of 172 studies were analyzed, highlighting applications in genomic variant identification, annotation, and interpretation, as well as medical imaging advancements through vision transformers. Key findings indicate that while transformer-based models significantly advance disease and risk stratification, variant interpretation, medical imaging analysis, and report generation, major challenges persist in integrating multimodal data (genomic sequences, imaging, and clinical records) into unified and clinically robust pipelines, facing limitations in generalizability and practical implementation in clinical settings. This review provides a comprehensive classification and assessment of the current capabilities and limitations of LLMs in transforming hereditary disease diagnostics and supporting genetic education, serving as a guide to navigate this rapidly evolving field.",
      "authors": "Anton Changalidis; Yury Barbitoff; Yulia Nasykhova; Andrey Glotov",
      "date": "2025-08-27",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.CL",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2508.13201v2",
      "doi": "",
      "title": "Benchmarking LLM-based Agents for Single-cell Omics Analysis",
      "abstract": "The surge in multimodal single-cell omics data exposes limitations in traditional, manually defined analysis workflows. AI agents offer a paradigm shift, enabling adaptive planning, executable code generation, traceable decisions, and real-time knowledge fusion. However, the lack of a comprehensive benchmark critically hinders progress. We introduce a novel benchmarking evaluation system to rigorously assess agent capabilities in single-cell omics analysis. This system comprises: a unified platform compatible with diverse agent frameworks and LLMs; multidimensional metrics assessing cognitive program synthesis, collaboration, execution efficiency, bioinformatics knowledge integration, and task completion quality; and 50 diverse real-world single-cell omics analysis tasks spanning multi-omics, species, and sequencing technologies. Our evaluation reveals that Grok-3-beta achieves state-of-the-art performance among tested agent frameworks. Multi-agent frameworks significantly enhance collaboration and execution efficiency over single-agent approaches through specialized role division. Attribution analyses of agent capabilities identify that high-quality code generation is crucial for task success, and self-reflection has the most significant overall impact, followed by retrieval-augmented generation (RAG) and planning. This work highlights persistent challenges in code generation, long-context handling, and context-aware knowledge retrieval, providing a critical empirical foundation and best practices for developing robust AI agents in computational biology.",
      "authors": "Yang Liu; Lu Zhou; Xiawei Du; Ruikun He; Rongbo Shen; Yixue Li",
      "date": "2025-08-16",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.MA"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2508.13191v1",
      "doi": "",
      "title": "NucEL: Single-Nucleotide ELECTRA-Style Genomic Pre-training for Efficient and Interpretable Representations",
      "abstract": "Pre-training large language models on genomic sequences is a powerful approach for learning biologically meaningful representations. Masked language modeling (MLM) methods, such as DNABERT and Nucleotide Transformer (NT), achieve strong performance but suffer from partial token supervision, pre-training/fine-tuning mismatches, and high computational costs. We introduce NucEL, the first ELECTRA-style pre-training framework for genomic foundation models, addressing these limitations. Using a discriminator to identify tokens altered by a generator, NucEL provides comprehensive token-level supervision across all sequence positions, improving efficiency over the partial supervision of MLM. Incorporating ModernBERT's hybrid local-global attention and flash attention, NucEL offers an optimized BERT architecture for genomic modeling. Unlike 6-mer tokenization, NucEL uses single-nucleotide tokens for fine-grained resolution, boosting both efficiency and interpretability. Pre-trained on the human genome, NucEL achieves state-of-the-art results on diverse downstream tasks -- regulatory element identification (e.g., promoters, enhancers), transcription factor binding prediction, open chromatin classification, and histone modification profiling -- surpassing similarly sized MLM-based models and rivaling models 25x larger, such as NT. Ablation studies highlight optimal tokenization and masking strategies for ELECTRA-style DNA pre-training. Attention analysis reveals NucEL's superior capture of biologically relevant motifs compared to NT, providing insights into hierarchical learning and regulatory element modeling. These findings demonstrate ELECTRA-style pre-training as an efficient, effective strategy for genomic representation learning with broad implications for genomic research.",
      "authors": "Ke Ding; Brian Parker; Jiayu Wen",
      "date": "2025-08-15",
      "year": "2025",
      "categories": [
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2508.10703v2",
      "doi": "",
      "title": "GenOM: Ontology Matching with Description Generation and Large Language Model",
      "abstract": "Ontology matching (OM) plays an essential role in enabling semantic interoperability and integration across heterogeneous knowledge sources, particularly in the biomedical domain which contains numerous complex concepts related to diseases and pharmaceuticals. This paper introduces GenOM, a large language model (LLM)-based ontology alignment framework, which enriches the semantic representations of ontology concepts via generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often achieve competitive performance, surpassing many baselines including traditional OM systems and recent LLM-based methods. Further ablation studies confirm the effectiveness of semantic enrichment and few-shot prompting, highlighting the framework's robustness and adaptability.",
      "authors": "Yiping Song; Jiaoyan Chen; Renate A. Schmidt",
      "date": "2025-08-14",
      "year": "2025",
      "categories": [
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2508.04757v1",
      "doi": "",
      "title": "Embedding Is (Almost) All You Need: Retrieval-Augmented Inference for Generalizable Genomic Prediction Tasks",
      "abstract": "Large pre-trained DNA language models such as DNABERT-2, Nucleotide Transformer, and HyenaDNA have demonstrated strong performance on various genomic benchmarks. However, most applications rely on expensive fine-tuning, which works best when the training and test data share a similar distribution. In this work, we investigate whether task-specific fine-tuning is always necessary. We show that simple embedding-based pipelines that extract fixed representations from these models and feed them into lightweight classifiers can achieve competitive performance. In evaluation settings with different data distributions, embedding-based methods often outperform fine-tuning while reducing inference time by 10x to 20x. Our results suggest that embedding extraction is not only a strong baseline but also a more generalizable and efficient alternative to fine-tuning, especially for deployment in diverse or unseen genomic contexts. For example, in enhancer classification, HyenaDNA embeddings combined with zCurve achieve 0.68 accuracy (vs. 0.58 for fine-tuning), with an 88% reduction in inference time and over 8x lower carbon emissions (0.02 kg vs. 0.17 kg CO2). In non-TATA promoter classification, DNABERT-2 embeddings with zCurve or GC content reach 0.85 accuracy (vs. 0.89 with fine-tuning) with a 22x lower carbon footprint (0.02 kg vs. 0.44 kg CO2). These results show that embedding-based pipelines offer over 10x better carbon efficiency while maintaining strong predictive performance. The code is available here: https://github.com/NIRJHOR-DATTA/EMBEDDING-IS-ALMOST-ALL-YOU-NEED.",
      "authors": "Nirjhor Datta; Swakkhar Shatabda; M Sohel Rahman",
      "date": "2025-08-06",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2508.04747v2",
      "doi": "",
      "title": "GRIT: Graph-Regularized Logit Refinement for Zero-shot Cell Type Annotation",
      "abstract": "Cell type annotation is a fundamental step in the analysis of single-cell RNA sequencing (scRNA-seq) data. In practice, human experts often rely on the structure revealed by principal component analysis (PCA) followed by $k$-nearest neighbor ($k$-NN) graph construction to guide annotation. While effective, this process is labor-intensive and does not scale to large datasets. Recent advances in CLIP-style models offer a promising path toward automating cell type annotation. By aligning scRNA-seq profiles with natural language descriptions, models like LangCell enable zero-shot annotation. While LangCell demonstrates decent zero-shot performance, its predictions remain suboptimal. In this paper, we propose a principled inference-time paradigm for zero-shot cell type annotation (GRIT) which bridges the scalability of pre-trained foundation models with the structural robustness relied upon in human expert annotation workflows. Specifically, we enforce local consistency of the zero-shot CLIP logits over the task-specific PCA-based $k$-NN graph. We evaluate our approach on 14 annotated human scRNA-seq datasets from 4 distinct studies, spanning 11 organs and over 200,000 single cells. Our method consistently improves zero-shot annotation accuracy, achieving accuracy gains of up to 10\\%. Further analysis showcase the mechanism by which GRIT effectively propagates correct signals through the graph, pulling back mislabeled cells toward more accurate predictions. The method is training-free, model-agnostic, and serves as a simple yet effective plug-in for enhancing zero-shot cell type annotation.",
      "authors": "Tianxiang Hu; Chenyi Zhou; Jiaxiang Liu; Jiongxin Wang; Ruizhe Chen; Haoxiang Xia; Gaoang Wang; Jian Wu; Zuozhu Liu",
      "date": "2025-08-06",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2508.02276v2",
      "doi": "",
      "title": "CellForge: Agentic Design of Virtual Cell Models",
      "abstract": "Virtual cell modeling aims to predict cellular responses to diverse perturbations but faces challenges from biological complexity, multimodal data heterogeneity, and the need for interdisciplinary expertise. We introduce CellForge, a multi-agent framework that autonomously designs and synthesizes neural network architectures tailored to specific single-cell datasets and perturbation tasks. Given raw multi-omics data and task descriptions, CellForge discovers candidate architectures through collaborative reasoning among specialized agents, then generates executable implementations. Our core contribution is the framework itself: showing that multi-agent collaboration mechanisms - rather than manual human design or single-LLM prompting - can autonomously produce executable, high-quality computational methods. This approach goes beyond conventional hyperparameter tuning by enabling entirely new architectural components such as trajectory-aware encoders and perturbation diffusion modules to emerge from agentic deliberation. We evaluate CellForge on six datasets spanning gene knockouts, drug treatments, and cytokine stimulations across multiple modalities (scRNA-seq, scATAC-seq, CITE-seq). The results demonstrate that the models generated by CellForge are highly competitive with established baselines, while revealing systematic patterns of architectural innovation. CellForge highlights the scientific value of multi-agent frameworks: collaboration among specialized agents enables genuine methodological innovation and executable solutions that single agents or human experts cannot achieve. This represents a paradigm shift toward autonomous scientific method development in computational biology. Code is available at https://github.com/gersteinlab/CellForge.",
      "authors": "Xiangru Tang; Zhuoyun Yu; Jiapeng Chen; Yan Cui; Daniel Shao; Weixu Wang; Fang Wu; Yuchen Zhuang; Wenqi Shi; Zhi Huang; Arman Cohan; Xihong Lin; Fabian Theis; Smita Krishnaswamy; Mark Gerstein",
      "date": "2025-08-04",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2508.01490v2",
      "doi": "",
      "title": "A Large-Scale Benchmark of Cross-Modal Learning for Histology and Gene Expression in Spatial Transcriptomics",
      "abstract": "Spatial transcriptomics enables simultaneous measurement of gene expression and tissue morphology, offering unprecedented insights into cellular organization and disease mechanisms. However, the field lacks comprehensive benchmarks for evaluating multimodal learning methods that leverage both histology images and gene expression data. Here, we present HESCAPE, a large-scale benchmark for cross-modal contrastive pretraining in spatial transcriptomics, built on a curated pan-organ dataset spanning 6 different gene panels and 54 donors. We systematically evaluated state-of-the-art image and gene expression encoders across multiple pretraining strategies and assessed their effectiveness on two downstream tasks: gene mutation classification and gene expression prediction. Our benchmark demonstrates that gene expression encoders are the primary determinant of strong representational alignment, and that gene models pretrained on spatial transcriptomics data outperform both those trained without spatial data and simple baseline approaches. However, downstream task evaluation reveals a striking contradiction: while contrastive pretraining consistently improves gene mutation classification performance, it degrades direct gene expression prediction compared to baseline encoders trained without cross-modal objectives. We identify batch effects as a key factor that interferes with effective cross-modal alignment. Our findings highlight the critical need for batch-robust multimodal learning approaches in spatial transcriptomics. To accelerate progress in this direction, we release HESCAPE, providing standardized datasets, evaluation protocols, and benchmarking tools for the community",
      "authors": "Rushin H. Gindra; Giovanni Palla; Mathias Nguyen; Sophia J. Wagner; Manuel Tran; Fabian J Theis; Dieter Saur; Lorin Crawford; Tingying Peng",
      "date": "2025-08-02",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "q-bio.TO",
        "stat.AP"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2508.00969v2",
      "doi": "",
      "title": "Masked Omics Modeling for Multimodal Representation Learning across Histopathology and Molecular Profiles",
      "abstract": "Self-supervised learning (SSL) has driven major advances in computational pathology by enabling the learning of rich representations from histopathology data. Yet, tissue analysis alone may fall short in capturing broader molecular complexity, as key complementary information resides in high-dimensional omics profiles such as transcriptomics, methylomics, and genomics. To address this gap, we introduce MORPHEUS, the first multimodal pre-training strategy that integrates histopathology images and multi-omics data within a shared transformer-based architecture. At its core, MORPHEUS relies on a novel masked omics modeling objective that encourages the model to learn meaningful cross-modal relationships. This yields a general-purpose pre-trained encoder that can be applied to histopathology alone or in combination with any subset of omics modalities. Beyond inference, MORPHEUS also supports flexible any-to-any omics reconstruction, enabling one or more omics profiles to be reconstructed from any modality subset that includes histopathology. Pre-trained on a large pan-cancer cohort, MORPHEUS shows substantial improvements over supervised and SSL baselines across diverse tasks and modality combinations. Together, these capabilities position it as a promising direction for the development of multimodal foundation models in oncology. Code is publicly available at https://github.com/Lucas-rbnt/MORPHEUS",
      "authors": "Lucas Robinet; Ahmad Berjaoui; Elizabeth Cohen-Jonathan Moyal",
      "date": "2025-08-01",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.22212v1",
      "doi": "",
      "title": "Evaluating Integrative Strategies for Incorporating Phenotypic Features in Spatial Transcriptomics",
      "abstract": "Spatial transcriptomics (ST) technologies not only offer an unprecedented opportunity to interrogate intact biological samples in a spatially informed manner, but also set the stage for integration with other imaging-based modalities. However, how to best exploit spatial context and integrate ST with imaging-based modalities remains an open question. To address this, particularly under real-world experimental constraints such as limited dataset size, class imbalance, and bounding-box-based segmentation, we used a publicly available murine ileum MERFISH dataset to evaluate whether a minimally tuned variational autoencoder (VAE) could extract informative low-dimensional representations from cell crops of spot counts, nuclear stain, membrane stain, or a combination thereof. We assessed the resulting embeddings through PERMANOVA, cross-validated classification, and unsupervised Leiden clustering, and compared them to classical image-based feature vectors extracted via CellProfiler. While transcript counts (TC) generally outperformed other feature spaces, the VAE-derived latent spaces (LSs) captured meaningful biological variation and enabled improved label recovery for specific cell types. LS2, in particular, trained solely on morphological input, also exhibited moderate predictive power for a handful of genes in a ridge regression model. Notably, combining TC with LSs through multiplex clustering led to consistent gains in cluster homogeneity, a trend that also held when augmenting only subsets of TC with the stain-derived LS2. In contrast, CellProfiler-derived features underperformed relative to LSs, highlighting the advantage of learned representations over hand-crafted features. Collectively, these findings demonstrate that even under constrained conditions, VAEs can extract biologically meaningful signals from imaging data and constitute a promising strategy for multi-modal integration.",
      "authors": "Levin M Moser; Ahmad Kamal Hamid; Esteban Miglietta; Nodar Gogoberidze; Beth A Cimini",
      "date": "2025-07-29",
      "year": "2025",
      "categories": [
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.21648v1",
      "doi": "",
      "title": "Hyperbolic Genome Embeddings",
      "abstract": "Current approaches to genomic sequence modeling often struggle to align the inductive biases of machine learning models with the evolutionarily-informed structure of biological systems. To this end, we formulate a novel application of hyperbolic CNNs that exploits this structure, enabling more expressive DNA sequence representations. Our strategy circumvents the need for explicit phylogenetic mapping while discerning key properties of sequences pertaining to core functional and regulatory behavior. Across 37 out of 42 genome interpretation benchmark datasets, our hyperbolic models outperform their Euclidean equivalents. Notably, our approach even surpasses state-of-the-art performance on seven GUE benchmark datasets, consistently outperforming many DNA language models while using orders of magnitude fewer parameters and avoiding pretraining. Our results include a novel set of benchmark datasets--the Transposable Elements Benchmark--which explores a major but understudied component of the genome with deep evolutionary significance. We further motivate our work by exploring how our hyperbolic models recognize genomic signal under various data-generating conditions and by constructing an empirical method for interpreting the hyperbolicity of dataset embeddings. Throughout these assessments, we find persistent evidence highlighting the potential of our hyperbolic framework as a robust paradigm for genome representation learning. Our code and benchmark datasets are available at https://github.com/rrkhan/HGE.",
      "authors": "Raiyan R. Khan; Philippe Chlenski; Itsik Pe'er",
      "date": "2025-07-29",
      "year": "2025",
      "categories": [
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.10039v1",
      "doi": "",
      "title": "Towards Applying Large Language Models to Complement Single-Cell Foundation Models",
      "abstract": "Single-cell foundation models such as scGPT represent a significant advancement in single-cell omics, with an ability to achieve state-of-the-art performance on various downstream biological tasks. However, these models are inherently limited in that a vast amount of information in biology exists as text, which they are unable to leverage. There have therefore been several recent works that propose the use of LLMs as an alternative to single-cell foundation models, achieving competitive results. However, there is little understanding of what factors drive this performance, along with a strong focus on using LLMs as an alternative, rather than complementary approach to single-cell foundation models. In this study, we therefore investigate what biological insights contribute toward the performance of LLMs when applied to single-cell data, and introduce scMPT; a model which leverages synergies between scGPT, and single-cell representations from LLMs that capture these insights. scMPT demonstrates stronger, more consistent performance than either of its component models, which frequently have large performance gaps between each other across datasets. We also experiment with alternate fusion methods, demonstrating the potential of combining specialized reasoning models with scGPT to improve performance. This study ultimately showcases the potential for LLMs to complement single-cell foundation models and drive improvements in single-cell analysis.",
      "authors": "Steven Palayew; Bo Wang; Gary Bader",
      "date": "2025-07-14",
      "year": "2025",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.09028v2",
      "doi": "",
      "title": "From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research",
      "abstract": "Cancer research is increasingly driven by the integration of diverse data modalities, spanning from genomics and proteomics to imaging and clinical factors. However, extracting actionable insights from these vast and heterogeneous datasets remains a key challenge. The rise of foundation models (FMs) -- large deep-learning models pretrained on extensive amounts of data serving as a backbone for a wide range of downstream tasks -- offers new avenues for discovering biomarkers, improving diagnosis, and personalizing treatment. This paper presents a comprehensive review of widely adopted integration strategies of multimodal data to assist advance the computational approaches for data-driven discoveries in oncology. We examine emerging trends in machine learning (ML) and deep learning (DL), including methodological frameworks, validation protocols, and open-source resources targeting cancer subtype classification, biomarker discovery, treatment guidance, and outcome prediction. This study also comprehensively covers the shift from traditional ML to FMs for multimodal integration. We present a holistic view of recent FMs advancements and challenges faced during the integration of multi-omics with advanced imaging data. We identify the state-of-the-art FMs, publicly available multi-modal repositories, and advanced tools and methods for data integration. We argue that current state-of-the-art integrative methods provide the essential groundwork for developing the next generation of large-scale, pre-trained models poised to further revolutionize oncology. To the best of our knowledge, this is the first review to systematically map the transition from conventional ML to advanced FM for multimodal data integration in oncology, while also framing these developments as foundational for the forthcoming era of large-scale AI models in cancer research.",
      "authors": "Amgad Muneer; Muhammad Waqas; Maliazurina B Saad; Eman Showkatian; Rukhmini Bandyopadhyay; Hui Xu; Wentao Li; Joe Y Chang; Zhongxing Liao; Cara Haymaker; Luisa Solis Soto; Carol C Wu; Natalie I Vokes; Xiuning Le; Lauren A Byers; Don L Gibbons; John V Heymach; Jianjun Zhang; Jia Wu",
      "date": "2025-07-11",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.07454v1",
      "doi": "",
      "title": "Mix-Geneformer: Unified Representation Learning for Human and Mouse scRNA-seq Data",
      "abstract": "Single-cell RNA sequencing (scRNA-seq) enables single-cell transcriptomic profiling, revealing cellular heterogeneity and rare populations. Recent deep learning models like Geneformer and Mouse-Geneformer perform well on tasks such as cell-type classification and in silico perturbation. However, their species-specific design limits cross-species generalization and translational applications, which are crucial for advancing translational research and drug discovery. We present Mix-Geneformer, a novel Transformer-based model that integrates human and mouse scRNA-seq data into a unified representation via a hybrid self-supervised approach combining Masked Language Modeling (MLM) and SimCSE-based contrastive loss to capture both shared and species-specific gene patterns. A rank-value encoding scheme further emphasizes high-variance gene signals during training. Trained on about 50 million cells from diverse human and mouse organs, Mix-Geneformer matched or outperformed state-of-the-art baselines in cell-type classification and in silico perturbation tasks, achieving 95.8% accuracy on mouse kidney data versus 94.9% from the best existing model. It also successfully identified key regulatory genes validated by in vivo studies. By enabling scalable cross-species transcriptomic modeling, Mix-Geneformer offers a powerful tool for comparative transcriptomics and translational applications. While our results demonstrate strong performance, we also acknowledge limitations, such as the computational cost and variability in zero-shot transfer.",
      "authors": "Yuki Nishio; Takayoshi Yamashita; Keita Ito; Tsubasa Hirakawa; Hironobu Fujiyoshi",
      "date": "2025-07-10",
      "year": "2025",
      "categories": [
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.06418v1",
      "doi": "",
      "title": "PAST: A multimodal single-cell foundation model for histopathology and spatial transcriptomics in cancer",
      "abstract": "While pathology foundation models have transformed cancer image analysis, they often lack integration with molecular data at single-cell resolution, limiting their utility for precision oncology. Here, we present PAST, a pan-cancer single-cell foundation model trained on 20 million paired histopathology images and single-cell transcriptomes spanning multiple tumor types and tissue contexts. By jointly encoding cellular morphology and gene expression, PAST learns unified cross-modal representations that capture both spatial and molecular heterogeneity at the cellular level. This approach enables accurate prediction of single-cell gene expression, virtual molecular staining, and multimodal survival analysis directly from routine pathology slides. Across diverse cancers and downstream tasks, PAST consistently exceeds the performance of existing approaches, demonstrating robust generalizability and scalability. Our work establishes a new paradigm for pathology foundation models, providing a versatile tool for high-resolution spatial omics, mechanistic discovery, and precision cancer research.",
      "authors": "Changchun Yang; Haoyang Li; Yushuai Wu; Yilan Zhang; Yifeng Jiao; Yu Zhang; Rihan Huang; Yuan Cheng; Yuan Qi; Xin Guo; Xin Gao",
      "date": "2025-07-08",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.CV",
        "stat.AP"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.04704v1",
      "doi": "",
      "title": "SPATIA: Multimodal Model for Prediction and Generation of Spatial Cell Phenotypes",
      "abstract": "Understanding how cellular morphology, gene expression, and spatial organization jointly shape tissue function is a central challenge in biology. Image-based spatial transcriptomics technologies now provide high-resolution measurements of cell images and gene expression profiles, but machine learning methods typically analyze these modalities in isolation or at limited resolution. We address the problem of learning unified, spatially aware representations that integrate cell morphology, gene expression, and spatial context across biological scales. This requires models that can operate at single-cell resolution, reason across spatial neighborhoods, and generalize to whole-slide tissue organization. Here, we introduce SPATIA, a multi-scale generative and predictive model for spatial transcriptomics. SPATIA learns cell-level embeddings by fusing image-derived morphological tokens and transcriptomic vector tokens using cross-attention and then aggregates them at niche and tissue levels using transformer modules to capture spatial dependencies. SPATIA incorporates token merging in its generative diffusion decoder to synthesize high-resolution cell images conditioned on gene expression. We assembled a multi-scale dataset consisting of 17 million cell-gene pairs, 1 million niche-gene pairs, and 10,000 tissue-gene pairs across 49 donors, 17 tissue types, and 12 disease states. We benchmark SPATIA against 13 existing models across 12 individual tasks, which span several categories including cell annotation, cell clustering, gene imputation, cross-modal prediction, and image generation. SPATIA achieves improved performance over all baselines and generates realistic cell morphologies that reflect transcriptomic perturbations.",
      "authors": "Zhenglun Kong; Mufan Qiu; John Boesen; Xiang Lin; Sukwon Yun; Tianlong Chen; Manolis Kellis; Marinka Zitnik",
      "date": "2025-07-07",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CV"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.04125v1",
      "doi": "",
      "title": "Graph Neural Networks as a Substitute for Transformers in Single-Cell Transcriptomics",
      "abstract": "Graph Neural Networks (GNNs) and Transformers share significant similarities in their encoding strategies for interacting with features from nodes of interest, where Transformers use query-key scores and GNNs use edges. Compared to GNNs, which are unable to encode relative positions, Transformers leverage dynamic attention capabilities to better represent relative relationships, thereby becoming the standard backbones in large-scale sequential pre-training. However, the subtle difference prompts us to consider: if positions are no longer crucial, could we substitute Transformers with Graph Neural Networks in some fields such as Single-Cell Transcriptomics? In this paper, we first explore the similarities and differences between GNNs and Transformers, specifically in terms of relative positions. Additionally, we design a synthetic example to illustrate their equivalence where there are no relative positions between tokens in the sample. Finally, we conduct extensive experiments on a large-scale position-agnostic dataset-single-cell transcriptomics-finding that GNNs achieve competitive performance compared to Transformers while consuming fewer computation resources. These findings provide novel insights for researchers in the field of single-cell transcriptomics, challenging the prevailing notion that the Transformer is always the optimum choice.",
      "authors": "Jiaxin Qi; Yan Cui; Jinli Ou; Jianqiang Huang; Gaogang Xie",
      "date": "2025-07-05",
      "year": "2025",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.02221v2",
      "doi": "https://doi.org/10.1093/bioadv/vbaf295",
      "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons",
      "abstract": "The Genomic Data Commons (GDC) provides access to high quality, harmonized cancer genomics data through a unified curation and analysis platform centered around patient cohorts. While GDC users can interactively create complex cohorts through the graphical Cohort Builder, users (especially new ones) may struggle to find specific cohort descriptors across hundreds of possible fields and properties. However, users may be better able to describe their desired cohort in free-text natural language. We introduce GDC Cohort Copilot, an open-source copilot tool for curating cohorts from the GDC. GDC Cohort Copilot automatically generates the GDC cohort filter corresponding to a user-input natural language description of their desired cohort, before exporting the cohort back to the GDC for further analysis. An interactive user interface allows users to further refine the generated cohort. We develop and evaluate multiple large language models (LLMs) for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC Cohort LLM achieves better results than GPT-4o prompting in generating GDC cohorts. We implement and share GDC Cohort Copilot as a containerized Gradio app on HuggingFace Spaces, available at https://huggingface.co/spaces/uc-ctds/GDC-Cohort-Copilot. GDC Cohort LLM weights are available at https://huggingface.co/uc-ctds. All source code is available at https://github.com/uc-cdis/gdc-cohort-copilot.",
      "authors": "Steven Song; Anirudh Subramanyam; Zhenyu Zhang; Aarti Venkat; Robert L. Grossman",
      "date": "2025-07-03",
      "year": "2025",
      "categories": [
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.02980v1",
      "doi": "",
      "title": "Modeling Gene Expression Distributional Shifts for Unseen Genetic Perturbations",
      "abstract": "We train a neural network to predict distributional responses in gene expression following genetic perturbations. This is an essential task in early-stage drug discovery, where such responses can offer insights into gene function and inform target identification. Existing methods only predict changes in the mean expression, overlooking stochasticity inherent in single-cell data. In contrast, we offer a more realistic view of cellular responses by modeling expression distributions. Our model predicts gene-level histograms conditioned on perturbations and outperforms baselines in capturing higher-order statistics, such as variance, skewness, and kurtosis, at a fraction of the training cost. To generalize to unseen perturbations, we incorporate prior knowledge via gene embeddings from large language models (LLMs). While modeling a richer output space, the method remains competitive in predicting mean expression changes. This work offers a practical step towards more expressive and biologically informative models of perturbation effects.",
      "authors": "Kalyan Ramakrishnan; Jonathan G. Hedley; Sisi Qu; Puneet K. Dokania; Philip H. S. Torr; Cesar A. Prada-Medina; Julien Fauqueur; Kaspar Martens",
      "date": "2025-07-01",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.05265v1",
      "doi": "",
      "title": "BMFM-DNA: A SNP-aware DNA foundation model to capture variant effects",
      "abstract": "Large language models (LLMs) trained on text demonstrated remarkable results on natural language processing (NLP) tasks. These models have been adapted to decipher the language of DNA, where sequences of nucleotides act as \"words\" that encode genomic functions. However, the genome differs fundamentally from natural language, as it lacks clearly defined words or a consistent grammar. Although DNA language models (DNALMs) such as DNABERT, GENA-LM have achieved high level of performance on genome-related biological tasks, these models do not encode biological functions in the presence of sequence variations. To address this problem, we pre-train foundation models that effectively integrate sequence variations, in particular Single Nucleotide Polymorphisms (SNPs), as they underlie important biological functions. Specifically, we use ModernBERT to pre-train two different Biomedical Foundation Models (BMFM), namely, BMFM-DNA-REF in which the model is trained with sequences of varying lengths along with their reverse complements derived from the reference genome and BMFM-DNA-SNP in which the model is trained with sequences created using a novel representation scheme that encodes sequence variations. Our findings indicate that integrating sequence variations into DNALMs helps capture the biological functions as seen in improvements on all fine-tuning tasks. To explore the model's practical utility, we experimented with various strategies for SNP imputation on promoter detection task introduced in DNABERT-2. However, we acknowledge that the current benchmarks are limited in their ability to fully evaluate these models. To enable more comprehensive assessment in the future and encourage community contributions, we release our models through HuggingFace and the code to reproduce the results at https://github.com/BiomedSciAI/biomed-multi-omic",
      "authors": "Hongyang Li; Sanjoy Dey; Bum Chul Kwon; Michael Danziger; Michal Rosen-Tzvi; Jianying Hu; James Kozloski; Ching-Huei Tsou; Bharath Dandala; Pablo Meyer",
      "date": "2025-06-26",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.17766v1",
      "doi": "",
      "title": "Improving Genomic Models via Task-Specific Self-Pretraining",
      "abstract": "Pretraining DNA language models (DNALMs) on the full human genome is resource-intensive, yet often considered necessary for strong downstream performance. Inspired by recent findings in NLP and long-context modeling, we explore an alternative: self-pretraining on task-specific, unlabeled data. Using the BEND benchmark, we show that DNALMs trained with self-pretraining match or exceed the performance of models trained from scratch under identical compute. While genome-scale pretraining may still offer higher absolute performance, task-specific self-pretraining provides a practical and compute-efficient strategy for building stronger supervised baselines.",
      "authors": "Sohan Mupparapu; Parameswari Krishnamurthy; Ratish Puduppully",
      "date": "2025-06-21",
      "year": "2025",
      "categories": [
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2507.02877v1",
      "doi": "",
      "title": "AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and Scalable Circular Genome Visualizations",
      "abstract": "Circular genome visualizations are essential for exploring structural variants and gene regulation. However, existing tools often require complex scripting and manual configuration, making the process time-consuming, error-prone, and difficult to learn. To address these challenges, we introduce AuraGenome, an LLM-powered framework for rapid, reusable, and scalable generation of multi-layered circular genome visualizations. AuraGenome combines a semantic-driven multi-agent workflow with an interactive visual analytics system. The workflow employs seven specialized LLM-driven agents, each assigned distinct roles such as intent recognition, layout planning, and code generation, to transform raw genomic data into tailored visualizations. The system supports multiple coordinated views tailored for genomic data, offering ring, radial, and chord-based layouts to represent multi-layered circular genome visualizations. In addition to enabling interactions and configuration reuse, the system supports real-time refinement and high-quality report export. We validate its effectiveness through two case studies and a comprehensive user study. AuraGenome is available at: https://github.com/Darius18/AuraGenome.",
      "authors": "Chi Zhang; Yu Dong; Yang Wang; Yuetong Han; Guihua Shan; Bixia Tang",
      "date": "2025-06-18",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.GR",
        "cs.HC"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.14861v1",
      "doi": "",
      "title": "BMFM-RNA: An Open Framework for Building and Evaluating Transcriptomic Foundation Models",
      "abstract": "Transcriptomic foundation models (TFMs) have recently emerged as powerful tools for analyzing gene expression in cells and tissues, supporting key tasks such as cell-type annotation, batch correction, and perturbation prediction. However, the diversity of model implementations and training strategies across recent TFMs, though promising, makes it challenging to isolate the contribution of individual design choices or evaluate their potential synergies. This hinders the field's ability to converge on best practices and limits the reproducibility of insights across studies. We present BMFM-RNA, an open-source, modular software package that unifies diverse TFM pretraining and fine-tuning objectives within a single framework. Leveraging this capability, we introduce a novel training objective, whole cell expression decoder (WCED), which captures global expression patterns using an autoencoder-like CLS bottleneck representation. In this paper, we describe the framework, supported input representations, and training objectives. We evaluated four model checkpoints pretrained on CELLxGENE using combinations of masked language modeling (MLM), WCED and multitask learning. Using the benchmarking capabilities of BMFM-RNA, we show that WCED-based models achieve performance that matches or exceeds state-of-the-art approaches like scGPT across more than a dozen datasets in both zero-shot and fine-tuning tasks. BMFM-RNA, available as part of the biomed-multi-omics project ( https://github.com/BiomedSciAI/biomed-multi-omic ), offers a reproducible foundation for systematic benchmarking and community-driven exploration of optimal TFM training strategies, enabling the development of more effective tools to leverage the latest advances in AI for understanding cell biology.",
      "authors": "Bharath Dandala; Michael M. Danziger; Ella Barkan; Tanwi Biswas; Viatcheslav Gurev; Jianying Hu; Matthew Madgwick; Akira Koseki; Tal Kozlovski; Michal Rosen-Zvi; Yishai Shimoni; Ching-Huei Tsou",
      "date": "2025-06-17",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.10271v3",
      "doi": "",
      "title": "Evaluating DNA function understanding in genomic language models using evolutionarily implausible sequences",
      "abstract": "Genomic language models (gLMs) hold promise for generating novel, functional DNA sequences for synthetic biology. However, realizing this potential requires models to go beyond evolutionary plausibility and understand how DNA sequence encodes gene expression and regulation. We introduce a benchmark called Nullsettes, which assesses how well models can predict in silico loss-of-function (LOF) mutations, in synthetic expression cassettes with little evolutionary precedent. Testing 12 state-of-the-art gLMs, we find that most fail to consistently detect these strong LOF mutations. All models show a sharp drop in predictive accuracy as the likelihood assigned to the original (nonmutant) sequence decreases, suggesting that gLMs rely heavily on pattern-matching to their evolutionary prior rather than on any mechanistic understanding of gene expression. Our findings highlight fundamental limitations in how gLMs generalize to engineered, non-natural sequences, and underscore the need for benchmarks and modeling strategies that prioritize functional understanding.",
      "authors": "Shiyu Jiang; Xuyin Liu; Zitong Jerry Wang",
      "date": "2025-06-12",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.08936v1",
      "doi": "",
      "title": "BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models",
      "abstract": "We present BioLangFusion, a simple approach for integrating pre-trained DNA, mRNA, and protein language models into unified molecular representations. Motivated by the central dogma of molecular biology (information flow from gene to transcript to protein), we align per-modality embeddings at the biologically meaningful codon level (three nucleotides encoding one amino acid) to ensure direct cross-modal correspondence. BioLangFusion studies three standard fusion techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized attention pooling inspired by multiple-instance learning, and (iii) cross-modal multi-head attention -- each technique providing a different inductive bias for combining modality-specific signals. These methods require no additional pre-training or modification of the base models, allowing straightforward integration with existing sequence-based foundation models. Across five molecular property prediction tasks, BioLangFusion outperforms strong unimodal baselines, showing that even simple fusion of pre-trained models can capture complementary multi-omic information with minimal overhead.",
      "authors": "Amina Mollaysa; Artem Moskale; Pushpak Pati; Tommaso Mansi; Mangal Prakash; Rui Liao",
      "date": "2025-06-10",
      "year": "2025",
      "categories": [
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.10031v1",
      "doi": "",
      "title": "scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data",
      "abstract": "Self-supervised learning (SSL) has proven to be a powerful approach for extracting biologically meaningful representations from single-cell data. To advance our understanding of SSL methods applied to single-cell data, we present scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL methods. Our evaluation spans nine datasets and focuses on three common downstream tasks: batch correction, cell type annotation, and missing modality prediction. Furthermore, we systematically assess various data augmentation strategies. Our analysis reveals task-specific trade-offs: the specialized single-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at uni-modal batch correction, while generic SSL methods, such as VICReg and SimCLR, demonstrate superior performance in cell typing and multi-modal data integration. Random masking emerges as the most effective augmentation technique across all tasks, surpassing domain-specific augmentations. Notably, our results indicate the need for a specialized single-cell multi-modal data integration framework. scSSL-Bench provides a standardized evaluation platform and concrete recommendations for applying SSL to single-cell analysis, advancing the convergence of deep learning and single-cell genomics.",
      "authors": "Olga Ovcharenko; Florian Barkmann; Philip Toma; Imant Daunhawer; Julia Vogt; Sebastian Schelter; Valentina Boeva",
      "date": "2025-06-10",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.05542v1",
      "doi": "",
      "title": "Agentomics-ML: Autonomous Machine Learning Experimentation Agent for Genomic and Transcriptomic Data",
      "abstract": "The adoption of machine learning (ML) and deep learning methods has revolutionized molecular medicine by driving breakthroughs in genomics, transcriptomics, drug discovery, and biological systems modeling. The increasing quantity, multimodality, and heterogeneity of biological datasets demand automated methods that can produce generalizable predictive models. Recent developments in large language model-based agents have shown promise for automating end-to-end ML experimentation on structured benchmarks. However, when applied to heterogeneous computational biology datasets, these methods struggle with generalization and success rates. Here, we introduce Agentomics-ML, a fully autonomous agent-based system designed to produce a classification model and the necessary files for reproducible training and inference. Our method follows predefined steps of an ML experimentation process, repeatedly interacting with the file system through Bash to complete individual steps. Once an ML model is produced, training and validation metrics provide scalar feedback to a reflection step to identify issues such as overfitting. This step then creates verbal feedback for future iterations, suggesting adjustments to steps such as data representation, model architecture, and hyperparameter choices. We have evaluated Agentomics-ML on several established genomic and transcriptomic benchmark datasets and show that it outperforms existing state-of-the-art agent-based methods in both generalization and success rates. While state-of-the-art models built by domain experts still lead in absolute performance on the majority of the computational biology datasets used in this work, Agentomics-ML narrows the gap for fully autonomous systems and achieves state-of-the-art performance on one of the used benchmark datasets. The code is available at https://github.com/BioGeMT/Agentomics-ML.",
      "authors": "Vlastimil Martinek; Andrea Gariboldi; Dimosthenis Tzimotoudis; Aitor Alberdi Escudero; Edward Blake; David Cechak; Luke Cassar; Alessandro Balestrucci; Panagiotis Alexiou",
      "date": "2025-06-05",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.MA"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.02911v1",
      "doi": "",
      "title": "Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning",
      "abstract": "Cell type annotation is a key task in analyzing the heterogeneity of single-cell RNA sequencing data. Although recent foundation models automate this process, they typically annotate cells independently, without considering batch-level cellular context or providing explanatory reasoning. In contrast, human experts often annotate distinct cell types for different cell clusters based on their domain knowledge. To mimic this workflow, we introduce the CellPuzzles task, where the objective is to assign unique cell types to a batch of cells. This benchmark spans diverse tissues, diseases, and donor conditions, and requires reasoning across the batch-level cellular context to ensure label uniqueness. We find that off-the-shelf large language models (LLMs) struggle on CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0% batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces, followed by reinforcement learning with batch-level rewards. Cell-o1 achieves state-of-the-art performance, outperforming o1 by over 73% and generalizing well across contexts. Further analysis of training dynamics and reasoning behaviors provides insights into batch-level annotation performance and emergent expert-like reasoning. Code and data are available at https://github.com/ncbi-nlp/cell-o1.",
      "authors": "Yin Fang; Qiao Jin; Guangzhi Xiong; Bowen Jin; Xianrui Zhong; Siru Ouyang; Aidong Zhang; Jiawei Han; Zhiyong Lu",
      "date": "2025-06-03",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.HC",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.02212v1",
      "doi": "",
      "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics",
      "abstract": "Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.",
      "authors": "Ella Rannon; David Burstein",
      "date": "2025-06-02",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.01918v1",
      "doi": "",
      "title": "Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis",
      "abstract": "Image mass cytometry (IMC) enables high-dimensional spatial profiling by combining mass cytometry's analytical power with spatial distributions of cell phenotypes. Recent studies leverage large language models (LLMs) to extract cell states by translating gene or protein expression into biological context. However, existing single-cell LLMs face two major challenges: (1) Integration of spatial information: they struggle to generalize spatial coordinates and effectively encode spatial context as text, and (2) Treating each cell independently: they overlook cell-cell interactions, limiting their ability to capture biological relationships. To address these limitations, we propose Spatial2Sentence, a novel framework that integrates single-cell expression and spatial information into natural language using a multi-sentence approach. Spatial2Sentence constructs expression similarity and distance matrices, pairing spatially adjacent and expressionally similar cells as positive pairs while using distant and dissimilar cells as negatives. These multi-sentence representations enable LLMs to learn cellular interactions in both expression and spatial contexts. Equipped with multi-task learning, Spatial2Sentence outperforms existing single-cell LLMs on preprocessed IMC datasets, improving cell-type classification by 5.98% and clinical status prediction by 4.18% on the diabetes dataset while enhancing interpretability. The source code can be found here: https://github.com/UNITES-Lab/Spatial2Sentence.",
      "authors": "Chi-Jane Chen; Yuhang Chen; Sukwon Yun; Natalie Stanley; Tianlong Chen",
      "date": "2025-06-02",
      "year": "2025",
      "categories": [
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.00821v2",
      "doi": "",
      "title": "SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation Models",
      "abstract": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated significant success in variant effect prediction. However, their adversarial robustness remains largely unexplored. To address this gap, we propose SafeGenes: a framework for Secure analysis of genomic foundation models, leveraging adversarial attacks to evaluate robustness against both engineered near-identical adversarial Genes and embedding-space manipulations. In this study, we assess the adversarial vulnerabilities of GFMs using two approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM introduces minimal perturbations to input sequences, while the soft prompt attack optimizes continuous embeddings to manipulate model predictions without modifying the input tokens. By combining these techniques, SafeGenes provides a comprehensive assessment of GFM susceptibility to adversarial manipulation. Targeted soft prompt attacks induced severe degradation in MLM-based shallow architectures such as ProteinBERT, while still producing substantial failure modes even in high-capacity foundation models such as ESM1b and ESM1v. These findings expose critical vulnerabilities in current foundation models, opening new research directions toward improving their security and robustness in high-stakes genomic applications such as variant effect prediction.",
      "authors": "Huixin Zhan; Clovis Barbour; Jason H. Moore",
      "date": "2025-06-01",
      "year": "2025",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2506.00082v1",
      "doi": "",
      "title": "An AI-powered Knowledge Hub for Potato Functional Genomics",
      "abstract": "Potato functional genomics lags due to unsystematic gene information curation, gene identifier inconsistencies across reference genome versions, and the increasing volume of research publications. To address these limitations, we developed the Potato Knowledge Hub (http://www.potato-ai.top), leveraging Large Language Models (LLMs) and a systematically curated collection of over 3,200 high-quality potato research papers spanning over 120 years. This platform integrates two key modules: a functional gene database containing 2,571 literature-reported genes, meticulously mapped to the latest DMv8.1 reference genome with resolved nomenclature discrepancies and links to original publications; and a potato knowledge base. The knowledge base, built using a Retrieval-Augmented Generation (RAG) architecture, accurately answers research queries with literature citations, mitigating LLM \"hallucination.\" Users can interact with the hub via a natural language AI agent, \"Potato Research Assistant,\" for querying specialized knowledge, retrieving gene information, and extracting sequences. The continuously updated Potato Knowledge Hub aims to be a comprehensive resource, fostering advancements in potato functional genomics and supporting breeding programs.",
      "authors": "Jia Yuxin; Li Jinye; Jia Yudong; Li Futing; Su Xiaoqi; Luo Jilin; Dong Yarui; Sun Chunyan; Cui Qinghan; Wang Li; Li Axiu; Shang Yi; Zhu Yujuan; Huang Sanwen",
      "date": "2025-05-30",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.DB"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2505.21317v1",
      "doi": "",
      "title": "A Cross Modal Knowledge Distillation & Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features",
      "abstract": "Understanding cellular responses to stimuli is crucial for biological discovery and drug development. Transcriptomics provides interpretable, gene-level insights, while microscopy imaging offers rich predictive features but is harder to interpret. Weakly paired datasets, where samples share biological states, enable multimodal learning but are scarce, limiting their utility for training and multimodal inference. We propose a framework to enhance transcriptomics by distilling knowledge from microscopy images. Using weakly paired data, our method aligns and binds modalities, enriching gene expression representations with morphological information. To address data scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal distillation using pretrained foundation models, achieving state-of-the-art results, and (2) PEA (Perturbation Embedding Augmentation), a novel augmentation technique that enhances transcriptomics data while preserving inherent biological information. These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.",
      "authors": "Ihab Bendidi; Yassir El Mesbahi; Alisandra K. Denton; Karush Suri; Kian Kenyon-Dean; Auguste Genovesio; Emmanuel Noutahi",
      "date": "2025-05-27",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2505.20836v1",
      "doi": "",
      "title": "HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling",
      "abstract": "Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.",
      "authors": "Hexiong Yang; Mingrui Chen; Huaibo Huang; Junxian Duan; Jie Cao; Zhen Zhou; Ran He",
      "date": "2025-05-27",
      "year": "2025",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2505.10729v1",
      "doi": "",
      "title": "Adaptive Spatial Transcriptomics Interpolation via Cross-modal Cross-slice Modeling",
      "abstract": "Spatial transcriptomics (ST) is a promising technique that characterizes the spatial gene profiling patterns within the tissue context. Comprehensive ST analysis depends on consecutive slices for 3D spatial insights, whereas the missing intermediate tissue sections and high costs limit the practical feasibility of generating multi-slice ST. In this paper, we propose C2-STi, the first attempt for interpolating missing ST slices at arbitrary intermediate positions between adjacent ST slices. Despite intuitive, effective ST interpolation presents significant challenges, including 1) limited continuity across heterogeneous tissue sections, 2) complex intrinsic correlation across genes, and 3) intricate cellular structures and biological semantics within each tissue section. To mitigate these challenges, in C2-STi, we design 1) a distance-aware local structural modulation module to adaptively capture cross-slice deformations and enhance positional correlations between ST slices, 2) a pyramid gene co-expression correlation module to capture multi-scale biological associations among genes, and 3) a cross-modal alignment module that integrates the ST-paired hematoxylin and eosin (H&E)-stained images to filter and align the essential cellular features across ST and H\\&E images. Extensive experiments on the public dataset demonstrate our superiority over state-of-the-art approaches on both single-slice and multi-slice ST interpolation. Codes are available at https://github.com/XiaofeiWang2018/C2-STi.",
      "authors": "NingFeng Que; Xiaofei Wang; Jingjing Chen; Yixuan Jiang; Chao Li",
      "date": "2025-05-15",
      "year": "2025",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2505.07896v1",
      "doi": "",
      "title": "Bridging Large Language Models and Single-Cell Transcriptomics in Dissecting Selective Motor Neuron Vulnerability",
      "abstract": "Understanding cell identity and function through single-cell level sequencing data remains a key challenge in computational biology. We present a novel framework that leverages gene-specific textual annotations from the NCBI Gene database to generate biologically contextualized cell embeddings. For each cell in a single-cell RNA sequencing (scRNA-seq) dataset, we rank genes by expression level, retrieve their NCBI Gene descriptions, and transform these descriptions into vector embedding representations using large language models (LLMs). The models used include OpenAI text-embedding-ada-002, text-embedding-3-small, and text-embedding-3-large (Jan 2024), as well as domain-specific models BioBERT and SciBERT. Embeddings are computed via an expression-weighted average across the top N most highly expressed genes in each cell, providing a compact, semantically rich representation. This multimodal strategy bridges structured biological data with state-of-the-art language modeling, enabling more interpretable downstream applications such as cell-type clustering, cell vulnerability dissection, and trajectory inference.",
      "authors": "Douglas Jiang; Zilin Dai; Luxuan Zhang; Qiyi Yu; Haoqi Sun; Feng Tian",
      "date": "2025-05-12",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2505.07865v1",
      "doi": "",
      "title": "CellVerse: Do Large Language Models Really Understand Cell Biology?",
      "abstract": "Recent studies have demonstrated the feasibility of modeling single-cell data as natural languages and the potential of leveraging powerful large language models (LLMs) for understanding cell biology. However, a comprehensive evaluation of LLMs' performance on language-driven single-cell analysis tasks still remains unexplored. Motivated by this challenge, we introduce CellVerse, a unified language-centric question-answering benchmark that integrates four types of single-cell multi-omics data and encompasses three hierarchical levels of single-cell analysis tasks: cell type annotation (cell-level), drug response prediction (drug-level), and perturbation analysis (gene-level). Going beyond this, we systematically evaluate the performance across 14 open-source and closed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the experimental results reveal: (1) Existing specialist models (C2S-Pythia) fail to make reasonable decisions across all sub-tasks within CellVerse, while generalist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit preliminary understanding capabilities within the realm of cell biology. (2) The performance of current LLMs falls short of expectations and has substantial room for improvement. Notably, in the widely studied drug response prediction task, none of the evaluated LLMs demonstrate significant performance improvement over random guessing. CellVerse offers the first large-scale empirical demonstration that significant challenges still remain in applying LLMs to cell biology. By introducing CellVerse, we lay the foundation for advancing cell biology through natural languages and hope this paradigm could facilitate next-generation single-cell analysis.",
      "authors": "Fan Zhang; Tianyu Liu; Zhihong Zhu; Hao Wu; Haixin Wang; Donghao Zhou; Yefeng Zheng; Kun Wang; Xian Wu; Pheng-Ann Heng",
      "date": "2025-05-09",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.CL",
        "q-bio.CB"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2505.05612v1",
      "doi": "",
      "title": "scDrugMap: Benchmarking Large Foundation Models for Drug Response Prediction",
      "abstract": "Drug resistance presents a major challenge in cancer therapy. Single cell profiling offers insights into cellular heterogeneity, yet the application of large-scale foundation models for predicting drug response in single cell data remains underexplored. To address this, we developed scDrugMap, an integrated framework featuring both a Python command-line interface and a web server for drug response prediction. scDrugMap evaluates a wide range of foundation models, including eight single-cell models and two large language models, using a curated dataset of over 326,000 cells in the primary collection and 18,800 cells in the validation set, spanning 36 datasets and diverse tissue and cancer types. We benchmarked model performance under pooled-data and cross-data evaluation settings, employing both layer freezing and Low-Rank Adaptation (LoRA) fine-tuning strategies. In the pooled-data scenario, scFoundation achieved the best performance, with mean F1 scores of 0.971 (layer freezing) and 0.947 (fine-tuning), outperforming the lowest-performing model by over 50%. In the cross-data setting, UCE excelled post fine-tuning (mean F1: 0.774), while scGPT led in zero-shot learning (mean F1: 0.858). Overall, scDrugMap provides the first large-scale benchmark of foundation models for drug response prediction in single-cell data and serves as a user-friendly, flexible platform for advancing drug discovery and translational research.",
      "authors": "Qing Wang; Yining Pan; Minghao Zhou; Zijia Tang; Yanfei Wang; Guangyu Wang; Qianqian Song",
      "date": "2025-05-08",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.LG",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2505.05577v2",
      "doi": "",
      "title": "PyTDC: A multimodal machine learning training, evaluation, and inference platform for biomedical foundation models",
      "abstract": "Existing biomedical benchmarks do not provide end-to-end infrastructure for training, evaluation, and inference of models that integrate multimodal biological data and a broad range of machine learning tasks in therapeutics. We present PyTDC, an open-source machine-learning platform providing streamlined training, evaluation, and inference software for multimodal biological AI models. PyTDC unifies distributed, heterogeneous, continuously updated data sources and model weights and standardizes benchmarking and inference endpoints. This paper discusses the components of PyTDC's architecture and, to our knowledge, the first-of-its-kind case study on the introduced single-cell drug-target nomination ML task. We find state-of-the-art methods in graph representation learning and domain-specific methods from graph theory perform poorly on this task. Though we find a context-aware geometric deep learning method that outperforms the evaluated SoTA and domain-specific baseline methods, the model is unable to generalize to unseen cell types or incorporate additional modalities, highlighting PyTDC's capacity to facilitate an exciting avenue of research developing multimodal, context-aware, foundation models for open problems in biomedical AI.",
      "authors": "Alejandro Velez-Arce; Jesus Caraballo; Marinka Zitnik",
      "date": "2025-05-08",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2505.00017v1",
      "doi": "",
      "title": "ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation",
      "abstract": "To enable precise and fully automated cell type annotation with large language models (LLMs), we developed a graph structured feature marker database to retrieve entities linked to differential genes for cell reconstruction. We further designed a multi task workflow to optimize the annotation process. Compared to general purpose LLMs, our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while more closely aligning with the cognitive logic of manual annotation.",
      "authors": "Dezheng Han; Yibin Jia; Ruxiao Chen; Wenjie Han; Shuaishuai Guo; Jianbo Wang",
      "date": "2025-04-24",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DB",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2504.11610v1",
      "doi": "",
      "title": "Generalized probabilistic canonical correlation analysis for multi-modal data integration with full or partial observations",
      "abstract": "Background: The integration and analysis of multi-modal data are increasingly essential across various domains including bioinformatics. As the volume and complexity of such data grow, there is a pressing need for computational models that not only integrate diverse modalities but also leverage their complementary information to improve clustering accuracy and insights, especially when dealing with partial observations with missing data. Results: We propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an unsupervised method for the integration and joint dimensionality reduction of multi-modal data. GPCCA addresses key challenges in multi-modal data analysis by handling missing values within the model, enabling the integration of more than two modalities, and identifying informative features while accounting for correlations within individual modalities. The model demonstrates robustness to various missing data patterns and provides low-dimensional embeddings that facilitate downstream clustering and analysis. In a range of simulation settings, GPCCA outperforms existing methods in capturing essential patterns across modalities. Additionally, we demonstrate its applicability to multi-omics data from TCGA cancer datasets and a multi-view image dataset. Conclusion: GPCCA offers a useful framework for multi-modal data integration, effectively handling missing data and providing informative low-dimensional embeddings. Its performance across cancer genomics and multi-view image data highlights its robustness and potential for broad application. To make the method accessible to the wider research community, we have released an R package, GPCCA, which is available at https://github.com/Kaversoniano/GPCCA.",
      "authors": "Tianjian Yang; Wei Vivian Li",
      "date": "2025-04-15",
      "year": "2025",
      "categories": [
        "stat.ML",
        "cs.LG",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2504.09704v1",
      "doi": "",
      "title": "Transformer-Based Representation Learning for Robust Gene Expression Modeling and Cancer Prognosis",
      "abstract": "Transformer-based models have achieved remarkable success in natural language and vision tasks, but their application to gene expression analysis remains limited due to data sparsity, high dimensionality, and missing values. We present GexBERT, a transformer-based autoencoder framework for robust representation learning of gene expression data. GexBERT learns context-aware gene embeddings by pretraining on large-scale transcriptomic profiles with a masking and restoration objective that captures co-expression relationships among thousands of genes. We evaluate GexBERT across three critical tasks in cancer research: pan-cancer classification, cancer-specific survival prediction, and missing value imputation. GexBERT achieves state-of-the-art classification accuracy from limited gene subsets, improves survival prediction by restoring expression of prognostic anchor genes, and outperforms conventional imputation methods under high missingness. Furthermore, its attention-based interpretability reveals biologically meaningful gene patterns across cancer types. These findings demonstrate the utility of GexBERT as a scalable and effective tool for gene expression modeling, with translational potential in settings where gene coverage is limited or incomplete.",
      "authors": "Shuai Jiang; Saeed Hassanpour",
      "date": "2025-04-13",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2504.09060v2",
      "doi": "",
      "title": "Multimodal 3D Genome Pre-training",
      "abstract": "Deep learning techniques have driven significant progress in various analytical tasks within 3D genomics in computational biology. However, a holistic understanding of 3D genomics knowledge remains underexplored. Here, we propose MIX-HIC, the first multimodal foundation model of 3D genome that integrates both 3D genome structure and epigenomic tracks, which obtains unified and comprehensive semantics. For accurate heterogeneous semantic fusion, we design the cross-modal interaction and mapping blocks for robust unified representation, yielding the accurate aggregation of 3D genome knowledge. Besides, we introduce the first large-scale dataset comprising over 1 million pairwise samples of Hi-C contact maps and epigenomic tracks for high-quality pre-training, enabling the exploration of functional implications in 3D genomics. Extensive experiments show that MIX-HIC can significantly surpass existing state-of-the-art methods in diverse downstream tasks. This work provides a valuable resource for advancing 3D genomics research.",
      "authors": "Minghao Yang; Pengteng Li; Yan Liang; Qianyi Cai; Zhihang Zheng; Shichen Zhang; Pengfei Zhang; Zhi-An Huang; Hui Xiong",
      "date": "2025-04-12",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2504.04698v1",
      "doi": "",
      "title": "scAgent: Universal Single-Cell Annotation via a LLM Agent",
      "abstract": "Cell type annotation is critical for understanding cellular heterogeneity. Based on single-cell RNA-seq data and deep learning models, good progress has been made in annotating a fixed number of cell types within a specific tissue. However, universal cell annotation, which can generalize across tissues, discover novel cell types, and extend to novel cell types, remains less explored. To fill this gap, this paper proposes scAgent, a universal cell annotation framework based on Large Language Models (LLMs). scAgent can identify cell types and discover novel cell types in diverse tissues; furthermore, it is data efficient to learn novel cell types. Experimental studies in 160 cell types and 35 tissues demonstrate the superior performance of scAgent in general cell-type annotation, novel cell discovery, and extensibility to novel cell type.",
      "authors": "Yuren Mao; Yu Mi; Peigen Liu; Mengfei Zhang; Hanqing Liu; Yunjun Gao",
      "date": "2025-04-07",
      "year": "2025",
      "categories": [
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2504.03976v2",
      "doi": "",
      "title": "OLAF: An Open Life Science Analysis Framework for Conversational Bioinformatics Powered by Large Language Models",
      "abstract": "OLAF (Open Life Science Analysis Framework) is an open-source platform that enables researchers to perform bioinformatics analyses using natural language. By combining large language models (LLMs) with a modular agent-pipe-router architecture, OLAF generates and executes bioinformatics code on real scientific data, including formats like .h5ad. The system includes an Angular front end and a Python/Firebase backend, allowing users to run analyses such as single-cell RNA-seq workflows, gene annotation, and data visualization through a simple web interface. Unlike general-purpose AI tools, OLAF integrates code execution, data handling, and scientific libraries in a reproducible, user-friendly environment. It is designed to lower the barrier to computational biology for non-programmers and support transparent, AI-powered life science research.",
      "authors": "Dylan Riffle; Nima Shirooni; Cody He; Manush Murali; Sovit Nayak; Rishikumar Gopalan; Diego Gonzalez Lopez",
      "date": "2025-04-04",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2504.00020v2",
      "doi": "",
      "title": "Celler:A Genomic Language Model for Long-Tailed Single-Cell Annotation",
      "abstract": "Recent breakthroughs in single-cell technology have ushered in unparalleled opportunities to decode the molecular intricacy of intricate biological systems, especially those linked to diseases unique to humans. However, these progressions have also ushered in novel obstacles-specifically, the efficient annotation of extensive, long-tailed single-cell data pertaining to disease conditions. To effectively surmount this challenge, we introduce Celler, a state-of-the-art generative pre-training model crafted specifically for the annotation of single-cell data. Celler incorporates two groundbreaking elements: First, we introduced the Gaussian Inflation (GInf) Loss function. By dynamically adjusting sample weights, GInf Loss significantly enhances the model's ability to learn from rare categories while reducing the risk of overfitting for common categories. Secondly, we introduce an innovative Hard Data Mining (HDM) strategy into the training process, specifically targeting the challenging-to-learn minority data samples, which significantly improved the model's predictive accuracy. Additionally, to further advance research in this field, we have constructed a large-scale single-cell dataset: Celler-75, which encompasses 40 million cells distributed across 80 human tissues and 75 specific diseases. This dataset provides critical support for comprehensively exploring the potential of single-cell technology in disease research. Our code is available at https://github.com/AI4science-ym/HiCeller.",
      "authors": "Huan Zhao; Yiming Liu; Jina Yao; Ling Xiong; Zexin Zhou; Zixing Zhang",
      "date": "2025-03-28",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2503.20278v2",
      "doi": "https://doi.org/10.1093/bioinformatics/btaf595",
      "title": "The cell as a token: high-dimensional geometry in language models and cell embeddings",
      "abstract": "Single-cell sequencing technology maps cells to a high-dimensional space encoding their internal activity. Recently-proposed virtual cell models extend this concept, enriching cells' representations based on patterns learned from pretraining on vast cell atlases. This review explores how advances in understanding the structure of natural language embeddings informs ongoing efforts to analyze single-cell datasets. Both fields process unstructured data by partitioning datasets into tokens embedded within a high-dimensional vector space. We discuss how the context of tokens influences the geometry of embedding space, and how low-dimensional manifolds shape this space's robustness and interpretation. We highlight how new developments in foundation models for language, such as interpretability probes and in-context reasoning, can inform efforts to construct cell atlases and train virtual cell models.",
      "authors": "William Gilpin",
      "date": "2025-03-26",
      "year": "2025",
      "categories": [
        "q-bio.QM",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2503.09427v4",
      "doi": "",
      "title": "Language-Enhanced Representation Learning for Single-Cell Transcriptomics",
      "abstract": "Single-cell RNA sequencing (scRNA-seq) offers detailed insights into cellular heterogeneity. Recent advancements leverage single-cell large language models (scLLMs) for effective representation learning. These models focus exclusively on transcriptomic data, neglecting complementary biological knowledge from textual descriptions. To overcome this limitation, we propose scMMGPT, a novel multimodal framework designed for language-enhanced representation learning in single-cell transcriptomics. Unlike existing methods, scMMGPT employs robust cell representation extraction, preserving quantitative gene expression data, and introduces an innovative two-stage pre-training strategy combining discriminative precision with generative flexibility. Extensive experiments demonstrate that scMMGPT significantly outperforms unimodal and multimodal baselines across key downstream tasks, including cell annotation and clustering, and exhibits superior generalization in out-of-distribution scenarios.",
      "authors": "Yaorui Shi; Jiaqi Yang; Changhao Nai; Sihang Li; Junfeng Fang; Xiang Wang; Zhiyuan Liu; Yang Zhang",
      "date": "2025-03-12",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2503.04490v2",
      "doi": "",
      "title": "Large Language Models in Bioinformatics: A Survey",
      "abstract": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling advanced analysis of DNA, RNA, proteins, and single-cell data. This survey provides a systematic review of recent advancements, focusing on genomic sequence modeling, RNA structure prediction, protein function inference, and single-cell transcriptomics. Meanwhile, we also discuss several key challenges, including data scarcity, computational complexity, and cross-omics integration, and explore future directions such as multimodal learning, hybrid AI models, and clinical applications. By offering a comprehensive perspective, this paper underscores the transformative potential of LLMs in driving innovations in bioinformatics and precision medicine.",
      "authors": "Zhenyu Wang; Zikang Wang; Jiyue Jiang; Pengan Chen; Xiangyu Shi; Yu Li",
      "date": "2025-03-06",
      "year": "2025",
      "categories": [
        "cs.CL",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2503.03773v1",
      "doi": "",
      "title": "A Phylogenetic Approach to Genomic Language Modeling",
      "abstract": "Genomic language models (gLMs) have shown mostly modest success in identifying evolutionarily constrained elements in mammalian genomes. To address this issue, we introduce a novel framework for training gLMs that explicitly models nucleotide evolution on phylogenetic trees using multispecies whole-genome alignments. Our approach integrates an alignment into the loss function during training but does not require it for making predictions, thereby enhancing the model's applicability. We applied this framework to train PhyloGPN, a model that excels at predicting functionally disruptive variants from a single sequence alone and demonstrates strong transfer learning capabilities.",
      "authors": "Carlos Albors; Jianan Canal Li; Gonzalo Benegas; Chengzhong Ye; Yun S. Song",
      "date": "2025-03-04",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2503.00804v1",
      "doi": "",
      "title": "DELST: Dual Entailment Learning for Hyperbolic Image-Gene Pretraining in Spatial Transcriptomics",
      "abstract": "Spatial transcriptomics (ST) maps gene expression within tissue at individual spots, making it a valuable resource for multimodal representation learning. Additionally, ST inherently contains rich hierarchical information both across and within modalities. For instance, different spots exhibit varying numbers of nonzero gene expressions, corresponding to different levels of cellular activity and semantic hierarchies. However, existing methods rely on contrastive alignment of image-gene pairs, failing to accurately capture the intricate hierarchical relationships in ST data. Here, we propose DELST, the first framework to embed hyperbolic representations while modeling hierarchy for image-gene pretraining at two levels: (1) Cross-modal entailment learning, which establishes an order relationship between genes and images to enhance image representation generalization; (2) Intra-modal entailment learning, which encodes gene expression patterns as hierarchical relationships, guiding hierarchical learning across different samples at a global scale and integrating biological insights into single-modal representations. Extensive experiments on ST benchmarks annotated by pathologists demonstrate the effectiveness of our framework, achieving improved predictive performance compared to existing methods. Our code and models are available at: https://github.com/XulinChen/DELST.",
      "authors": "Xulin Chen; Junzhou Huang",
      "date": "2025-03-02",
      "year": "2025",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2502.15109v4",
      "doi": "",
      "title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models",
      "abstract": "Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce SOCIAL GENOME, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. SOCIAL GENOME contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). SOCIAL GENOME is also the first modeling challenge to study external knowledge in social reasoning. SOCIAL GENOME computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of SOCIAL GENOME through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models.",
      "authors": "Leena Mathur; Marian Qian; Paul Pu Liang; Louis-Philippe Morency",
      "date": "2025-02-21",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2502.07272v5",
      "doi": "",
      "title": "GENERator: A Long-Context Generative Genomic Foundation Model",
      "abstract": "The rapid advancement of DNA sequencing has produced vast genomic datasets, yet interpreting and engineering genomic function remain fundamental challenges. Recent large language models have opened new avenues for genomic analysis, but existing approaches are often limited by restricted training scope, constrained generative capability, or prohibitive computational cost. We introduce GENErator, a generative genomic foundation model for long-context DNA modeling, with a context length of 98k nucleotides, pre-trained on 386 billion nucleotides of eukaryotic DNA. Without task-specific fine-tuning, GENERator exhibits strong intrinsic capabilities: unsupervised embedding analyses reveal phylogenetically coherent structure, and sequence recovery benchmarks demonstrate generative accuracy comparable to or exceeding state-of-the-art models with substantially improved computational efficiency. In a zero-shot setting, GENERator achieves competitive variant effect prediction performance relative to alignment-based methods, while remaining fully alignment-free and broadly applicable across species. With task-specific fine-tuning, the model attains leading performance on established genomic benchmarks. We further demonstrate practical generative applications. GENERator can generate protein-coding DNA sequences that translate into structurally plausible proteins and, through a prompt-guided design framework, design cis-regulatory elements with targeted activity profiles, including synthetic super-enhancers validated by high-throughput UMI-STARR-seq assays. Together, these results establish GENERator as an efficient and biologically grounded framework for genomic interpretation and programmable sequence design. Code and supplementary resources are available at https://github.com/GenerTeam/GENERator.",
      "authors": "Wei Wu; Qiuyi Li; Yuanyuan Zhang; Zhihao Zhan; Ruipu Chen; Mingyang Li; Kun Fu; Junyan Qi; Yongzhou Bao; Chao Wang; Yiheng Zhu; Zhiyun Zhang; Jian Tang; Fuli Feng; Jieping Ye; Yuwen Liu; Hui Xiong; Zheng Wang",
      "date": "2025-02-11",
      "year": "2025",
      "categories": [
        "cs.CL",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2502.03499v1",
      "doi": "",
      "title": "Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and Multi-Task Learning",
      "abstract": "Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model sizes grow. Moreover, existing GFMs are constrained by rigid output formats, limiting their applicability to various genomic tasks. In this work, we revisit the transformer-based auto-regressive models and introduce Omni-DNA, a family of cross-modal multi-task models ranging from 20 million to 1 billion parameters. Our approach consists of two stages: (i) pretraining on DNA sequences with next token prediction objective, and (ii) expanding the multi-modal task-specific tokens and finetuning for multiple downstream tasks simultaneously. When evaluated on the Nucleotide Transformer and GB benchmarks, Omni-DNA achieves state-of-the-art performance on 18 out of 26 tasks. Through multi-task finetuning, Omni-DNA addresses 10 acetylation and methylation tasks at once, surpassing models trained on each task individually. Finally, we design two complex genomic tasks, DNA2Function and Needle-in-DNA, which map DNA sequences to textual functional descriptions and images, respectively, indicating Omni-DNA's cross-modal capabilities to broaden the scope of genomic applications. All the models are available through https://huggingface.co/collections/zehui127",
      "authors": "Zehui Li; Vallijah Subasri; Yifei Shen; Dongsheng Li; Yiren Zhao; Guy-Bart Stan; Caihua Shan",
      "date": "2025-02-05",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2502.00568v4",
      "doi": "",
      "title": "Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions",
      "abstract": "Emerging research has highlighted that artificial intelligence-based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction. However, such direct fusion is impractical in clinical settings, where histopathology remains the gold standard and transcriptomic tests are rarely requested in public healthcare. We experiment on two publicly available multimodal datasets, The Cancer Genomic Atlas and the Clinical Proteomic Tumor Analysis Consortium, spanning four independent cohorts: glioma-glioblastoma, renal, uterine, and breast, and observe significant performance gains in gradation and risk estimation (p-value<0.05) when incorporating synthesized transcriptomic data with WSIs. Also, predictions using synthesized features were statistically close to those obtained with real transcriptomic data (p-value>0.05), consistently across cohorts. Here we show that with our diffusion based crossmodal generative AI model, PathGen, gene expressions synthesized from digital histopathology jointly predict cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed co-attention maps). PathGen code is available for open use on GitHub at https://github.com/Samiran-Dey/PathGen.",
      "authors": "Samiran Dey; Christopher R. S. Banerji; Partha Basuchowdhuri; Sanjoy K. Saha; Deepak Parashar; Tapabrata Chakraborti",
      "date": "2025-02-01",
      "year": "2025",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2501.17729v1",
      "doi": "",
      "title": "A technical review of multi-omics data integration methods: from classical statistical to deep generative approaches",
      "abstract": "The rapid advancement of high-throughput sequencing and other assay technologies has resulted in the generation of large and complex multi-omics datasets, offering unprecedented opportunities for advancing precision medicine strategies. However, multi-omics data integration presents significant challenges due to the high dimensionality, heterogeneity, experimental gaps, and frequency of missing values across data types. Computational methods have been developed to address these issues, employing statistical and machine learning approaches to uncover complex biological patterns and provide deeper insights into our understanding of disease mechanisms. Here, we comprehensively review state-of-the-art multi-omics data integration methods with a focus on deep generative models, particularly variational autoencoders (VAEs) that have been widely used for data imputation and augmentation, joint embedding creation, and batch effect correction. We explore the technical aspects of loss functions and regularisation techniques including adversarial training, disentanglement and contrastive learning. Moreover, we discuss recent advancements in foundation models and the integration of emerging data modalities, while describing the current limitations and outlining future directions for enhancing multi-modal methodologies in biomedical research.",
      "authors": "Ana R. Baião; Zhaoxiang Cai; Rebecca C Poulos; Phillip J. Robinson; Roger R Reddel; Qing Zhong; Susana Vinga; Emanuel Gonçalves",
      "date": "2025-01-29",
      "year": "2025",
      "categories": [
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2501.16361v1",
      "doi": "",
      "title": "Large Language Models Meet Graph Neural Networks for Text-Numeric Graph Reasoning",
      "abstract": "In real-world scientific discovery, human beings always make use of the accumulated prior knowledge with imagination pick select one or a few most promising hypotheses from large and noisy data analysis results. In this study, we introduce a new type of graph structure, the text-numeric graph (TNG), which is defined as graph entities and associations have both text-attributed information and numeric information. The TNG is an ideal data structure model for novel scientific discovery via graph reasoning because it integrates human-understandable textual annotations or prior knowledge, with numeric values that represent the observed or activation levels of graph entities or associations in different samples. Together both the textual information and numeric values determine the importance of graph entities and associations in graph reasoning for novel scientific knowledge discovery. We further propose integrating large language models (LLMs) and graph neural networks (GNNs) to analyze the TNGs for graph understanding and reasoning. To demonstrate the utility, we generated the text-omic(numeric) signaling graphs (TOSG), as one type of TNGs, in which all graphs have the same entities, associations and annotations, but have sample-specific entity numeric (omic) values using single cell RNAseq (scRNAseq) datasets of different diseases. We proposed joint LLM-GNN models for key entity mining and signaling pathway mining on the TOSGs. The evaluation results showed the LLM-GNN and TNGs models significantly improve classification accuracy and network inference. In conclusion, the TNGs and joint LLM-GNN models are important approaches for scientific discovery.",
      "authors": "Haoran Song; Jiarui Feng; Guangfu Li; Michael Province; Philip Payne; Yixin Chen; Fuhai Li",
      "date": "2025-01-21",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2501.08187v2",
      "doi": "",
      "title": "A Multi-Modal AI Copilot for Single-Cell Analysis with Instruction Following",
      "abstract": "Large language models excel at interpreting complex natural language instructions, enabling them to perform a wide range of tasks. In the life sciences, single-cell RNA sequencing (scRNA-seq) data serves as the \"language of cellular biology\", capturing intricate gene expression patterns at the single-cell level. However, interacting with this \"language\" through conventional tools is often inefficient and unintuitive, posing challenges for researchers. To address these limitations, we present InstructCell, a multi-modal AI copilot that leverages natural language as a medium for more direct and flexible single-cell analysis. We construct a comprehensive multi-modal instruction dataset that pairs text-based instructions with scRNA-seq profiles from diverse tissues and species. Building on this, we develop a multi-modal cell language architecture capable of simultaneously interpreting and processing both modalities. InstructCell empowers researchers to accomplish critical tasks-such as cell type annotation, conditional pseudo-cell generation, and drug sensitivity prediction-using straightforward natural language commands. Extensive evaluations demonstrate that InstructCell consistently meets or exceeds the performance of existing single-cell foundation models, while adapting to diverse experimental conditions. More importantly, InstructCell provides an accessible and intuitive tool for exploring complex single-cell data, lowering technical barriers and enabling deeper biological insights.",
      "authors": "Yin Fang; Xinle Deng; Kangwei Liu; Ningyu Zhang; Jingyang Qian; Penghui Yang; Xiaohui Fan; Huajun Chen",
      "date": "2025-01-14",
      "year": "2025",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.HC",
        "cs.LG",
        "q-bio.CB"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2501.07737v1",
      "doi": "",
      "title": "Multi-megabase scale genome interpretation with genetic language models",
      "abstract": "Understanding how molecular changes caused by genetic variation drive disease risk is crucial for deciphering disease mechanisms. However, interpreting genome sequences is challenging because of the vast size of the human genome, and because its consequences manifest across a wide range of cells, tissues and scales -- spanning from molecular to whole organism level. Here, we present Phenformer, a multi-scale genetic language model that learns to generate mechanistic hypotheses as to how differences in genome sequence lead to disease-relevant changes in expression across cell types and tissues directly from DNA sequences of up to 88 million base pairs. Using whole genome sequencing data from more than 150 000 individuals, we show that Phenformer generates mechanistic hypotheses about disease-relevant cell and tissue types that match literature better than existing state-of-the-art methods, while using only sequence data. Furthermore, disease risk predictors enriched by Phenformer show improved prediction performance and generalisation to diverse populations. Accurate multi-megabase scale interpretation of whole genomes without additional experimental data enables both a deeper understanding of molecular mechanisms involved in disease and improved disease risk prediction at the level of individuals.",
      "authors": "Frederik Träuble; Lachlan Stuart; Andreas Georgiou; Pascal Notin; Arash Mehrjou; Ron Schwessinger; Mathieu Chevalley; Kim Branson; Bernhard Schölkopf; Cornelia van Duijn; Debora Marks; Patrick Schwab",
      "date": "2025-01-13",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2412.19191v2",
      "doi": "",
      "title": "Biology-Instructions: A Dataset and Benchmark for Multi-Omics Sequence Understanding Capability of Large Language Models",
      "abstract": "Large language models (LLMs) have shown remarkable capabilities in general domains, but their application to multi-omics biology remains underexplored. To address this gap, we introduce Biology-Instructions, the first large-scale instruction-tuning dataset for multi-omics biological sequences, including DNA, RNA, proteins, and multi-molecules. This dataset bridges LLMs and complex biological sequence-related tasks, enhancing their versatility and reasoning while maintaining conversational fluency. We also highlight significant limitations of current state-of-the-art LLMs on multi-omics tasks without specialized training. To overcome this, we propose ChatMultiOmics, a strong baseline with a novel three-stage training pipeline, demonstrating superior biological understanding through Biology-Instructions. Both resources are publicly available, paving the way for better integration of LLMs in multi-omics analysis. The Biology-Instructions is publicly available at: https://github.com/hhnqqq/Biology-Instructions.",
      "authors": "Haonan He; Yuchen Ren; Yining Tang; Ziyang Xu; Junxian Li; Minghao Yang; Di Zhang; Dong Yuan; Tao Chen; Shufei Zhang; Yuqiang Li; Nanqing Dong; Wanli Ouyang; Dongzhan Zhou; Peng Ye",
      "date": "2024-12-26",
      "year": "2024",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2412.18156v1",
      "doi": "",
      "title": "scReader: Prompting Large Language Models to Interpret scRNA-seq Data",
      "abstract": "Large language models (LLMs) have demonstrated remarkable advancements, primarily due to their capabilities in modeling the hidden relationships within text sequences. This innovation presents a unique opportunity in the field of life sciences, where vast collections of single-cell omics data from multiple species provide a foundation for training foundational models. However, the challenge lies in the disparity of data scales across different species, hindering the development of a comprehensive model for interpreting genetic data across diverse organisms. In this study, we propose an innovative hybrid approach that integrates the general knowledge capabilities of LLMs with domain-specific representation models for single-cell omics data interpretation. We begin by focusing on genes as the fundamental unit of representation. Gene representations are initialized using functional descriptions, leveraging the strengths of mature language models such as LLaMA-2. By inputting single-cell gene-level expression data with prompts, we effectively model cellular representations based on the differential expression levels of genes across various species and cell types. In the experiments, we constructed developmental cells from humans and mice, specifically targeting cells that are challenging to annotate. We evaluated our methodology through basic tasks such as cell annotation and visualization analysis. The results demonstrate the efficacy of our approach compared to other methods using LLMs, highlighting significant improvements in accuracy and interoperability. Our hybrid approach enhances the representation of single-cell data and offers a robust framework for future research in cross-species genetic analysis.",
      "authors": "Cong Li; Qingqing Long; Yuanchun Zhou; Meng Xiao",
      "date": "2024-12-24",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2412.15790v1",
      "doi": "",
      "title": "GraphSeqLM: A Unified Graph Language Framework for Omic Graph Learning",
      "abstract": "The integration of multi-omic data is pivotal for understanding complex diseases, but its high dimensionality and noise present significant challenges. Graph Neural Networks (GNNs) offer a robust framework for analyzing large-scale signaling pathways and protein-protein interaction networks, yet they face limitations in expressivity when capturing intricate biological relationships. To address this, we propose Graph Sequence Language Model (GraphSeqLM), a framework that enhances GNNs with biological sequence embeddings generated by Large Language Models (LLMs). These embeddings encode structural and biological properties of DNA, RNA, and proteins, augmenting GNNs with enriched features for analyzing sample-specific multi-omic data. By integrating topological, sequence-derived, and biological information, GraphSeqLM demonstrates superior predictive accuracy and outperforms existing methods, paving the way for more effective multi-omic data integration in precision medicine.",
      "authors": "Heming Zhang; Di Huang; Yixin Chen; Fuhai Li",
      "date": "2024-12-20",
      "year": "2024",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2412.10659v1",
      "doi": "",
      "title": "MEATRD: Multimodal Anomalous Tissue Region Detection Enhanced with Spatial Transcriptomics",
      "abstract": "The detection of anomalous tissue regions (ATRs) within affected tissues is crucial in clinical diagnosis and pathological studies. Conventional automated ATR detection methods, primarily based on histology images alone, falter in cases where ATRs and normal tissues have subtle visual differences. The recent spatial transcriptomics (ST) technology profiles gene expressions across tissue regions, offering a molecular perspective for detecting ATRs. However, there is a dearth of ATR detection methods that effectively harness complementary information from both histology images and ST. To address this gap, we propose MEATRD, a novel ATR detection method that integrates histology image and ST data. MEATRD is trained to reconstruct image patches and gene expression profiles of normal tissue spots (inliers) from their multimodal embeddings, followed by learning a one-class classification AD model based on latent multimodal reconstruction errors. This strategy harmonizes the strengths of reconstruction-based and one-class classification approaches. At the heart of MEATRD is an innovative masked graph dual-attention transformer (MGDAT) network, which not only facilitates cross-modality and cross-node information sharing but also addresses the model over-generalization issue commonly seen in reconstruction-based AD methods. Additionally, we demonstrate that modality-specific, task-relevant information is collated and condensed in multimodal bottleneck encoding generated in MGDAT, marking the first theoretical analysis of the informational properties of multimodal bottleneck encoding. Extensive evaluations across eight real ST datasets reveal MEATRD's superior performance in ATR detection, surpassing various state-of-the-art AD methods. Remarkably, MEATRD also proves adept at discerning ATRs that only show slight visual deviations from normal tissues.",
      "authors": "Kaichen Xu; Qilong Wu; Yan Lu; Yinan Zheng; Wenlin Li; Xingjie Tang; Jun Wang; Xiaobo Sun",
      "date": "2024-12-14",
      "year": "2024",
      "categories": [
        "cs.CV",
        "cs.LG",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2412.07136v1",
      "doi": "",
      "title": "A multimodal ensemble approach for clear cell renal cell carcinoma treatment outcome prediction",
      "abstract": "Purpose: A reliable cancer prognosis model for clear cell renal cell carcinoma (ccRCC) can enhance personalized treatment. We developed a multi-modal ensemble model (MMEM) that integrates pretreatment clinical data, multi-omics data, and histopathology whole slide image (WSI) data to predict overall survival (OS) and disease-free survival (DFS) for ccRCC patients. Methods: We analyzed 226 patients from The Cancer Genome Atlas Kidney Renal Clear Cell Carcinoma (TCGA-KIRC) dataset, which includes OS, DFS follow-up data, and five data modalities: clinical data, WSIs, and three multi-omics datasets (mRNA, miRNA, and DNA methylation). Separate survival models were built for OS and DFS. Cox-proportional hazards (CPH) model with forward feature selection is used for clinical and multi-omics data. Features from WSIs were extracted using ResNet and three general-purpose foundation models. A deep learning-based CPH model predicted survival using encoded WSI features. Risk scores from all models were combined based on training performance. Results: Performance was assessed using concordance index (C-index) and AUROC. The clinical feature-based CPH model received the highest weight for both OS and DFS tasks. Among WSI-based models, the general-purpose foundation model (UNI) achieved the best performance. The final MMEM model surpassed single-modality models, achieving C-indices of 0.820 (OS) and 0.833 (DFS), and AUROC values of 0.831 (3-year patient death) and 0.862 (cancer recurrence). Using predicted risk medians to stratify high- and low-risk groups, log-rank tests showed improved performance in both OS and DFS compared to single-modality models. Conclusion: MMEM is the first multi-modal model for ccRCC patients, integrating five data modalities. It outperformed single-modality models in prognostic ability and has the potential to assist in ccRCC patient management if independently validated.",
      "authors": "Meixu Chen; Kai Wang; Payal Kapur; James Brugarolas; Raquibul Hannan; Jing Wang",
      "date": "2024-12-10",
      "year": "2024",
      "categories": [
        "cs.CV",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2412.02915v1",
      "doi": "",
      "title": "Single-Cell Omics Arena: A Benchmark Study for Large Language Models on Cell Type Annotation Using Single-Cell Data",
      "abstract": "Over the past decade, the revolution in single-cell sequencing has enabled the simultaneous molecular profiling of various modalities across thousands of individual cells, allowing scientists to investigate the diverse functions of complex tissues and uncover underlying disease mechanisms. Among all the analytical steps, assigning individual cells to specific types is fundamental for understanding cellular heterogeneity. However, this process is usually labor-intensive and requires extensive expert knowledge. Recent advances in large language models (LLMs) have demonstrated their ability to efficiently process and synthesize vast corpora of text to automatically extract essential biological knowledge, such as marker genes, potentially promoting more efficient and automated cell type annotations. To thoroughly evaluate the capability of modern instruction-tuned LLMs in automating the cell type identification process, we introduce SOAR, a comprehensive benchmarking study of LLMs for cell type annotation tasks in single-cell genomics. Specifically, we assess the performance of 8 instruction-tuned LLMs across 11 datasets, spanning multiple cell types and species. Our study explores the potential of LLMs to accurately classify and annotate cell types in single-cell RNA sequencing (scRNA-seq) data, while extending their application to multiomics data through cross-modality translation. Additionally, we evaluate the effectiveness of chain-of-thought (CoT) prompting techniques in generating detailed biological insights during the annotation process. The results demonstrate that LLMs can provide robust interpretations of single-cell data without requiring additional fine-tuning, advancing the automation of cell type annotation in genomics research.",
      "authors": "Junhao Liu; Siwei Xu; Lei Zhang; Jing Zhang",
      "date": "2024-12-03",
      "year": "2024",
      "categories": [
        "cs.CL",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2412.12112v3",
      "doi": "",
      "title": "Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations",
      "abstract": "We propose a probabilistic framework for developing computational models of biological neural systems. In this framework, physiological recordings are viewed as discrete-time partial observations of an underlying continuous-time stochastic dynamical system which implements computations through its state evolution. To model this dynamical system, we employ a system of coupled stochastic differential equations with differentiable drift and diffusion functions and use variational inference to infer its states and parameters. This formulation enables seamless integration of existing mathematical models in the literature, neural networks, or a hybrid of both to learn and compare different models. We demonstrate this in our framework by developing a generative model that combines coupled oscillators with neural networks to capture latent population dynamics from single-cell recordings. Evaluation across three neuroscience datasets spanning different species, brain regions, and behavioral tasks show that these hybrid models achieve competitive performance in predicting stimulus-evoked neural and behavioral responses compared to sophisticated black-box approaches while requiring an order of magnitude fewer parameters, providing uncertainty estimates, and offering a natural language for interpretation.",
      "authors": "Ahmed ElGazzar; Marcel van Gerven",
      "date": "2024-12-01",
      "year": "2024",
      "categories": [
        "q-bio.NC",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2411.16793v1",
      "doi": "",
      "title": "ST-Align: A Multimodal Foundation Model for Image-Gene Alignment in Spatial Transcriptomics",
      "abstract": "Spatial transcriptomics (ST) provides high-resolution pathological images and whole-transcriptomic expression profiles at individual spots across whole-slide scales. This setting makes it an ideal data source to develop multimodal foundation models. Although recent studies attempted to fine-tune visual encoders with trainable gene encoders based on spot-level, the absence of a wider slide perspective and spatial intrinsic relationships limits their ability to capture ST-specific insights effectively. Here, we introduce ST-Align, the first foundation model designed for ST that deeply aligns image-gene pairs by incorporating spatial context, effectively bridging pathological imaging with genomic features. We design a novel pretraining framework with a three-target alignment strategy for ST-Align, enabling (1) multi-scale alignment across image-gene pairs, capturing both spot- and niche-level contexts for a comprehensive perspective, and (2) cross-level alignment of multimodal insights, connecting localized cellular characteristics and broader tissue architecture. Additionally, ST-Align employs specialized encoders tailored to distinct ST contexts, followed by an Attention-Based Fusion Network (ABFN) for enhanced multimodal fusion, effectively merging domain-shared knowledge with ST-specific insights from both pathological and genomic data. We pre-trained ST-Align on 1.3 million spot-niche pairs and evaluated its performance through two downstream tasks across six datasets, demonstrating superior zero-shot and few-shot capabilities. ST-Align highlights the potential for reducing the cost of ST and providing valuable insights into the distinction of critical compositions within human tissue.",
      "authors": "Yuxiang Lin; Ling Luo; Ying Chen; Xushi Zhang; Zihui Wang; Wenxian Yang; Mengsha Tong; Rongshan Yu",
      "date": "2024-11-25",
      "year": "2024",
      "categories": [
        "cs.CV",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2411.16084v1",
      "doi": "https://doi.org/10.1093/jamia/ocaf029",
      "title": "Deciphering genomic codes using advanced NLP techniques: a scoping review",
      "abstract": "Objectives: The vast and complex nature of human genomic sequencing data presents challenges for effective analysis. This review aims to investigate the application of Natural Language Processing (NLP) techniques, particularly Large Language Models (LLMs) and transformer architectures, in deciphering genomic codes, focusing on tokenization, transformer models, and regulatory annotation prediction. The goal of this review is to assess data and model accessibility in the most recent literature, gaining a better understanding of the existing capabilities and constraints of these tools in processing genomic sequencing data.   Methods: Following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, our scoping review was conducted across PubMed, Medline, Scopus, Web of Science, Embase, and ACM Digital Library. Studies were included if they focused on NLP methodologies applied to genomic sequencing data analysis, without restrictions on publication date or article type.   Results: A total of 26 studies published between 2021 and April 2024 were selected for review. The review highlights that tokenization and transformer models enhance the processing and understanding of genomic data, with applications in predicting regulatory annotations like transcription-factor binding sites and chromatin accessibility.   Discussion: The application of NLP and LLMs to genomic sequencing data interpretation is a promising field that can help streamline the processing of large-scale genomic data while also providing a better understanding of its complex structures. It has the potential to drive advancements in personalized medicine by offering more efficient and scalable solutions for genomic analysis. Further research is also needed to discuss and overcome current limitations, enhancing model transparency and applicability.",
      "authors": "Shuyan Cheng; Yishu Wei; Yiliang Zhou; Zihan Xu; Drew N Wright; Jinze Liu; Yifan Peng",
      "date": "2024-11-25",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2411.15076v2",
      "doi": "",
      "title": "RankByGene: Gene-Guided Histopathology Representation Learning Through Cross-Modal Ranking Consistency",
      "abstract": "Spatial transcriptomics (ST) provides essential spatial context by mapping gene expression within tissue, enabling detailed study of cellular heterogeneity and tissue organization. However, aligning ST data with histology images poses challenges due to inherent spatial distortions and modality-specific variations. Existing methods largely rely on direct alignment, which often fails to capture complex cross-modal relationships. To address these limitations, we propose a novel framework that aligns gene and image features using a ranking-based alignment loss, preserving relative similarity across modalities and enabling robust multi-scale alignment. To further enhance the alignment's stability, we employ self-supervised knowledge distillation with a teacher-student network architecture, effectively mitigating disruptions from high dimensionality, sparsity, and noise in gene expression data. Extensive experiments on seven public datasets that encompass gene expression prediction, slide-level classification, and survival analysis demonstrate the efficacy of our method, showing improved alignment and predictive performance over existing methods.",
      "authors": "Wentao Huang; Meilong Xu; Xiaoling Hu; Shahira Abousamra; Aniruddha Ganguly; Saarthak Kapse; Alisa Yurovsky; Prateek Prasanna; Tahsin Kurc; Joel Saltz; Michael L. Miller; Chao Chen",
      "date": "2024-11-22",
      "year": "2024",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2411.11158v2",
      "doi": "",
      "title": "Leveraging genomic deep learning models for the prediction of non-coding variant effects",
      "abstract": "Characterizing non-coding variant function remains an important challenge in human genetics. Genomic deep learning models have emerged as a promising approach to enable in silico prediction of variant effects. These include supervised sequence-to-activity models, which predict molecular phenotypes such as genome-wide chromatin states or gene expression levels directly from DNA sequence, and self-supervised genomic language models. Here, we review progress in leveraging these models for non-coding variant effect prediction. We describe practical considerations for making such predictions and categorize the types of ground truth data used to evaluate variant effect predictions, providing insight into the settings in which current models are most useful. Our Review highlights key considerations for practitioners and opportunities for improvement in model development and evaluation.",
      "authors": "Pooja Kathail; Ayesha Bajwa; Nilah M. Ioannidis",
      "date": "2024-11-17",
      "year": "2024",
      "categories": [
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2411.03743v1",
      "doi": "",
      "title": "Automating Exploratory Proteomics Research via Language Models",
      "abstract": "With the development of artificial intelligence, its contribution to science is evolving from simulating a complex problem to automating entire research processes and producing novel discoveries. Achieving this advancement requires both specialized general models grounded in real-world scientific data and iterative, exploratory frameworks that mirror human scientific methodologies. In this paper, we present PROTEUS, a fully automated system for scientific discovery from raw proteomics data. PROTEUS uses large language models (LLMs) to perform hierarchical planning, execute specialized bioinformatics tools, and iteratively refine analysis workflows to generate high-quality scientific hypotheses. The system takes proteomics datasets as input and produces a comprehensive set of research objectives, analysis results, and novel biological hypotheses without human intervention. We evaluated PROTEUS on 12 proteomics datasets collected from various biological samples (e.g. immune cells, tumors) and different sample types (single-cell and bulk), generating 191 scientific hypotheses. These were assessed using both automatic LLM-based scoring on 5 metrics and detailed reviews from human experts. Results demonstrate that PROTEUS consistently produces reliable, logically coherent results that align well with existing literature while also proposing novel, evaluable hypotheses. The system's flexible architecture facilitates seamless integration of diverse analysis tools and adaptation to different proteomics data types. By automating complex proteomics analysis workflows and hypothesis generation, PROTEUS has the potential to considerably accelerate the pace of scientific discovery in proteomics research, enabling researchers to efficiently explore large-scale datasets and uncover biological insights.",
      "authors": "Ning Ding; Shang Qu; Linhai Xie; Yifei Li; Zaoqu Liu; Kaiyan Zhang; Yibai Xiong; Yuxin Zuo; Zhangren Chen; Ermo Hua; Xingtai Lv; Youbang Sun; Yang Li; Dong Li; Fuchu He; Bowen Zhou",
      "date": "2024-11-06",
      "year": "2024",
      "categories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2411.00749v1",
      "doi": "",
      "title": "PathoGen-X: A Cross-Modal Genomic Feature Trans-Align Network for Enhanced Survival Prediction from Histopathology Images",
      "abstract": "Accurate survival prediction is essential for personalized cancer treatment. However, genomic data - often a more powerful predictor than pathology data - is costly and inaccessible. We present the cross-modal genomic feature translation and alignment network for enhanced survival prediction from histopathology images (PathoGen-X). It is a deep learning framework that leverages both genomic and imaging data during training, relying solely on imaging data at testing. PathoGen-X employs transformer-based networks to align and translate image features into the genomic feature space, enhancing weaker imaging signals with stronger genomic signals. Unlike other methods, PathoGen-X translates and aligns features without projecting them to a shared latent space and requires fewer paired samples. Evaluated on TCGA-BRCA, TCGA-LUAD, and TCGA-GBM datasets, PathoGen-X demonstrates strong survival prediction performance, emphasizing the potential of enriched imaging models for accessible cancer prognosis.",
      "authors": "Akhila Krishna; Nikhil Cherian Kurian; Abhijeet Patil; Amruta Parulekar; Amit Sethi",
      "date": "2024-11-01",
      "year": "2024",
      "categories": [
        "eess.IV",
        "cs.CV",
        "q-bio.GN",
        "q-bio.TO"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2410.15828v1",
      "doi": "",
      "title": "LLM4GRN: Discovering Causal Gene Regulatory Networks with LLMs -- Evaluation through Synthetic Data Generation",
      "abstract": "Gene regulatory networks (GRNs) represent the causal relationships between transcription factors (TFs) and target genes in single-cell RNA sequencing (scRNA-seq) data. Understanding these networks is crucial for uncovering disease mechanisms and identifying therapeutic targets. In this work, we investigate the potential of large language models (LLMs) for GRN discovery, leveraging their learned biological knowledge alone or in combination with traditional statistical methods. We develop a task-based evaluation strategy to address the challenge of unavailable ground truth causal graphs. Specifically, we use the GRNs suggested by LLMs to guide causal synthetic data generation and compare the resulting data against the original dataset. Our statistical and biological assessments show that LLMs can support statistical modeling and data synthesis for biological research.",
      "authors": "Tejumade Afonja; Ivaxi Sheth; Ruta Binkyte; Waqar Hanif; Thomas Ulas; Matthias Becker; Mario Fritz",
      "date": "2024-10-21",
      "year": "2024",
      "categories": [
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2410.11468v3",
      "doi": "",
      "title": "Can sparse autoencoders make sense of gene expression latent variable models?",
      "abstract": "Sparse autoencoders (SAEs) have lately been used to uncover interpretable latent features in large language models. By projecting dense embeddings into a much higher-dimensional and sparse space, learned features become disentangled and easier to interpret. This work explores the potential of SAEs for decomposing embeddings in complex and high-dimensional biological data. Using simulated data, it outlines the efficacy, hyperparameter landscape, and limitations of SAEs when it comes to extracting ground truth generative variables from latent space. The application to embeddings from pretrained single-cell models shows that SAEs can find and steer key biological processes and even uncover subtle biological signals that might otherwise be missed. This work further introduces scFeatureLens, an automated interpretability approach for linking SAE features and biological concepts from gene sets to enable large-scale analysis and hypothesis generation in single-cell gene expression models.",
      "authors": "Viktoria Schuster",
      "date": "2024-10-15",
      "year": "2024",
      "categories": [
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2410.11281v2",
      "doi": "",
      "title": "DynaCLR: Contrastive Learning of Cellular Dynamics with Temporal Regularization",
      "abstract": "We report DynaCLR, a self-supervised method for embedding cell and organelle Dynamics via Contrastive Learning of Representations of time-lapse images. DynaCLR integrates single-cell tracking and time-aware contrastive sampling to learn robust, temporally regularized representations of cell dynamics. DynaCLR embeddings generalize effectively to in-distribution and out-of-distribution datasets, and can be used for several downstream tasks with sparse human annotations. We demonstrate efficient annotations of cell states with a human-in-the-loop using fluorescence and label-free imaging channels. DynaCLR method enables diverse downstream biological analyses: classification of cell division and infection, clustering heterogeneous cell migration patterns, cross-modal distillation of cell states from fluorescence to label-free channel, alignment of asynchronous cellular responses and broken cell tracks, and discovering organelle response due to infection. DynaCLR is a flexible method for comparative analyses of dynamic cellular responses to pharmacological, microbial, and genetic perturbations. We provide PyTorch-based implementations of the model training and inference pipeline (https://github.com/mehta-lab/viscy) and a GUI (https://github.com/czbiohub-sf/napari-iohub) for the visualization and annotation of trajectories of cells in the real space and the embedding space.",
      "authors": "Eduardo Hirata-Miyasaki; Soorya Pradeep; Ziwen Liu; Alishba Imran; Taylla Milena Theodoro; Ivan E. Ivanov; Sudip Khadka; See-Chi Lee; Michelle Grunberg; Hunter Woosley; Madhura Bhave; Carolina Arias; Shalin B. Mehta",
      "date": "2024-10-15",
      "year": "2024",
      "categories": [
        "cs.CV",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2410.05292v1",
      "doi": "",
      "title": "CaLMFlow: Volterra Flow Matching using Causal Language Models",
      "abstract": "We introduce CaLMFlow (Causal Language Models for Flow Matching), a novel framework that casts flow matching as a Volterra integral equation (VIE), leveraging the power of large language models (LLMs) for continuous data generation. CaLMFlow enables the direct application of LLMs to learn complex flows by formulating flow matching as a sequence modeling task, bridging discrete language modeling and continuous generative modeling. Our method implements tokenization across space and time, thereby solving a VIE over these domains. This approach enables efficient handling of high-dimensional data and outperforms ODE solver-dependent methods like conditional flow matching (CFM). We demonstrate CaLMFlow's effectiveness on synthetic and real-world data, including single-cell perturbation response prediction, showcasing its ability to incorporate textual context and generalize to unseen conditions. Our results highlight LLM-driven flow matching as a promising paradigm in generative modeling, offering improved scalability, flexibility, and context-awareness.",
      "authors": "Sizhuang He; Daniel Levine; Ivan Vrkic; Marco Francesco Bressana; David Zhang; Syed Asad Rizvi; Yangtian Zhang; Emanuele Zappala; David van Dijk",
      "date": "2024-10-03",
      "year": "2024",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2410.01784v1",
      "doi": "",
      "title": "OmniGenBench: Automating Large-scale in-silico Benchmarking for Genomic Foundation Models",
      "abstract": "The advancements in artificial intelligence in recent years, such as Large Language Models (LLMs), have fueled expectations for breakthroughs in genomic foundation models (GFMs). The code of nature, hidden in diverse genomes since the very beginning of life's evolution, holds immense potential for impacting humans and ecosystems through genome modeling. Recent breakthroughs in GFMs, such as Evo, have attracted significant investment and attention to genomic modeling, as they address long-standing challenges and transform in-silico genomic studies into automated, reliable, and efficient paradigms. In the context of this flourishing era of consecutive technological revolutions in genomics, GFM studies face two major challenges: the lack of GFM benchmarking tools and the absence of open-source software for diverse genomics. These challenges hinder the rapid evolution of GFMs and their wide application in tasks such as understanding and synthesizing genomes, problems that have persisted for decades. To address these challenges, we introduce GFMBench, a framework dedicated to GFM-oriented benchmarking. GFMBench standardizes benchmark suites and automates benchmarking for a wide range of open-source GFMs. It integrates millions of genomic sequences across hundreds of genomic tasks from four large-scale benchmarks, democratizing GFMs for a wide range of in-silico genomic applications. Additionally, GFMBench is released as open-source software, offering user-friendly interfaces and diverse tutorials, applicable for AutoBench and complex tasks like RNA design and structure prediction. To facilitate further advancements in genome modeling, we have launched a public leaderboard showcasing the benchmark performance derived from AutoBench. GFMBench represents a step toward standardizing GFM benchmarking and democratizing GFM applications.",
      "authors": "Heng Yang; Jack Cole; Ke Li",
      "date": "2024-10-02",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2410.01858v1",
      "doi": "",
      "title": "Long-range gene expression prediction with token alignment of large language model",
      "abstract": "Gene expression is a cellular process that plays a fundamental role in human phenotypical variations and diseases. Despite advances of deep learning models for gene expression prediction, recent benchmarks have revealed their inability to learn distal regulatory grammar. Here, we address this challenge by leveraging a pretrained large language model to enhance gene expression prediction. We introduce Genetic sequence Token Alignment (GTA), which aligns genetic sequence features with natural language tokens, allowing for symbolic reasoning of genomic sequence features via the frozen language model. This cross-modal adaptation learns the regulatory grammar and allows us to further incorporate gene-specific human annotations as prompts, enabling in-context learning that is not possible with existing models. Trained on lymphoblastoid cells, GTA was evaluated on cells from the Geuvadis consortium and outperforms state-of-the-art models such as Enformer, achieving a Spearman correlation of 0.65, a 10\\% improvement. Additionally, GTA offers improved interpretation of long-range interactions through the identification of the most meaningful sections of the input genetic context. GTA represents a powerful and novel cross-modal approach to gene expression prediction by utilizing a pretrained language model, in a paradigm shift from conventional gene expression models trained only on sequence data.",
      "authors": "Edouardo Honig; Huixin Zhan; Ying Nian Wu; Zijun Frank Zhang",
      "date": "2024-10-02",
      "year": "2024",
      "categories": [
        "q-bio.CB",
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2409.15697v1",
      "doi": "",
      "title": "dnaGrinder: a lightweight and high-capacity genomic foundation model",
      "abstract": "The task of understanding and interpreting the complex information encoded within genomic sequences remains a grand challenge in biological research and clinical applications. In this context, recent advancements in large language model research have led to the development of both encoder-only and decoder-only foundation models designed to decode intricate information in DNA sequences. However, several issues persist, particularly regarding the efficient management of long-range dependencies inherent in genomic sequences, the effective representation of nucleotide variations, and the considerable computational costs associated with large model architectures and extensive pretraining datasets. Current genomic foundation models often face a critical tradeoff: smaller models with mediocre performance versus large models with improved performance. To address these challenges, we introduce dnaGrinder, a unique and efficient genomic foundation model. dnaGrinder excels at managing long-range dependencies within genomic sequences while minimizing computational costs without compromising performance. It achieves results that are not just comparable but often superior to leading DNA models such as Nucleotide Transformer and DNABERT-2. Furthermore, dnaGrinder is designed for easy fine-tuning on workstation-grade GPUs, accommodating input lengths exceeding 17,000 tokens. On a single high-performance GPU, it supports sequences longer than 140,000 tokens, making it a highly efficient and accessible tool for both basic biological research and clinical applications.",
      "authors": "Qihang Zhao; Chi Zhang; Weixiong Zhang",
      "date": "2024-09-24",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CE",
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2409.10502v1",
      "doi": "",
      "title": "Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles",
      "abstract": "Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years. However, the extent to which fundamental search and reasoning capabilities emerged within LLMs remains a topic of ongoing debate. In this work, we study if causal language modeling can learn a complex task such as solving Sudoku puzzles. To solve a Sudoku, the model is first required to search over all empty cells of the puzzle to decide on a cell to fill and then apply an appropriate strategy to fill the decided cell. Sometimes, the application of a strategy only results in thinning down the possible values in a cell rather than concluding the exact value of the cell. In such cases, multiple strategies are applied one after the other to fill a single cell. We observe that Transformer models trained on this synthetic task can indeed learn to solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly) when trained on a logical sequence of steps taken by a solver. We find that training Transformers with the logical sequence of steps is necessary and without such training, they fail to learn Sudoku. We also extend our analysis to Zebra puzzles (known as Einstein puzzles) and show that the model solves $92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal representations of the trained Transformer and find that through linear probing, we can decode information about the set of possible values in any given cell from them, pointing to the presence of a strong reasoning engine implicit in the Transformer weights.",
      "authors": "Kulin Shah; Nishanth Dikkala; Xin Wang; Rina Panigrahy",
      "date": "2024-09-16",
      "year": "2024",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2409.02732v1",
      "doi": "",
      "title": "Approximating mutual information of high-dimensional variables using learned representations",
      "abstract": "Mutual information (MI) is a general measure of statistical dependence with widespread application across the sciences. However, estimating MI between multi-dimensional variables is challenging because the number of samples necessary to converge to an accurate estimate scales unfavorably with dimensionality. In practice, existing techniques can reliably estimate MI in up to tens of dimensions, but fail in higher dimensions, where sufficient sample sizes are infeasible. Here, we explore the idea that underlying low-dimensional structure in high-dimensional data can be exploited to faithfully approximate MI in high-dimensional settings with realistic sample sizes. We develop a method that we call latent MI (LMI) approximation, which applies a nonparametric MI estimator to low-dimensional representations learned by a simple, theoretically-motivated model architecture. Using several benchmarks, we show that unlike existing techniques, LMI can approximate MI well for variables with $> 10^3$ dimensions if their dependence structure has low intrinsic dimensionality. Finally, we showcase LMI on two open problems in biology. First, we approximate MI between protein language model (pLM) representations of interacting proteins, and find that pLMs encode non-trivial information about protein-protein interactions. Second, we quantify cell fate information contained in single-cell RNA-seq (scRNA-seq) measurements of hematopoietic stem cells, and find a sharp transition during neutrophil differentiation when fate information captured by scRNA-seq increases dramatically.",
      "authors": "Gokul Gowri; Xiao-Kang Lun; Allon M. Klein; Peng Yin",
      "date": "2024-09-03",
      "year": "2024",
      "categories": [
        "q-bio.QM",
        "cs.IT",
        "stat.ML"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2408.16245v6",
      "doi": "",
      "title": "Large-Scale Multi-omic Biosequence Transformers for Modeling Protein-Nucleic Acid Interactions",
      "abstract": "The transformer architecture has revolutionized bioinformatics and driven progress in the understanding and prediction of the properties of biomolecules. To date, most biosequence transformers have been trained on single-omic data - either proteins or nucleic acids - and have seen incredible success in downstream tasks in each domain, with particularly noteworthy breakthroughs in protein structural modeling. However, single-omic pretraining limits the ability of these models to capture cross-modal interactions. Here we present OmniBioTE, the largest open-source multi-omic model trained on over 250 billion tokens of mixed protein and nucleic acid data. We show that despite only being trained on unlabeled sequence data, OmniBioTE learns joint representations mapping genes to their corresponding protein sequences. We further demonstrate that OmniBioTE achieves state-of-the-art results predicting the change in Gibbs free energy ({ΔG}) of the binding interaction between a given nucleic acid and protein. Remarkably, we show that multi-omic biosequence transformers emergently learn useful structural information without any a priori structural training, allowing us to predict which protein residues are most involved in the protein-nucleic acid binding interaction. Compared to single-omic controls trained with identical compute, OmniBioTE also demonstrates superior performance-per-FLOP across both multi-omic and single-omic benchmarks. Together, these results highlight the power of a unified modeling approach for biological sequences and establish OmniBioTE as a foundation model for multi-omic discovery.",
      "authors": "Sully F. Chen; Robert J. Steele; Glen M. Hocky; Beakal Lemeneh; Shivanand P. Lad; Eric K. Oermann",
      "date": "2024-08-29",
      "year": "2024",
      "categories": [
        "cs.LG",
        "q-bio.BM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2407.19385v1",
      "doi": "",
      "title": "Multi-modal Imaging Genomics Transformer: Attentive Integration of Imaging with Genomic Biomarkers for Schizophrenia Classification",
      "abstract": "Schizophrenia (SZ) is a severe brain disorder marked by diverse cognitive impairments, abnormalities in brain structure, function, and genetic factors. Its complex symptoms and overlap with other psychiatric conditions challenge traditional diagnostic methods, necessitating advanced systems to improve precision. Existing research studies have mostly focused on imaging data, such as structural and functional MRI, for SZ diagnosis. There has been less focus on the integration of genomic features despite their potential in identifying heritable SZ traits. In this study, we introduce a Multi-modal Imaging Genomics Transformer (MIGTrans), that attentively integrates genomics with structural and functional imaging data to capture SZ-related neuroanatomical and connectome abnormalities. MIGTrans demonstrated improved SZ classification performance with an accuracy of 86.05% (+/- 0.02), offering clear interpretations and identifying significant genomic locations and brain morphological/connectivity patterns associated with SZ.",
      "authors": "Nagur Shareef Shaik; Teja Krishna Cherukuri; Vince D. Calhoun; Dong Hye Ye",
      "date": "2024-07-28",
      "year": "2024",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "q-bio.NC"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2407.18181v1",
      "doi": "",
      "title": "Gene Regulatory Network Inference from Pre-trained Single-Cell Transcriptomics Transformer with Joint Graph Learning",
      "abstract": "Inferring gene regulatory networks (GRNs) from single-cell RNA sequencing (scRNA-seq) data is a complex challenge that requires capturing the intricate relationships between genes and their regulatory interactions. In this study, we tackle this challenge by leveraging the single-cell BERT-based pre-trained transformer model (scBERT), trained on extensive unlabeled scRNA-seq data, to augment structured biological knowledge from existing GRNs. We introduce a novel joint graph learning approach that combines the rich contextual representations learned by pre-trained single-cell language models with the structured knowledge encoded in GRNs using graph neural networks (GNNs). By integrating these two modalities, our approach effectively reasons over boththe gene expression level constraints provided by the scRNA-seq data and the structured biological knowledge inherent in GRNs. We evaluate our method on human cell benchmark datasets from the BEELINE study with cell type-specific ground truth networks. The results demonstrate superior performance over current state-of-the-art baselines, offering a deeper understanding of cellular regulatory mechanisms.",
      "authors": "Sindhura Kommu; Yizhi Wang; Yue Wang; Xuan Wang",
      "date": "2024-07-25",
      "year": "2024",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2407.13205v1",
      "doi": "",
      "title": "Transformer-based Single-Cell Language Model: A Survey",
      "abstract": "The transformers have achieved significant accomplishments in the natural language processing as its outstanding parallel processing capabilities and highly flexible attention mechanism. In addition, increasing studies based on transformers have been proposed to model single-cell data. In this review, we attempt to systematically summarize the single-cell language models and applications based on transformers. First, we provide a detailed introduction about the structure and principles of transformers. Then, we review the single-cell language models and large language models for single-cell data analysis. Moreover, we explore the datasets and applications of single-cell language models in downstream tasks such as batch correction, cell clustering, cell type annotation, gene regulatory network inference and perturbation response. Further, we discuss the challenges of single-cell language models and provide promising research directions. We hope this review will serve as an up-to-date reference for researchers interested in the direction of single-cell language models.",
      "authors": "Wei Lan; Guohang He; Mingyang Liu; Qingfeng Chen; Junyue Cao; Wei Peng",
      "date": "2024-07-18",
      "year": "2024",
      "categories": [
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2407.11734v2",
      "doi": "",
      "title": "Multi-Modal and Multi-Attribute Generation of Single Cells with CFGen",
      "abstract": "Generative modeling of single-cell RNA-seq data is crucial for tasks like trajectory inference, batch effect removal, and simulation of realistic cellular data. However, recent deep generative models simulating synthetic single cells from noise operate on pre-processed continuous gene expression approximations, overlooking the discrete nature of single-cell data, which limits their effectiveness and hinders the incorporation of robust noise models. Additionally, aspects like controllable multi-modal and multi-label generation of cellular data remain underexplored. This work introduces CellFlow for Generation (CFGen), a flow-based conditional generative model that preserves the inherent discreteness of single-cell data. CFGen generates whole-genome multi-modal single-cell data reliably, improving the recovery of crucial biological data characteristics while tackling relevant generative tasks such as rare cell type augmentation and batch correction. We also introduce a novel framework for compositional data generation using Flow Matching. By showcasing CFGen on a diverse set of biological datasets and settings, we provide evidence of its value to the fields of computational biology and deep generative models.",
      "authors": "Alessandro Palma; Till Richter; Hanyi Zhang; Manuel Lubetzki; Alexander Tong; Andrea Dittadi; Fabian Theis",
      "date": "2024-07-16",
      "year": "2024",
      "categories": [
        "q-bio.QM",
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2407.11435v2",
      "doi": "",
      "title": "Genomic Language Models: Opportunities and Challenges",
      "abstract": "Large language models (LLMs) are having transformative impacts across a wide range of scientific fields, particularly in the biomedical sciences. Just as the goal of Natural Language Processing is to understand sequences of words, a major objective in biology is to understand biological sequences. Genomic Language Models (gLMs), which are LLMs trained on DNA sequences, have the potential to significantly advance our understanding of genomes and how DNA elements at various scales interact to give rise to complex functions. To showcase this potential, we highlight key applications of gLMs, including functional constraint prediction, sequence design, and transfer learning. Despite notable recent progress, however, developing effective and efficient gLMs presents numerous challenges, especially for species with large, complex genomes. Here, we discuss major considerations for developing and evaluating gLMs.",
      "authors": "Gonzalo Benegas; Chengzhong Ye; Carlos Albors; Jianan Canal Li; Yun S. Song",
      "date": "2024-07-16",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.LG",
        "stat.ML"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2407.09811v1",
      "doi": "",
      "title": "CellAgent: An LLM-driven Multi-Agent Framework for Automated Single-cell Data Analysis",
      "abstract": "Single-cell RNA sequencing (scRNA-seq) data analysis is crucial for biological research, as it enables the precise characterization of cellular heterogeneity. However, manual manipulation of various tools to achieve desired outcomes can be labor-intensive for researchers. To address this, we introduce CellAgent (http://cell.agent4science.cn/), an LLM-driven multi-agent framework, specifically designed for the automatic processing and execution of scRNA-seq data analysis tasks, providing high-quality results with no human intervention. Firstly, to adapt general LLMs to the biological field, CellAgent constructs LLM-driven biological expert roles - planner, executor, and evaluator - each with specific responsibilities. Then, CellAgent introduces a hierarchical decision-making mechanism to coordinate these biological experts, effectively driving the planning and step-by-step execution of complex data analysis tasks. Furthermore, we propose a self-iterative optimization mechanism, enabling CellAgent to autonomously evaluate and optimize solutions, thereby guaranteeing output quality. We evaluate CellAgent on a comprehensive benchmark dataset encompassing dozens of tissues and hundreds of distinct cell types. Evaluation results consistently show that CellAgent effectively identifies the most suitable tools and hyperparameters for single-cell analysis tasks, achieving optimal performance. This automated framework dramatically reduces the workload for science data analyses, bringing us into the \"Agent for Science\" era.",
      "authors": "Yihang Xiao; Jinyi Liu; Yan Zheng; Xiaohan Xie; Jianye Hao; Mingzhi Li; Ruitao Wang; Fei Ni; Yuxiao Li; Jintian Luo; Shaoqing Jiao; Jiajie Peng",
      "date": "2024-07-13",
      "year": "2024",
      "categories": [
        "cs.AI",
        "cs.HC",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2407.08216v1",
      "doi": "",
      "title": "Multimodal contrastive learning for spatial gene expression prediction using histology images",
      "abstract": "In recent years, the advent of spatial transcriptomics (ST) technology has unlocked unprecedented opportunities for delving into the complexities of gene expression patterns within intricate biological systems. Despite its transformative potential, the prohibitive cost of ST technology remains a significant barrier to its widespread adoption in large-scale studies. An alternative, more cost-effective strategy involves employing artificial intelligence to predict gene expression levels using readily accessible whole-slide images (WSIs) stained with Hematoxylin and Eosin (H\\&E). However, existing methods have yet to fully capitalize on multimodal information provided by H&E images and ST data with spatial location. In this paper, we propose \\textbf{mclSTExp}, a multimodal contrastive learning with Transformer and Densenet-121 encoder for Spatial Transcriptomics Expression prediction. We conceptualize each spot as a \"word\", integrating its intrinsic features with spatial context through the self-attention mechanism of a Transformer encoder. This integration is further enriched by incorporating image features via contrastive learning, thereby enhancing the predictive capability of our model. Our extensive evaluation of \\textbf{mclSTExp} on two breast cancer datasets and a skin squamous cell carcinoma dataset demonstrates its superior performance in predicting spatial gene expression. Moreover, mclSTExp has shown promise in interpreting cancer-specific overexpressed genes, elucidating immune-related genes, and identifying specialized spatial domains annotated by pathologists. Our source code is available at https://github.com/shizhiceng/mclSTExp.",
      "authors": "Wenwen Min; Zhiceng Shi; Jun Zhang; Jun Wan; Changmiao Wang",
      "date": "2024-07-11",
      "year": "2024",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2407.03381v1",
      "doi": "",
      "title": "SeqMate: A Novel Large Language Model Pipeline for Automating RNA Sequencing",
      "abstract": "RNA sequencing techniques, like bulk RNA-seq and Single Cell (sc) RNA-seq, are critical tools for the biologist looking to analyze the genetic activity/transcriptome of a tissue or cell during an experimental procedure. Platforms like Illumina's next-generation sequencing (NGS) are used to produce the raw data for this experimental procedure. This raw FASTQ data must then be prepared via a complex series of data manipulations by bioinformaticians. This process currently takes place on an unwieldy textual user interface like a terminal/command line that requires the user to install and import multiple program packages, preventing the untrained biologist from initiating data analysis. Open-source platforms like Galaxy have produced a more user-friendly pipeline, yet the visual interface remains cluttered and highly technical, remaining uninviting for the natural scientist. To address this, SeqMate is a user-friendly tool that allows for one-click analytics by utilizing the power of a large language model (LLM) to automate both data preparation and analysis (differential expression, trajectory analysis, etc). Furthermore, by utilizing the power of generative AI, SeqMate is also capable of analyzing such findings and producing written reports of upregulated/downregulated/user-prompted genes with sources cited from known repositories like PubMed, PDB, and Uniprot.",
      "authors": "Devam Mondal; Atharva Inamdar",
      "date": "2024-07-02",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2406.15534v1",
      "doi": "",
      "title": "Geneverse: A collection of Open-source Multimodal Large Language Models for Genomic and Proteomic Research",
      "abstract": "The applications of large language models (LLMs) are promising for biomedical and healthcare research. Despite the availability of open-source LLMs trained using a wide range of biomedical data, current research on the applications of LLMs to genomics and proteomics is still limited. To fill this gap, we propose a collection of finetuned LLMs and multimodal LLMs (MLLMs), known as Geneverse, for three novel tasks in genomic and proteomic research. The models in Geneverse are trained and evaluated based on domain-specific datasets, and we use advanced parameter-efficient finetuning techniques to achieve the model adaptation for tasks including the generation of descriptions for gene functions, protein function inference from its structure, and marker gene selection from spatial transcriptomic data. We demonstrate that adapted LLMs and MLLMs perform well for these tasks and may outperform closed-source large-scale models based on our evaluations focusing on both truthfulness and structural correctness. All of the training strategies and base models we used are freely accessible.",
      "authors": "Tianyu Liu; Yijia Xiao; Xiao Luo; Hua Xu; W. Jim Zheng; Hongyu Zhao",
      "date": "2024-06-21",
      "year": "2024",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2406.14307v2",
      "doi": "",
      "title": "QuST-LLM: Integrating Large Language Models for Comprehensive Spatial Transcriptomics Analysis",
      "abstract": "In this paper, we introduce QuST-LLM, an innovative extension of QuPath that utilizes the capabilities of large language models (LLMs) to analyze and interpret spatial transcriptomics (ST) data. In addition to simplifying the intricate and high-dimensional nature of ST data by offering a comprehensive workflow that includes data loading, region selection, gene expression analysis, and functional annotation, QuST-LLM employs LLMs to transform complex ST data into understandable and detailed biological narratives based on gene ontology annotations, thereby significantly improving the interpretability of ST data. Consequently, users can interact with their own ST data using natural language. Hence, QuST-LLM provides researchers with a potent functionality to unravel the spatial and functional complexities of tissues, fostering novel insights and advancements in biomedical research. QuST-LLM is a part of QuST project. The source code is hosted on GitHub and documentation is available at (https://github.com/huangch/qust).",
      "authors": "Chao Hui Huang",
      "date": "2024-06-20",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.CL",
        "cs.CV"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2406.13292v3",
      "doi": "https://doi.org/10.1088/1741-2552/ae087d",
      "title": "An interpretable generative multimodal neuroimaging-genomics framework for decoding Alzheimer's disease",
      "abstract": "\\textbf{Objective:} Alzheimer's disease (AD) is the most prevalent form of dementia worldwide, encompassing a prodromal stage known as Mild Cognitive Impairment (MCI), where patients may either progress to AD or remain stable. The objective of the work was to capture structural and functional modulations of brain structure and function relying on multimodal MRI data and Single Nucleotide Polymorphisms, also in case of missing views, with the twofold goal of classifying AD patients versus healthy controls and detecting MCI converters. % in two distinct tasks, dealing with also missing data.\\\\ \\textbf{Approach:} We propose a multimodal DL-based classification framework where a generative module employing Cycle Generative Adversarial Networks was introduced in the latent space for imputing missing data (a common issue of multimodal approaches). Explainable AI method was then used to extract input features' relevance allowing for post-hoc validation and enhancing the interpretability of the learned representations. \\textbf{Main results:} Experimental results on two tasks, AD detection and MCI conversion, showed that our framework reached competitive performance in the state-of-the-art with an accuracy of $0.926\\pm0.02$ and $0.711\\pm0.01$ in the two tasks, respectively. The interpretability analysis revealed gray matter modulations in cortical and subcortical brain areas typically associated with AD. Moreover, impairments in sensory-motor and visual resting state networks along the disease continuum, as well as genetic mutations defining biological processes linked to endocytosis, amyloid-beta, and cholesterol, were identified. \\textbf{Significance:} Our integrative and interpretable DL approach shows promising performance for AD detection and MCI prediction while shedding light on important biological insights.",
      "authors": "Giorgio Dolci; Federica Cruciani; Md Abdur Rahaman; Anees Abrol; Jiayu Chen; Zening Fu; Ilaria Boscolo Galazzo; Gloria Menegaz; Vince D. Calhoun",
      "date": "2024-06-19",
      "year": "2024",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "eess.IV"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2406.13133v1",
      "doi": "",
      "title": "PathoLM: Identifying pathogenicity from the DNA sequence through the Genome Foundation Model",
      "abstract": "Pathogen identification is pivotal in diagnosing, treating, and preventing diseases, crucial for controlling infections and safeguarding public health. Traditional alignment-based methods, though widely used, are computationally intense and reliant on extensive reference databases, often failing to detect novel pathogens due to their low sensitivity and specificity. Similarly, conventional machine learning techniques, while promising, require large annotated datasets and extensive feature engineering and are prone to overfitting. Addressing these challenges, we introduce PathoLM, a cutting-edge pathogen language model optimized for the identification of pathogenicity in bacterial and viral sequences. Leveraging the strengths of pre-trained DNA models such as the Nucleotide Transformer, PathoLM requires minimal data for fine-tuning, thereby enhancing pathogen detection capabilities. It effectively captures a broader genomic context, significantly improving the identification of novel and divergent pathogens. We developed a comprehensive data set comprising approximately 30 species of viruses and bacteria, including ESKAPEE pathogens, seven notably virulent bacterial strains resistant to antibiotics. Additionally, we curated a species classification dataset centered specifically on the ESKAPEE group. In comparative assessments, PathoLM dramatically outperforms existing models like DciPatho, demonstrating robust zero-shot and few-shot capabilities. Furthermore, we expanded PathoLM-Sp for ESKAPEE species classification, where it showed superior performance compared to other advanced deep learning methods, despite the complexities of the task.",
      "authors": "Sajib Acharjee Dip; Uddip Acharjee Shuvo; Tran Chau; Haoqiu Song; Petra Choi; Xuan Wang; Liqing Zhang",
      "date": "2024-06-19",
      "year": "2024",
      "categories": [
        "cs.CL",
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2405.18655v1",
      "doi": "",
      "title": "CAVACHON: a hierarchical variational autoencoder to integrate multi-modal single-cell data",
      "abstract": "Paired single-cell sequencing technologies enable the simultaneous measurement of complementary modalities of molecular data at single-cell resolution. Along with the advances in these technologies, many methods based on variational autoencoders have been developed to integrate these data. However, these methods do not explicitly incorporate prior biological relationships between the data modalities, which could significantly enhance modeling and interpretation. We propose a novel probabilistic learning framework that explicitly incorporates conditional independence relationships between multi-modal data as a directed acyclic graph using a generalized hierarchical variational autoencoder. We demonstrate the versatility of our framework across various applications pertinent to single-cell multi-omics data integration. These include the isolation of common and distinct information from different modalities, modality-specific differential analysis, and integrated cell clustering. We anticipate that the proposed framework can facilitate the construction of highly flexible graphical models that can capture the complexities of biological hypotheses and unravel the connections between different biological data types, such as different modalities of paired single-cell multi-omics data. The implementation of the proposed framework can be found in the repository https://github.com/kuijjerlab/CAVACHON.",
      "authors": "Ping-Han Hsieh; Ru-Xiu Hsiao; Katalin Ferenc; Anthony Mathelier; Rebekka Burkholz; Chien-Yu Chen; Geir Kjetil Sandve; Tatiana Belova; Marieke Lydia Kuijjer",
      "date": "2024-05-28",
      "year": "2024",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2405.11280v1",
      "doi": "",
      "title": "Joint Analysis of Single-Cell Data across Cohorts with Missing Modalities",
      "abstract": "Joint analysis of multi-omic single-cell data across cohorts has significantly enhanced the comprehensive analysis of cellular processes. However, most of the existing approaches for this purpose require access to samples with complete modality availability, which is impractical in many real-world scenarios. In this paper, we propose (Single-Cell Cross-Cohort Cross-Category) integration, a novel framework that learns unified cell representations under domain shift without requiring full-modality reference samples. Our generative approach learns rich cross-modal and cross-domain relationships that enable imputation of these missing modalities. Through experiments on real-world multi-omic datasets, we demonstrate that offers a robust solution to single-cell tasks such as cell type clustering, cell type classification, and feature imputation.",
      "authors": "Marianne Arriola; Weishen Pan; Manqi Zhou; Qiannan Zhang; Chang Su; Fei Wang",
      "date": "2024-05-18",
      "year": "2024",
      "categories": [
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2405.10812v2",
      "doi": "",
      "title": "VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling",
      "abstract": "Similar to natural language models, pre-trained genome language models are proposed to capture the underlying intricacies within genomes with unsupervised sequence modeling. They have become essential tools for researchers and practitioners in biology. However, the hand-crafted tokenization policies used in these models may not encode the most discriminative patterns from the limited vocabulary of genomic data. In this paper, we introduce VQDNA, a general-purpose framework that renovates genome tokenization from the perspective of genome vocabulary learning. By leveraging vector-quantized codebooks as learnable vocabulary, VQDNA can adaptively tokenize genomes into pattern-aware embeddings in an end-to-end manner. To further push its limits, we propose Hierarchical Residual Quantization (HRQ), where varying scales of codebooks are designed in a hierarchy to enrich the genome vocabulary in a coarse-to-fine manner. Extensive experiments on 32 genome datasets demonstrate VQDNA's superiority and favorable parameter efficiency compared to existing genome language models. Notably, empirical analysis of SARS-CoV-2 mutations reveals the fine-grained pattern awareness and biological significance of learned HRQ vocabulary, highlighting its untapped potential for broader applications in genomics.",
      "authors": "Siyuan Li; Zedong Wang; Zicheng Liu; Di Wu; Cheng Tan; Jiangbin Zheng; Yufei Huang; Stan Z. Li",
      "date": "2024-05-13",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2403.11375v1",
      "doi": "",
      "title": "Path-GPTOmic: A Balanced Multi-modal Learning Framework for Survival Outcome Prediction",
      "abstract": "For predicting cancer survival outcomes, standard approaches in clinical research are often based on two main modalities: pathology images for observing cell morphology features, and genomic (e.g., bulk RNA-seq) for quantifying gene expressions. However, existing pathology-genomic multi-modal algorithms face significant challenges: (1) Valuable biological insights regarding genes and gene-gene interactions are frequently overlooked; (2) one modality often dominates the optimization process, causing inadequate training for the other modality. In this paper, we introduce a new multi-modal ``Path-GPTOmic\" framework for cancer survival outcome prediction. First, to extract valuable biological insights, we regulate the embedding space of a foundation model, scGPT, initially trained on single-cell RNA-seq data, making it adaptable for bulk RNA-seq data. Second, to address the imbalance-between-modalities problem, we propose a gradient modulation mechanism tailored to the Cox partial likelihood loss for survival prediction. The contributions of the modalities are dynamically monitored and adjusted during the training process, encouraging that both modalities are sufficiently trained. Evaluated on two TCGA(The Cancer Genome Atlas) datasets, our model achieves substantially improved survival prediction accuracy.",
      "authors": "Hongxiao Wang; Yang Yang; Zhuo Zhao; Pengfei Gu; Nishchal Sapkota; Danny Z. Chen",
      "date": "2024-03-18",
      "year": "2024",
      "categories": [
        "cs.CV",
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2403.06338v1",
      "doi": "",
      "title": "Disentangling shared and private latent factors in multimodal Variational Autoencoders",
      "abstract": "Generative models for multimodal data permit the identification of latent factors that may be associated with important determinants of observed data heterogeneity. Common or shared factors could be important for explaining variation across modalities whereas other factors may be private and important only for the explanation of a single modality. Multimodal Variational Autoencoders, such as MVAE and MMVAE, are a natural choice for inferring those underlying latent factors and separating shared variation from private. In this work, we investigate their capability to reliably perform this disentanglement. In particular, we highlight a challenging problem setting where modality-specific variation dominates the shared signal. Taking a cross-modal prediction perspective, we demonstrate limitations of existing models, and propose a modification how to make them more robust to modality-specific variation. Our findings are supported by experiments on synthetic as well as various real-world multi-omics data sets.",
      "authors": "Kaspar Märtens; Christopher Yau",
      "date": "2024-03-10",
      "year": "2024",
      "categories": [
        "stat.ML",
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2402.12405v1",
      "doi": "",
      "title": "scInterpreter: Training Large Language Models to Interpret scRNA-seq Data for Cell Type Annotation",
      "abstract": "Despite the inherent limitations of existing Large Language Models in directly reading and interpreting single-cell omics data, they demonstrate significant potential and flexibility as the Foundation Model. This research focuses on how to train and adapt the Large Language Model with the capability to interpret and distinguish cell types in single-cell RNA sequencing data. Our preliminary research results indicate that these foundational models excel in accurately categorizing known cell types, demonstrating the potential of the Large Language Models as effective tools for uncovering new biological insights.",
      "authors": "Cong Li; Meng Xiao; Pengfei Wang; Guihai Feng; Xin Li; Yuanchun Zhou",
      "date": "2024-02-18",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2402.08303v4",
      "doi": "",
      "title": "ChatCell: Facilitating Single-Cell Analysis with Natural Language",
      "abstract": "As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to deepen single-cell insights, paving the way for more accessible and intuitive exploration in this pivotal field. Our project homepage is available at https://zjunlp.github.io/project/ChatCell.",
      "authors": "Yin Fang; Kangwei Liu; Ningyu Zhang; Xinle Deng; Penghui Yang; Zhuo Chen; Xiangru Tang; Mark Gerstein; Xiaohui Fan; Huajun Chen",
      "date": "2024-02-13",
      "year": "2024",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE",
        "cs.HC",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2402.08075v1",
      "doi": "",
      "title": "Efficient and Scalable Fine-Tune of Language Models for Genome Understanding",
      "abstract": "Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics. Here, we present \\textsc{Lingo}: \\textsc{L}anguage prefix f\\textsc{In}e-tuning for \\textsc{G}en\\textsc{O}mes. Unlike DNA foundation models, \\textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences. \\textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors within small computational budgets. Adaptive rank sampling outperformed existing fine-tuning methods on all benchmarked 14 genome understanding tasks, while requiring fewer than 2\\% of trainable parameters as genomic-specific adapters. Impressively, applying these adapters on natural language foundation models matched or even exceeded the performance of DNA foundation models. \\textsc{Lingo} presents a new paradigm of efficient and scalable genome understanding via genomic-specific adapters on language models.",
      "authors": "Huixin Zhan; Ying Nian Wu; Zijun Zhang",
      "date": "2024-02-12",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2401.07543v1",
      "doi": "",
      "title": "Must: Maximizing Latent Capacity of Spatial Transcriptomics Data",
      "abstract": "Spatial transcriptomics (ST) technologies have revolutionized the study of gene expression patterns in tissues by providing multimodality data in transcriptomic, spatial, and morphological, offering opportunities for understanding tissue biology beyond transcriptomics. However, we identify the modality bias phenomenon in ST data species, i.e., the inconsistent contribution of different modalities to the labels leads to a tendency for the analysis methods to retain the information of the dominant modality. How to mitigate the adverse effects of modality bias to satisfy various downstream tasks remains a fundamental challenge. This paper introduces Multiple-modality Structure Transformation, named MuST, a novel methodology to tackle the challenge. MuST integrates the multi-modality information contained in the ST data effectively into a uniform latent space to provide a foundation for all the downstream tasks. It learns intrinsic local structures by topology discovery strategy and topology fusion loss function to solve the inconsistencies among different modalities. Thus, these topology-based and deep learning techniques provide a solid foundation for a variety of analytical tasks while coordinating different modalities. The effectiveness of MuST is assessed by performance metrics and biological significance. The results show that it outperforms existing state-of-the-art methods with clear advantages in the precision of identifying and preserving structures of tissues and biomarkers. MuST offers a versatile toolkit for the intricate analysis of complex biological systems.",
      "authors": "Zelin Zang; Liangyu Li; Yongjie Xu; Chenrui Duan; Kai Wang; Yang You; Yi Sun; Stan Z. Li",
      "date": "2024-01-15",
      "year": "2024",
      "categories": [
        "cs.CE",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2401.04155v2",
      "doi": "",
      "title": "Advancing bioinformatics with large language models: components, applications and perspectives",
      "abstract": "Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will provide a comprehensive overview of the essential components of large language models (LLMs) in bioinformatics, spanning genomics, transcriptomics, proteomics, drug discovery, and single-cell analysis. Key aspects covered include tokenization methods for diverse data types, the architecture of transformer models, the core attention mechanism, and the pre-training processes underlying these models. Additionally, we will introduce currently available foundation models and highlight their downstream applications across various bioinformatics domains. Finally, drawing from our experience, we will offer practical guidance for both LLM users and developers, emphasizing strategies to optimize their use and foster further innovation in the field.",
      "authors": "Jiajia Liu; Mengyuan Yang; Yankai Yu; Haixia Xu; Tiangang Wang; Kang Li; Xiaobo Zhou",
      "date": "2024-01-08",
      "year": "2024",
      "categories": [
        "q-bio.QM",
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2311.11659v1",
      "doi": "",
      "title": "MGCT: Mutual-Guided Cross-Modality Transformer for Survival Outcome Prediction using Integrative Histopathology-Genomic Features",
      "abstract": "The rapidly emerging field of deep learning-based computational pathology has shown promising results in utilizing whole slide images (WSIs) to objectively prognosticate cancer patients. However, most prognostic methods are currently limited to either histopathology or genomics alone, which inevitably reduces their potential to accurately predict patient prognosis. Whereas integrating WSIs and genomic features presents three main challenges: (1) the enormous heterogeneity of gigapixel WSIs which can reach sizes as large as 150,000x150,000 pixels; (2) the absence of a spatially corresponding relationship between histopathology images and genomic molecular data; and (3) the existing early, late, and intermediate multimodal feature fusion strategies struggle to capture the explicit interactions between WSIs and genomics. To ameliorate these issues, we propose the Mutual-Guided Cross-Modality Transformer (MGCT), a weakly-supervised, attention-based multimodal learning framework that can combine histology features and genomic features to model the genotype-phenotype interactions within the tumor microenvironment. To validate the effectiveness of MGCT, we conduct experiments using nearly 3,600 gigapixel WSIs across five different cancer types sourced from The Cancer Genome Atlas (TCGA). Extensive experimental results consistently emphasize that MGCT outperforms the state-of-the-art (SOTA) methods.",
      "authors": "Mingxin Liu; Yunzan Liu; Hui Cui; Chunquan Li; Jiquan Ma",
      "date": "2023-11-20",
      "year": "2023",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2311.07621v1",
      "doi": "",
      "title": "To Transformers and Beyond: Large Language Models for the Genome",
      "abstract": "In the rapidly evolving landscape of genomics, deep learning has emerged as a useful tool for tackling complex computational challenges. This review focuses on the transformative role of Large Language Models (LLMs), which are mostly based on the transformer architecture, in genomics. Building on the foundation of traditional convolutional neural networks and recurrent neural networks, we explore both the strengths and limitations of transformers and other LLMs for genomics. Additionally, we contemplate the future of genomic modeling beyond the transformer architecture based on current trends in research. The paper aims to serve as a guide for computational biologists and computer scientists interested in LLMs for genomic data. We hope the paper can also serve as an educational introduction and discussion for biologists to a fundamental shift in how we will be analyzing genomic data in the future.",
      "authors": "Micaela E. Consens; Cameron Dufault; Michael Wainberg; Duncan Forster; Mehran Karimzadeh; Hani Goodarzi; Fabian J. Theis; Alan Moses; Bo Wang",
      "date": "2023-11-13",
      "year": "2023",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2310.02275v1",
      "doi": "",
      "title": "MuSe-GNN: Learning Unified Gene Representation From Multimodal Biological Graph Data",
      "abstract": "Discovering genes with similar functions across diverse biomedical contexts poses a significant challenge in gene representation learning due to data heterogeneity. In this study, we resolve this problem by introducing a novel model called Multimodal Similarity Learning Graph Neural Network, which combines Multimodal Machine Learning and Deep Graph Neural Networks to learn gene representations from single-cell sequencing and spatial transcriptomic data. Leveraging 82 training datasets from 10 tissues, three sequencing techniques, and three species, we create informative graph structures for model training and gene representations generation, while incorporating regularization with weighted similarity learning and contrastive learning to learn cross-data gene-gene relationships. This novel design ensures that we can offer gene representations containing functional similarity across different contexts in a joint space. Comprehensive benchmarking analysis shows our model's capacity to effectively capture gene function similarity across multiple modalities, outperforming state-of-the-art methods in gene representation learning by up to 97.5%. Moreover, we employ bioinformatics tools in conjunction with gene representations to uncover pathway enrichment, regulation causal networks, and functions of disease-associated or dosage-sensitive genes. Therefore, our model efficiently produces unified gene representations for the analysis of gene functions, tissue functions, diseases, and species evolution.",
      "authors": "Tianyu Liu; Yuge Wang; Rex Ying; Hongyu Zhao",
      "date": "2023-09-29",
      "year": "2023",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2308.05864v2",
      "doi": "https://doi.org/10.1038/s41592-024-02233-6",
      "title": "The Multi-modality Cell Segmentation Challenge: Towards Universal Solutions",
      "abstract": "Cell segmentation is a critical step for quantitative single-cell analysis in microscopy images. Existing cell segmentation methods are often tailored to specific modalities or require manual interventions to specify hyper-parameters in different experimental settings. Here, we present a multi-modality cell segmentation benchmark, comprising over 1500 labeled images derived from more than 50 diverse biological experiments. The top participants developed a Transformer-based deep-learning algorithm that not only exceeds existing methods but can also be applied to diverse microscopy images across imaging platforms and tissue types without manual parameter adjustments. This benchmark and the improved algorithm offer promising avenues for more accurate and versatile cell analysis in microscopy imaging.",
      "authors": "Jun Ma; Ronald Xie; Shamini Ayyadhury; Cheng Ge; Anubha Gupta; Ritu Gupta; Song Gu; Yao Zhang; Gihun Lee; Joonkee Kim; Wei Lou; Haofeng Li; Eric Upschulte; Timo Dickscheid; José Guilherme de Almeida; Yixin Wang; Lin Han; Xin Yang; Marco Labagnara; Vojislav Gligorovski; Maxime Scheder; Sahand Jamal Rahi; Carly Kempster; Alice Pollitt; Leon Espinosa; Tâm Mignot; Jan Moritz Middeke; Jan-Niklas Eckardt; Wangkai Li; Zhaoyang Li; Xiaochen Cai; Bizhe Bai; Noah F. Greenwald; David Van Valen; Erin Weisbart; Beth A. Cimini; Trevor Cheung; Oscar Brück; Gary D. Bader; Bo Wang",
      "date": "2023-08-10",
      "year": "2023",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2308.06288v1",
      "doi": "",
      "title": "Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei with Histology and Spatial Transcriptomics Data in Renal Pathology",
      "abstract": "Podocytes, specialized epithelial cells that envelop the glomerular capillaries, play a pivotal role in maintaining renal health. The current description and quantification of features on pathology slides are limited, prompting the need for innovative solutions to comprehensively assess diverse phenotypic attributes within Whole Slide Images (WSIs). In particular, understanding the morphological characteristics of podocytes, terminally differentiated glomerular epithelial cells, is crucial for studying glomerular injury. This paper introduces the Spatial Pathomics Toolkit (SPT) and applies it to podocyte pathomics. The SPT consists of three main components: (1) instance object segmentation, enabling precise identification of podocyte nuclei; (2) pathomics feature generation, extracting a comprehensive array of quantitative features from the identified nuclei; and (3) robust statistical analyses, facilitating a comprehensive exploration of spatial relationships between morphological and spatial transcriptomics features.The SPT successfully extracted and analyzed morphological and textural features from podocyte nuclei, revealing a multitude of podocyte morphomic features through statistical analysis. Additionally, we demonstrated the SPT's ability to unravel spatial information inherent to podocyte distribution, shedding light on spatial patterns associated with glomerular injury. By disseminating the SPT, our goal is to provide the research community with a powerful and user-friendly resource that advances cellular spatial pathomics in renal pathology. The implementation and its complete source code of the toolkit are made openly accessible at https://github.com/hrlblab/spatial_pathomics.",
      "authors": "Jiayuan Chen; Yu Wang; Ruining Deng; Quan Liu; Can Cui; Tianyuan Yao; Yilin Liu; Jianyong Zhong; Agnes B. Fogo; Haichun Yang; Shilin Zhao; Yuankai Huo",
      "date": "2023-08-10",
      "year": "2023",
      "categories": [
        "q-bio.QM",
        "cs.CV",
        "eess.IV"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2307.11952v1",
      "doi": "",
      "title": "Pathology-and-genomics Multimodal Transformer for Survival Outcome Prediction",
      "abstract": "Survival outcome assessment is challenging and inherently associated with multiple clinical factors (e.g., imaging and genomics biomarkers) in cancer. Enabling multimodal analytics promises to reveal novel predictive patterns of patient outcomes. In this study, we propose a multimodal transformer (PathOmics) integrating pathology and genomics insights into colon-related cancer survival prediction. We emphasize the unsupervised pretraining to capture the intrinsic interaction between tissue microenvironments in gigapixel whole slide images (WSIs) and a wide range of genomics data (e.g., mRNA-sequence, copy number variant, and methylation). After the multimodal knowledge aggregation in pretraining, our task-specific model finetuning could expand the scope of data utility applicable to both multi- and single-modal data (e.g., image- or genomics-only). We evaluate our approach on both TCGA colon and rectum cancer cohorts, showing that the proposed approach is competitive and outperforms state-of-the-art studies. Finally, our approach is desirable to utilize the limited number of finetuned samples towards data-efficient analytics for survival outcome prediction. The code is available at https://github.com/Cassie07/PathOmics.",
      "authors": "Kexin Ding; Mu Zhou; Dimitris N. Metaxas; Shaoting Zhang",
      "date": "2023-07-22",
      "year": "2023",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2307.02502v1",
      "doi": "",
      "title": "Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics",
      "abstract": "The advancement in generative AI could be boosted with more accessible mathematics. Beyond human-AI chat, large language models (LLMs) are emerging in programming, algorithm discovery, and theorem proving, yet their genomics application is limited. This project introduces Math Agents and mathematical embedding as fresh entries to the \"Moore's Law of Mathematics\", using a GPT-based workflow to convert equations from literature into LaTeX and Python formats. While many digital equation representations exist, there's a lack of automated large-scale evaluation tools. LLMs are pivotal as linguistic user interfaces, providing natural language access for human-AI chat and formal languages for large-scale AI-assisted computational infrastructure. Given the infinite formal possibility spaces, Math Agents, which interact with math, could potentially shift us from \"big data\" to \"big math\". Math, unlike the more flexible natural language, has properties subject to proof, enabling its use beyond traditional applications like high-validation math-certified icons for AI alignment aims. This project aims to use Math Agents and mathematical embeddings to address the ageing issue in information systems biology by applying multiscalar physics mathematics to disease models and genomic data. Generative AI with episodic memory could help analyse causal relations in longitudinal health records, using SIR Precision Health models. Genomic data is suggested for addressing the unsolved Alzheimer's disease problem.",
      "authors": "Melanie Swan; Takashi Kido; Eric Roland; Renato P. dos Santos",
      "date": "2023-07-04",
      "year": "2023",
      "categories": [
        "q-bio.OT",
        "cs.AI",
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2306.15794v2",
      "doi": "",
      "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
      "abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.",
      "authors": "Eric Nguyen; Michael Poli; Marjan Faizi; Armin Thomas; Callum Birch-Sykes; Michael Wornow; Aman Patel; Clayton Rabideau; Stefano Massaroli; Yoshua Bengio; Stefano Ermon; Stephen A. Baccus; Chris Ré",
      "date": "2023-06-27",
      "year": "2023",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2304.13084v1",
      "doi": "",
      "title": "Representing and extracting knowledge from single cell data",
      "abstract": "Single-cell analysis is currently one of the most high-resolution techniques to study biology. The large complex datasets that have been generated have spurred numerous developments in computational biology, in particular the use of advanced statistics and machine learning. This review attempts to explain the deeper theoretical concepts that underpin current state-of-the-art analysis methods. Single-cell analysis is covered from cell, through instruments, to current and upcoming models. A minimum of mathematics and statistics has been used, but the reader is assumed to either have basic knowledge of single-cell analysis workflows, or have a solid knowledge of statistics. The aim of this review is to spread concepts which are not yet in common use, especially from topology and generative processes, and how new statistical models can be developed to capture more of biology. This opens epistemological questions regarding our ontology and models, and some pointers will be given to how natural language processing (NLP) may help overcome our cognitive limitations for understanding single-cell data.",
      "authors": "Ionut Sebastian Mihai; Sarang Chafle; Johan Henriksson",
      "date": "2023-04-25",
      "year": "2023",
      "categories": [
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2303.00233v3",
      "doi": "https://doi.org/10.1145/3583780.3615061",
      "title": "Single-Cell Multimodal Prediction via Transformers",
      "abstract": "The recent development of multimodal single-cell technology has made the possibility of acquiring multiple omics data from individual cells, thereby enabling a deeper understanding of cellular states and dynamics. Nevertheless, the proliferation of multimodal single-cell data also introduces tremendous challenges in modeling the complex interactions among different modalities. The recently advanced methods focus on constructing static interaction graphs and applying graph neural networks (GNNs) to learn from multimodal data. However, such static graphs can be suboptimal as they do not take advantage of the downstream task information; meanwhile GNNs also have some inherent limitations when deeply stacking GNN layers. To tackle these issues, in this work, we investigate how to leverage transformers for multimodal single-cell data in an end-to-end manner while exploiting downstream task information. In particular, we propose a scMoFormer framework which can readily incorporate external domain knowledge and model the interactions within each modality and cross modalities. Extensive experiments demonstrate that scMoFormer achieves superior performance on various benchmark datasets. Remarkably, scMoFormer won a Kaggle silver medal with the rank of 24/1221 (Top 2%) without ensemble in a NeurIPS 2022 competition. Our implementation is publicly available at Github.",
      "authors": "Wenzhuo Tang; Hongzhi Wen; Renming Liu; Jiayuan Ding; Wei Jin; Yuying Xie; Hui Liu; Jiliang Tang",
      "date": "2023-03-01",
      "year": "2023",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2302.10907v1",
      "doi": "",
      "title": "Diffusion Models in Bioinformatics: A New Wave of Deep Learning Revolution in Action",
      "abstract": "Denoising diffusion models have emerged as one of the most powerful generative models in recent years. They have achieved remarkable success in many fields, such as computer vision, natural language processing (NLP), and bioinformatics. Although there are a few excellent reviews on diffusion models and their applications in computer vision and NLP, there is a lack of an overview of their applications in bioinformatics. This review aims to provide a rather thorough overview of the applications of diffusion models in bioinformatics to aid their further development in bioinformatics and computational biology. We start with an introduction of the key concepts and theoretical foundations of three cornerstone diffusion modeling frameworks (denoising diffusion probabilistic models, noise-conditioned scoring networks, and stochastic differential equations), followed by a comprehensive description of diffusion models employed in the different domains of bioinformatics, including cryo-EM data enhancement, single-cell data analysis, protein design and generation, drug and small molecule design, and protein-ligand interaction. The review is concluded with a summary of the potential new development and applications of diffusion models in bioinformatics.",
      "authors": "Zhiye Guo; Jian Liu; Yanli Wang; Mengrui Chen; Duolin Wang; Dong Xu; Jianlin Cheng",
      "date": "2023-02-13",
      "year": "2023",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2301.02383v1",
      "doi": "",
      "title": "Deep Biological Pathway Informed Pathology-Genomic Multimodal Survival Prediction",
      "abstract": "The integration of multi-modal data, such as pathological images and genomic data, is essential for understanding cancer heterogeneity and complexity for personalized treatments, as well as for enhancing survival predictions. Despite the progress made in integrating pathology and genomic data, most existing methods cannot mine the complex inter-modality relations thoroughly. Additionally, identifying explainable features from these models that govern preclinical discovery and clinical prediction is crucial for cancer diagnosis, prognosis, and therapeutic response studies. We propose PONET- a novel biological pathway-informed pathology-genomic deep model that integrates pathological images and genomic data not only to improve survival prediction but also to identify genes and pathways that cause different survival rates in patients. Empirical results on six of The Cancer Genome Atlas (TCGA) datasets show that our proposed method achieves superior predictive performance and reveals meaningful biological interpretations. The proposed method establishes insight into how to train biologically informed deep networks on multimodal biomedical data which will have general applicability for understanding diseases and predicting response and resistance to treatment.",
      "authors": "Lin Qiu; Aminollah Khormali; Kai Liu",
      "date": "2023-01-06",
      "year": "2023",
      "categories": [
        "q-bio.QM",
        "cs.LG",
        "eess.IV",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2211.16632v1",
      "doi": "",
      "title": "Hierarchical Transformer for Survival Prediction Using Multimodality Whole Slide Images and Genomics",
      "abstract": "Learning good representation of giga-pixel level whole slide pathology images (WSI) for downstream tasks is critical. Previous studies employ multiple instance learning (MIL) to represent WSIs as bags of sampled patches because, for most occasions, only slide-level labels are available, and only a tiny region of the WSI is disease-positive area. However, WSI representation learning still remains an open problem due to: (1) patch sampling on a higher resolution may be incapable of depicting microenvironment information such as the relative position between the tumor cells and surrounding tissues, while patches at lower resolution lose the fine-grained detail; (2) extracting patches from giant WSI results in large bag size, which tremendously increases the computational cost. To solve the problems, this paper proposes a hierarchical-based multimodal transformer framework that learns a hierarchical mapping between pathology images and corresponding genes. Precisely, we randomly extract instant-level patch features from WSIs with different magnification. Then a co-attention mapping between imaging and genomics is learned to uncover the pairwise interaction and reduce the space complexity of imaging features. Such early fusion makes it computationally feasible to use MIL Transformer for the survival prediction task. Our architecture requires fewer GPU resources compared with benchmark methods while maintaining better WSI representation ability. We evaluate our approach on five cancer types from the Cancer Genome Atlas database and achieved an average c-index of $0.673$, outperforming the state-of-the-art multimodality methods.",
      "authors": "Chunyuan Li; Xinliang Zhu; Jiawen Yao; Junzhou Huang",
      "date": "2022-11-29",
      "year": "2022",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2211.02920v3",
      "doi": "",
      "title": "GmGM: a Fast Multi-Axis Gaussian Graphical Model",
      "abstract": "This paper introduces the Gaussian multi-Graphical Model, a model to construct sparse graph representations of matrix- and tensor-variate data. We generalize prior work in this area by simultaneously learning this representation across several tensors that share axes, which is necessary to allow the analysis of multimodal datasets such as those encountered in multi-omics. Our algorithm uses only a single eigendecomposition per axis, achieving an order of magnitude speedup over prior work in the ungeneralized case. This allows the use of our methodology on large multi-modal datasets such as single-cell multi-omics data, which was challenging with previous approaches. We validate our model on synthetic data and five real-world datasets.",
      "authors": "Bailey Andrew; David Westhead; Luisa Cutillo",
      "date": "2022-11-05",
      "year": "2022",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2210.14330v1",
      "doi": "",
      "title": "A single-cell gene expression language model",
      "abstract": "Gene regulation is a dynamic process that connects genotype and phenotype. Given the difficulty of physically mapping mammalian gene circuitry, we require new computational methods to learn regulatory rules. Natural language is a valuable analogy to the communication of regulatory control. Machine learning systems model natural language by explicitly learning context dependencies between words. We propose a similar system applied to single-cell RNA expression profiles to learn context dependencies between genes. Our model, Exceiver, is trained across a diversity of cell types using a self-supervised task formulated for discrete count data, accounting for feature sparsity. We found agreement between the similarity profiles of latent sample representations and learned gene embeddings with respect to biological annotations. We evaluated Exceiver on a new dataset and a downstream prediction task and found that pretraining supports transfer learning. Our work provides a framework to model gene regulation on a single-cell level and transfer knowledge to downstream tasks.",
      "authors": "William Connell; Umair Khan; Michael J. Keiser",
      "date": "2022-10-25",
      "year": "2022",
      "categories": [
        "q-bio.QM",
        "cs.AI",
        "q-bio.MN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2205.14923v3",
      "doi": "",
      "title": "Unbalanced CO-Optimal Transport",
      "abstract": "Optimal transport (OT) compares probability distributions by computing a meaningful alignment between their samples. CO-optimal transport (COOT) takes this comparison further by inferring an alignment between features as well. While this approach leads to better alignments and generalizes both OT and Gromov-Wasserstein distances, we provide a theoretical result showing that it is sensitive to outliers that are omnipresent in real-world data. This prompts us to propose unbalanced COOT for which we provably show its robustness to noise in the compared datasets. To the best of our knowledge, this is the first such result for OT methods in incomparable spaces. With this result in hand, we provide empirical evidence of this robustness for the challenging tasks of heterogeneous domain adaptation with and without varying proportions of classes and simultaneous alignment of samples and features across single-cell measurements.",
      "authors": "Quang Huy Tran; Hicham Janati; Nicolas Courty; Rémi Flamary; Ievgen Redko; Pinar Demetci; Ritambhara Singh",
      "date": "2022-05-30",
      "year": "2022",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2205.01133v2",
      "doi": "",
      "title": "Hausa Visual Genome: A Dataset for Multi-Modal English to Hausa Machine Translation",
      "abstract": "Multi-modal Machine Translation (MMT) enables the use of visual information to enhance the quality of translations. The visual information can serve as a valuable piece of context information to decrease the ambiguity of input sentences. Despite the increasing popularity of such a technique, good and sizeable datasets are scarce, limiting the full extent of their potential. Hausa, a Chadic language, is a member of the Afro-Asiatic language family. It is estimated that about 100 to 150 million people speak the language, with more than 80 million indigenous speakers. This is more than any of the other Chadic languages. Despite a large number of speakers, the Hausa language is considered low-resource in natural language processing (NLP). This is due to the absence of sufficient resources to implement most NLP tasks. While some datasets exist, they are either scarce, machine-generated, or in the religious domain. Therefore, there is a need to create training and evaluation data for implementing machine learning tasks and bridging the research gap in the language. This work presents the Hausa Visual Genome (HaVG), a dataset that contains the description of an image or a section within the image in Hausa and its equivalent in English. To prepare the dataset, we started by translating the English description of the images in the Hindi Visual Genome (HVG) into Hausa automatically. Afterward, the synthetic Hausa data was carefully post-edited considering the respective images. The dataset comprises 32,923 images and their descriptions that are divided into training, development, test, and challenge test set. The Hausa Visual Genome is the first dataset of its kind and can be used for Hausa-English machine translation, multi-modal research, and image description, among various other natural language processing and generation tasks.",
      "authors": "Idris Abdulmumin; Satya Ranjan Dash; Musa Abdullahi Dawud; Shantipriya Parida; Shamsuddeen Hassan Muhammad; Ibrahim Sa'id Ahmad; Subhadarshi Panda; Ondřej Bojar; Bashir Shehu Galadanci; Bello Shehu Bello",
      "date": "2022-05-02",
      "year": "2022",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2203.04419v2",
      "doi": "",
      "title": "Survival Prediction of Brain Cancer with Incomplete Radiology, Pathology, Genomics, and Demographic Data",
      "abstract": "Integrating cross-department multi-modal data (e.g., radiological, pathological, genomic, and clinical data) is ubiquitous in brain cancer diagnosis and survival prediction. To date, such an integration is typically conducted by human physicians (and panels of experts), which can be subjective and semi-quantitative. Recent advances in multi-modal deep learning, however, have opened a door to leverage such a process to a more objective and quantitative manner. Unfortunately, the prior arts of using four modalities on brain cancer survival prediction are limited by a \"complete modalities\" setting (i.e., with all modalities available). Thus, there are still open questions on how to effectively predict brain cancer survival from the incomplete radiological, pathological, genomic, and demographic data (e.g., one or more modalities might not be collected for a patient). For instance, should we use both complete and incomplete data, and more importantly, how to use those data? To answer the preceding questions, we generalize the multi-modal learning on cross-department multi-modal data to a missing data setting. Our contribution is three-fold: 1) We introduce optimal multi-modal learning with missing data (MMD) pipeline with optimized hardware consumption and computational efficiency; 2) We extend multi-modal learning on radiological, pathological, genomic, and demographic data into missing data scenarios; 3) a large-scale public dataset (with 962 patients) is collected to systematically evaluate glioma tumor survival prediction using four modalities. The proposed method improved the C-index of survival prediction from 0.7624 to 0.8053.",
      "authors": "Can Cui; Han Liu; Quan Liu; Ruining Deng; Zuhayr Asad; Yaohong WangShilin Zhao; Haichun Yang; Bennett A. Landman; Yuankai Huo",
      "date": "2022-03-08",
      "year": "2022",
      "categories": [
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2203.01884v3",
      "doi": "https://doi.org/10.1145/3534678.3539213",
      "title": "Graph Neural Networks for Multimodal Single-Cell Data Integration",
      "abstract": "Recent advances in multimodal single-cell technologies have enabled simultaneous acquisitions of multiple omics data from the same cell, providing deeper insights into cellular states and dynamics. However, it is challenging to learn the joint representations from the multimodal data, model the relationship between modalities, and, more importantly, incorporate the vast amount of single-modality datasets into the downstream analyses. To address these challenges and correspondingly facilitate multimodal single-cell data analyses, three key tasks have been introduced: $\\textit{modality prediction}$, $\\textit{modality matching}$ and $\\textit{joint embedding}$. In this work, we present a general Graph Neural Network framework $\\textit{scMoGNN}$ to tackle these three tasks and show that $\\textit{scMoGNN}$ demonstrates superior results in all three tasks compared with the state-of-the-art and conventional approaches. Our method is an official winner in the overall ranking of $\\textit{Modality prediction}$ from NeurIPS 2021 Competition, and all implementations of our methods have been integrated into DANCE package~\\url{https://github.com/OmicsML/dance}.",
      "authors": "Hongzhi Wen; Jiayuan Ding; Wei Jin; Yiqi Wang; Yuying Xie; Jiliang Tang",
      "date": "2022-03-03",
      "year": "2022",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2112.10068v1",
      "doi": "",
      "title": "Lerna: Transformer Architectures for Configuring Error Correction Tools for Short- and Long-Read Genome Sequencing",
      "abstract": "Sequencing technologies are prone to errors, making error correction (EC) necessary for downstream applications. EC tools need to be manually configured for optimal performance. We find that the optimal parameters (e.g., k-mer size) are both tool- and dataset-dependent. Moreover, evaluating the performance (i.e., Alignment-rate or Gain) of a given tool usually relies on a reference genome, but quality reference genomes are not always available. We introduce Lerna for the automated configuration of k-mer-based EC tools. Lerna first creates a language model (LM) of the uncorrected genomic reads; then, calculates the perplexity metric to evaluate the corrected reads for different parameter choices. Next, it finds the one that produces the highest alignment rate without using a reference genome. The fundamental intuition of our approach is that the perplexity metric is inversely correlated with the quality of the assembly after error correction. Results: First, we show that the best k-mer value can vary for different datasets, even for the same EC tool. Second, we show the gains of our LM using its component attention-based transformers. We show the model's estimation of the perplexity metric before and after error correction. The lower the perplexity after correction, the better the k-mer size. We also show that the alignment rate and assembly quality computed for the corrected reads are strongly negatively correlated with the perplexity, enabling the automated selection of k-mer values for better error correction, and hence, improved assembly quality. Additionally, we show that our attention-based models have significant runtime improvement for the entire pipeline -- 18X faster than previous works, due to parallelizing the attention mechanism and the use of JIT compilation for GPU inferencing.",
      "authors": "Atul Sharma; Pranjal Jain; Ashraf Mahgoub; Zihan Zhou; Kanak Mahadik; Somali Chaterji",
      "date": "2021-12-19",
      "year": "2021",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2111.13987v1",
      "doi": "",
      "title": "Multi-modality fusion using canonical correlation analysis methods: Application in breast cancer survival prediction from histology and genomics",
      "abstract": "The availability of multi-modality datasets provides a unique opportunity to characterize the same object of interest using multiple viewpoints more comprehensively. In this work, we investigate the use of canonical correlation analysis (CCA) and penalized variants of CCA (pCCA) for the fusion of two modalities. We study a simple graphical model for the generation of two-modality data. We analytically show that, with known model parameters, posterior mean estimators that jointly use both modalities outperform arbitrary linear mixing of single modality posterior estimators in latent variable prediction. Penalized extensions of CCA (pCCA) that incorporate domain knowledge can discover correlations with high-dimensional, low-sample data, whereas traditional CCA is inapplicable. To facilitate the generation of multi-dimensional embeddings with pCCA, we propose two matrix deflation schemes that enforce desirable properties exhibited by CCA. We propose a two-stage prediction pipeline using pCCA embeddings generated with deflation for latent variable prediction by combining all the above. On simulated data, our proposed model drastically reduces the mean-squared error in latent variable prediction. When applied to publicly available histopathology data and RNA-sequencing data from The Cancer Genome Atlas (TCGA) breast cancer patients, our model can outperform principal components analysis (PCA) embeddings of the same dimension in survival prediction.",
      "authors": "Vaishnavi Subramanian; Tanveer Syeda-Mahmood; Minh N. Do",
      "date": "2021-11-27",
      "year": "2021",
      "categories": [
        "cs.LG",
        "eess.SP",
        "q-bio.QM",
        "stat.AP"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2110.05231v2",
      "doi": "",
      "title": "Multi-modal Self-supervised Pre-training for Regulatory Genome Across Cell Types",
      "abstract": "In the genome biology research, regulatory genome modeling is an important topic for many regulatory downstream tasks, such as promoter classification, transaction factor binding sites prediction. The core problem is to model how regulatory elements interact with each other and its variability across different cell types. However, current deep learning methods often focus on modeling genome sequences of a fixed set of cell types and do not account for the interaction between multiple regulatory elements, making them only perform well on the cell types in the training set and lack the generalizability required in biological applications. In this work, we propose a simple yet effective approach for pre-training genome data in a multi-modal and self-supervised manner, which we call GeneBERT. Specifically, we simultaneously take the 1d sequence of genome data and a 2d matrix of (transcription factors x regions) as the input, where three pre-training tasks are proposed to improve the robustness and generalizability of our model. We pre-train our model on the ATAC-seq dataset with 17 million genome sequences. We evaluate our GeneBERT on regulatory downstream tasks across different cell types, including promoter classification, transaction factor binding sites prediction, disease risk estimation, and splicing sites prediction. Extensive experiments demonstrate the effectiveness of multi-modal and self-supervised pre-training for large-scale regulatory genomics data.",
      "authors": "Shentong Mo; Xi Fu; Chenyang Hong; Yizhen Chen; Yuxuan Zheng; Xiangru Tang; Zhiqiang Shen; Eric P Xing; Yanyan Lan",
      "date": "2021-10-11",
      "year": "2021",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2102.06757v3",
      "doi": "",
      "title": "Multimodal Data Visualization and Denoising with Integrated Diffusion",
      "abstract": "We propose a method called integrated diffusion for combining multimodal datasets, or data gathered via several different measurements on the same system, to create a joint data diffusion operator. As real world data suffers from both local and global noise, we introduce mechanisms to optimally calculate a diffusion operator that reflects the combined information from both modalities. We show the utility of this joint operator in data denoising, visualization and clustering, performing better than other methods to integrate and analyze multimodal data. We apply our method to multi-omic data generated from blood cells, measuring both gene expression and chromatin accessibility. Our approach better visualizes the geometry of the joint data, captures known cross-modality associations and identifies known cellular populations. More generally, integrated diffusion is broadly applicable to multimodal datasets generated in many medical and biological systems.",
      "authors": "Manik Kuchroo; Abhinav Godavarthi; Alexander Tong; Guy Wolf; Smita Krishnaswamy",
      "date": "2021-02-12",
      "year": "2021",
      "categories": [
        "cs.LG",
        "cs.HC"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2012.05995v4",
      "doi": "",
      "title": "Towards a robust out-of-the-box neural network model for genomic data",
      "abstract": "The accurate prediction of biological features from genomic data is paramount for precision medicine and sustainable agriculture. For decades, neural network models have been widely popular in fields like computer vision, astrophysics and targeted marketing given their prediction accuracy and their robust performance under big data settings. Yet neural network models have not made a successful transition into the medical and biological world due to the ubiquitous characteristics of biological data such as modest sample sizes, sparsity, and extreme heterogeneity. Here, we investigate the robustness, generalization potential and prediction accuracy of widely used convolutional neural network and natural language processing models with a variety of heterogeneous genomic datasets. Mainly, recurrent neural network models outperform convolutional neural network models in terms of prediction accuracy, overfitting and transferability across the datasets under study. While the perspective of a robust out-of-the-box neural network model is out of reach, we identify certain model characteristics that translate well across datasets and could serve as a baseline model for translational researchers.",
      "authors": "Zhaoyi Zhang; Songyang Cheng; Claudia Solis-Lemus",
      "date": "2020-12-10",
      "year": "2020",
      "categories": [
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2012.00933v3",
      "doi": "",
      "title": "Global and Individualized Community Detection in Inhomogeneous Multilayer Networks",
      "abstract": "In network applications, it has become increasingly common to obtain datasets in the form of multiple networks observed on the same set of subjects, where each network is obtained in a related but different experiment condition or application scenario. Such datasets can be modeled by multilayer networks where each layer is a separate network itself while different layers are associated and share some common information. The present paper studies community detection in a stylized yet informative inhomogeneous multilayer network model. In our model, layers are generated by different stochastic block models, the community structures of which are (random) perturbations of a common global structure while the connecting probabilities in different layers are not related. Focusing on the symmetric two block case, we establish minimax rates for both global estimation of the common structure and individualized estimation of layer-wise community structures. Both minimax rates have sharp exponents. In addition, we provide an efficient algorithm that is simultaneously asymptotic minimax optimal for both estimation tasks under mild conditions. The optimal rates depend on the parity of the number of most informative layers, a phenomenon that is caused by inhomogeneity across layers. The method is extended to handle multiple and potentially asymmetric community cases. We demonstrate its effectiveness on both simulated examples and a real multi-modal single-cell dataset.",
      "authors": "Shuxiao Chen; Sifan Liu; Zongming Ma",
      "date": "2020-12-02",
      "year": "2020",
      "categories": [
        "math.ST",
        "cs.IT",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2005.03994v1",
      "doi": "",
      "title": "Cell Type Identification from Single-Cell Transcriptomic Data via Semi-supervised Learning",
      "abstract": "Cell type identification from single-cell transcriptomic data is a common goal of single-cell RNA sequencing (scRNAseq) data analysis. Neural networks have been employed to identify cell types from scRNAseq data with high performance. However, it requires a large mount of individual cells with accurate and unbiased annotated types to build the identification models. Unfortunately, labeling the scRNAseq data is cumbersome and time-consuming as it involves manual inspection of marker genes. To overcome this challenge, we propose a semi-supervised learning model to use unlabeled scRNAseq cells and limited amount of labeled scRNAseq cells to implement cell identification. Firstly, we transform the scRNAseq cells to \"gene sentences\", which is inspired by similarities between natural language system and gene system. Then genes in these sentences are represented as gene embeddings to reduce data sparsity. With these embeddings, we implement a semi-supervised learning model based on recurrent convolutional neural networks (RCNN), which includes a shared network, a supervised network and an unsupervised network. The proposed model is evaluated on macosko2015, a large scale single-cell transcriptomic dataset with ground truth of individual cell types. It is observed that the proposed model is able to achieve encouraging performance by learning on very limited amount of labeled scRNAseq cells together with a large number of unlabeled scRNAseq cells.",
      "authors": "Xishuang Dong; Shanta Chowdhury; Uboho Victor; Xiangfang Li; Lijun Qian",
      "date": "2020-05-06",
      "year": "2020",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2003.12773v1",
      "doi": "",
      "title": "Knowledge synthesis from 100 million biomedical documents augments the deep expression profiling of coronavirus receptors",
      "abstract": "The COVID-19 pandemic demands assimilation of all available biomedical knowledge to decode its mechanisms of pathogenicity and transmission. Despite the recent renaissance in unsupervised neural networks for decoding unstructured natural languages, a platform for the real-time synthesis of the exponentially growing biomedical literature and its comprehensive triangulation with deep omic insights is not available. Here, we present the nferX platform for dynamic inference from over 45 quadrillion possible conceptual associations extracted from unstructured biomedical text, and their triangulation with Single Cell RNA-sequencing based insights from over 25 tissues. Using this platform, we identify intersections between the pathologic manifestations of COVID-19 and the comprehensive expression profile of the SARS-CoV-2 receptor ACE2. We find that tongue keratinocytes and olfactory epithelial cells are likely under-appreciated targets of SARS-CoV-2 infection, correlating with reported loss of sense of taste and smell as early indicators of COVID-19 infection, including in otherwise asymptomatic patients. Airway club cells, ciliated cells and type II pneumocytes in the lung, and enterocytes of the gut also express ACE2. This study demonstrates how a holistic data science platform can leverage unprecedented quantities of structured and unstructured publicly available data to accelerate the generation of impactful biological insights and hypotheses.",
      "authors": "AJ Venkatakrishnan; Arjun Puranik; Akash Anand; David Zemmour; Xiang Yao; Xiaoying Wu; Ramakrishna Chilaka; Dariusz K. Murakowski; Kristopher Standish; Bharathwaj Raghunathan; Tyler Wagner; Enrique Garcia-Rivera; Hugo Solomon; Abhinav Garg; Rakesh Barve; Anuli Anyanwu-Ofili; Najat Khan; Venky Soundararajan",
      "date": "2020-03-28",
      "year": "2020",
      "categories": [
        "q-bio.GN",
        "cs.IR",
        "q-bio.BM",
        "q-bio.TO"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "1910.07763v3",
      "doi": "",
      "title": "Mixture-of-Experts Variational Autoencoder for Clustering and Generating from Similarity-Based Representations on Single Cell Data",
      "abstract": "Clustering high-dimensional data, such as images or biological measurements, is a long-standingproblem and has been studied extensively. Recently, Deep Clustering has gained popularity due toits flexibility in fitting the specific peculiarities of complex data. Here we introduce the Mixture-of-Experts Similarity Variational Autoencoder (MoE-Sim-VAE), a novel generative clustering model.The model can learn multi-modal distributions of high-dimensional data and use these to generaterealistic data with high efficacy and efficiency. MoE-Sim-VAE is based on a Variational Autoencoder(VAE), where the decoder consists of a Mixture-of-Experts (MoE) architecture. This specific architecture allows for various modes of the data to be automatically learned by means of the experts.Additionally, we encourage the lower dimensional latent representation of our model to follow aGaussian mixture distribution and to accurately represent the similarities between the data points. Weassess the performance of our model on the MNIST benchmark data set and challenging real-worldtasks of clustering mouse organs from single-cell RNA-sequencing measurements and defining cellsubpopulations from mass cytometry (CyTOF) measurements on hundreds of different datasets.MoE-Sim-VAE exhibits superior clustering performance on all these tasks in comparison to thebaselines as well as competitor methods.",
      "authors": "Andreas Kopf; Vincent Fortuin; Vignesh Ram Somnath; Manfred Claassen",
      "date": "2019-10-17",
      "year": "2019",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "1906.05226v1",
      "doi": "",
      "title": "Continual and Multi-Task Architecture Search",
      "abstract": "Architecture search is the process of automatically learning the neural model or cell structure that best suits the given task. Recently, this approach has shown promising performance improvements (on language modeling and image classification) with reasonable training speed, using a weight sharing strategy called Efficient Neural Architecture Search (ENAS). In our work, we first introduce a novel continual architecture search (CAS) approach, so as to continually evolve the model parameters during the sequential training of several tasks, without losing performance on previously learned tasks (via block-sparsity and orthogonality constraints), thus enabling life-long learning. Next, we explore a multi-task architecture search (MAS) approach over ENAS for finding a unified, single cell structure that performs well across multiple tasks (via joint controller rewards), and hence allows more generalizable transfer of the cell structure knowledge to an unseen new task. We empirically show the effectiveness of our sequential continual learning and parallel multi-task learning based architecture search approaches on diverse sentence-pair classification tasks (GLUE) and multimodal-generation based video captioning tasks. Further, we present several ablations and analyses on the learned cell structures.",
      "authors": "Ramakanth Pasunuru; Mohit Bansal",
      "date": "2019-06-12",
      "year": "2019",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "1903.10536v2",
      "doi": "https://doi.org/10.1371/journal.pone.0224446",
      "title": "Gene Expression based Survival Prediction for Cancer Patients: A Topic Modeling Approach",
      "abstract": "Cancer is one of the leading cause of death, worldwide. Many believe that genomic data will enable us to better predict the survival time of these patients, which will lead to better, more personalized treatment options and patient care. As standard survival prediction models have a hard time coping with the high-dimensionality of such gene expression (GE) data, many projects use some dimensionality reduction techniques to overcome this hurdle. We introduce a novel methodology, inspired by topic modeling from the natural language domain, to derive expressive features from the high-dimensional GE data. There, a document is represented as a mixture over a relatively small number of topics, where each topic corresponds to a distribution over the words; here, to accommodate the heterogeneity of a patient's cancer, we represent each patient (~document) as a mixture over cancer-topics, where each cancer-topic is a mixture over GE values (~words). This required some extensions to the standard LDA model eg: to accommodate the \"real-valued\" expression values - leading to our novel \"discretized\" Latent Dirichlet Allocation (dLDA) procedure. We initially focus on the METABRIC dataset, which describes breast cancer patients using the r=49,576 GE values, from microarrays. Our results show that our approach provides survival estimates that are more accurate than standard models, in terms of the standard Concordance measure. We then validate this approach by running it on the Pan-kidney (KIPAN) dataset, over r=15,529 GE values - here using the mRNAseq modality - and find that it again achieves excellent results. In both cases, we also show that the resulting model is calibrated, using the recent \"D-calibrated\" measure. These successes, in two different cancer types and expression modalities, demonstrates the generality, and the effectiveness, of this approach.",
      "authors": "Luke Kumar; Russell Greiner",
      "date": "2019-03-25",
      "year": "2019",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "1811.03377v1",
      "doi": "",
      "title": "Spectral Simplicial Theory for Feature Selection and Applications to Genomics",
      "abstract": "The scale and complexity of modern data sets and the limitations associated with testing large numbers of hypotheses underline the need for feature selection methods. Spectral techniques rank features according to their degree of consistency with an underlying metric structure, but their current graph-based formulation restricts their applicability to point features. We extend spectral methods for feature selection to abstract simplicial complexes and present a general framework which can be applied to 2-point and higher-order features. Combinatorial Laplacian scores take into account the topology spanned by the data and reduce to the ordinary Laplacian score in the case of point features. We demonstrate the utility of spectral simplicial methods for feature selection with several examples of application to the analysis of gene expression and multi-modal genomic data. Our results provide a unifying perspective on topological data analysis and manifold learning approaches.",
      "authors": "Kiya W. Govek; Venkata S. Yamajala; Pablo G. Camara",
      "date": "2018-11-08",
      "year": "2018",
      "categories": [
        "stat.ML",
        "cs.LG",
        "q-bio.QM"
      ],
      "source": "arxiv",
      "found_by_query": "primary"
    },
    {
      "arxiv_id": "2601.10581v1",
      "doi": "",
      "title": "From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA",
      "abstract": "Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.",
      "authors": "Kimia Abedini; Farzad Shami; Gianmaria Silvello",
      "date": "2026-01-15",
      "year": "2026",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "source": "arxiv",
      "found_by_query": "model_names"
    },
    {
      "arxiv_id": "2405.06708v5",
      "doi": "",
      "title": "LangCell: Language-Cell Pre-training for Cell Identity Understanding",
      "abstract": "Cell identity encompasses various semantic aspects of a cell, including cell type, pathway information, disease information, and more, which are essential for biologists to gain insights into its biological characteristics. Understanding cell identity from the transcriptomic data, such as annotating cell types, has become an important task in bioinformatics. As these semantic aspects are determined by human experts, it is impossible for AI models to effectively carry out cell identity understanding tasks without the supervision signals provided by single-cell and label pairs. The single-cell pre-trained language models (PLMs) currently used for this task are trained only on a single modality, transcriptomics data, lack an understanding of cell identity knowledge. As a result, they have to be fine-tuned for downstream tasks and struggle when lacking labeled data with the desired semantic labels. To address this issue, we propose an innovative solution by constructing a unified representation of single-cell data and natural language during the pre-training phase, allowing the model to directly incorporate insights related to cell identity. More specifically, we introduce $\\textbf{LangCell}$, the first $\\textbf{Lang}$uage-$\\textbf{Cell}$ pre-training framework. LangCell utilizes texts enriched with cell identity information to gain a profound comprehension of cross-modal knowledge. Results from experiments conducted on different benchmarks show that LangCell is the only single-cell PLM that can work effectively in zero-shot cell identity understanding scenarios, and also significantly outperforms existing models in few-shot and fine-tuning cell identity understanding scenarios.",
      "authors": "Suyuan Zhao; Jiahuan Zhang; Yushuai Wu; Yizhen Luo; Zaiqing Nie",
      "date": "2024-05-09",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "cs.CL"
      ],
      "source": "arxiv",
      "found_by_query": "model_names"
    },
    {
      "arxiv_id": "2403.03459v1",
      "doi": "",
      "title": "TGPT-PINN: Nonlinear model reduction with transformed GPT-PINNs",
      "abstract": "We introduce the Transformed Generative Pre-Trained Physics-Informed Neural Networks (TGPT-PINN) for accomplishing nonlinear model order reduction (MOR) of transport-dominated partial differential equations in an MOR-integrating PINNs framework. Building on the recent development of the GPT-PINN that is a network-of-networks design achieving snapshot-based model reduction, we design and test a novel paradigm for nonlinear model reduction that can effectively tackle problems with parameter-dependent discontinuities. Through incorporation of a shock-capturing loss function component as well as a parameter-dependent transform layer, the TGPT-PINN overcomes the limitations of linear model reduction in the transport-dominated regime. We demonstrate this new capability for nonlinear model reduction in the PINNs framework by several nontrivial parametric partial differential equations.",
      "authors": "Yanlai Chen; Yajie Ji; Akil Narayan; Zhenli Xu",
      "date": "2024-03-06",
      "year": "2024",
      "categories": [
        "math.NA",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "model_names"
    },
    {
      "arxiv_id": "2304.09667v3",
      "doi": "https://doi.org/10.1093/bioinformatics/btae075",
      "title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
      "abstract": "While large language models (LLMs) have been successfully applied to various tasks, they still face challenges with hallucinations. Augmenting LLMs with domain-specific tools such as database utilities can facilitate easier and more precise access to specialized knowledge. In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions. Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs by in-context learning and an augmented decoding algorithm that can detect and execute API calls. Experimental results show that GeneGPT achieves state-of-the-art performance on eight tasks in the GeneTuring benchmark with an average score of 0.83, largely surpassing retrieval-augmented LLMs such as the new Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as well as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1) API demonstrations have good cross-task generalizability and are more useful than documentations for in-context learning; (2) GeneGPT can generalize to longer chains of API calls and answer multi-hop questions in GeneHop, a novel dataset introduced in this work; (3) Different types of errors are enriched in different tasks, providing valuable insights for future improvements.",
      "authors": "Qiao Jin; Yifan Yang; Qingyu Chen; Zhiyong Lu",
      "date": "2023-04-19",
      "year": "2023",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "model_names"
    },
    {
      "arxiv_id": "2511.05810v2",
      "doi": "",
      "title": "DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis",
      "abstract": "Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \\texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.",
      "authors": "Bowen Xu; Xinyue Zeng; Jiazhen Hu; Tuo Wang; Adithya Kulkarni",
      "date": "2025-11-08",
      "year": "2025",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "bio_llm"
    },
    {
      "arxiv_id": "2508.08331v2",
      "doi": "",
      "title": "miRKatAI: An Integrated Database and Multi-agent AI system for microRNA Research",
      "abstract": "MicroRNAs (miRs) are robust regulators of gene expression, implicated in most biological processes. microRNAs predominantly downregulate the expression of genes post-transcriptionally and each miR is predicted to target several hundred genes. The accurate identification and annotation of miR-mRNA target interactions is central to understanding miRs function and their therapeutic potential. However, computational target prediction is challenging due to imperfect complementarity of miRs with their targets and the growing volume and heterogeneity of experimental data present challenges in accessing, integrating, and analysing miR-target interaction information across biological contexts. This creates a need for integrated resources and intelligent query tools.   We present the miRKat Suite, comprising miRKatDB, a comprehensive, curated database of predicted and validated miR-target interactions and associated annotations, and miRKatAI, a multi-agent system powered by large language models (LLMs) and LangGraph. miRKatDB integrates data from multiple publicly available sources, providing a comprehensive foundation for miR studies, including miR target genes and changes in levels of tissue expression previously reported. miRKatAI offers a natural language interface for complex querying of miRKatDB, facilitates grounded information retrieval from established sources in the field, and supports basic data visualisation. The miRKat Suite aims to accelerate miR research by streamlining data access, enhancing exploratory analysis, and supporting hypothesis generation.",
      "authors": "Karen Guerrero-Vazquez; Jacopo Umberto Verga; Pilib O Broin; Eva Szegezdi; Katarzyna Goljanek-Whysall",
      "date": "2025-08-10",
      "year": "2025",
      "categories": [
        "q-bio.GN",
        "cs.MA"
      ],
      "source": "arxiv",
      "found_by_query": "bio_llm"
    },
    {
      "arxiv_id": "2505.17257v4",
      "doi": "",
      "title": "JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model",
      "abstract": "Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: https://github.com/Qihao-Duan/JanusDNA",
      "authors": "Qihao Duan; Bingding Huang; Zhenqiao Song; Irina Lehmann; Lei Gu; Roland Eils; Benjamin Wild",
      "date": "2025-05-22",
      "year": "2025",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "source": "arxiv",
      "found_by_query": "bio_llm"
    },
    {
      "arxiv_id": "2505.15747v2",
      "doi": "https://doi.org/10.1109/ACCESS.2025.3582853",
      "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs",
      "abstract": "We propose a novel framework for integrating fragmented multi-modal data in Alzheimer's disease (AD) research using large language models (LLMs) and knowledge graphs. While traditional multimodal analysis requires matched patient IDs across datasets, our approach demonstrates population-level integration of MRI, gene expression, biomarkers, EEG, and clinical indicators from independent cohorts. Statistical analysis identified significant features in each modality, which were connected as nodes in a knowledge graph. LLMs then analyzed the graph to extract potential correlations and generate hypotheses in natural language. This approach revealed several novel relationships, including a potential pathway linking metabolic risk factors to tau protein abnormalities via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between frontal EEG channels and specific gene expression profiles (r=0.42-0.58, p<0.01). Cross-validation with independent datasets confirmed the robustness of major findings, with consistent effect sizes across cohorts (variance <15%). The reproducibility of these findings was further supported by expert review (Cohen's k=0.82) and computational validation. Our framework enables cross modal integration at a conceptual level without requiring patient ID matching, offering new possibilities for understanding AD pathology through fragmented data reuse and generating testable hypotheses for future research.",
      "authors": "Kanan Kiguchi; Yunhao Tu; Katsuhiro Ajito; Fady Alnajjar; Kazuyuki Murase",
      "date": "2025-05-21",
      "year": "2025",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "bio_llm"
    },
    {
      "arxiv_id": "2409.13537v1",
      "doi": "",
      "title": "ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources",
      "abstract": "Recent developments in large language models (LLMs) have led to significant improvements in intelligent dialogue systems'ability to handle complex inquiries. However, current LLMs still exhibit limitations in specialized domain knowledge, particularly in technical fields such as agriculture. To address this problem, we propose ShizishanGPT, an intelligent question answering system for agriculture based on the Retrieval Augmented Generation (RAG) framework and agent architecture. ShizishanGPT consists of five key modules: including a generic GPT-4 based module for answering general questions; a search engine module that compensates for the problem that the large language model's own knowledge cannot be updated in a timely manner; an agricultural knowledge graph module for providing domain facts; a retrieval module which uses RAG to supplement domain knowledge; and an agricultural agent module, which invokes specialized models for crop phenotype prediction, gene expression analysis, and so on. We evaluated the ShizishanGPT using a dataset containing 100 agricultural questions specially designed for this study. The experimental results show that the tool significantly outperforms general LLMs as it provides more accurate and detailed answers due to its modular design and integration of different domain knowledge sources. Our source code, dataset, and model weights are publicly available at https://github.com/Zaiwen/CropGPT.",
      "authors": "Shuting Yang; Zehui Liu; Wolfgang Mayer",
      "date": "2024-09-20",
      "year": "2024",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "source": "arxiv",
      "found_by_query": "bio_llm"
    },
    {
      "arxiv_id": "2405.03726v1",
      "doi": "",
      "title": "sc-OTGM: Single-Cell Perturbation Modeling by Solving Optimal Mass Transport on the Manifold of Gaussian Mixtures",
      "abstract": "Influenced by breakthroughs in LLMs, single-cell foundation models are emerging. While these models show successful performance in cell type clustering, phenotype classification, and gene perturbation response prediction, it remains to be seen if a simpler model could achieve comparable or better results, especially with limited data. This is important, as the quantity and quality of single-cell data typically fall short of the standards in textual data used for training LLMs. Single-cell sequencing often suffers from technical artifacts, dropout events, and batch effects. These challenges are compounded in a weakly supervised setting, where the labels of cell states can be noisy, further complicating the analysis. To tackle these challenges, we present sc-OTGM, streamlined with less than 500K parameters, making it approximately 100x more compact than the foundation models, offering an efficient alternative. sc-OTGM is an unsupervised model grounded in the inductive bias that the scRNAseq data can be generated from a combination of the finite multivariate Gaussian distributions. The core function of sc-OTGM is to create a probabilistic latent space utilizing a GMM as its prior distribution and distinguish between distinct cell populations by learning their respective marginal PDFs. It uses a Hit-and-Run Markov chain sampler to determine the OT plan across these PDFs within the GMM framework. We evaluated our model against a CRISPR-mediated perturbation dataset, called CROP-seq, consisting of 57 one-gene perturbations. Our results demonstrate that sc-OTGM is effective in cell state classification, aids in the analysis of differential gene expression, and ranks genes for target identification through a recommender system. It also predicts the effects of single-gene perturbations on downstream gene regulation and generates synthetic scRNA-seq data conditioned on specific cell states.",
      "authors": "Andac Demir; Elizaveta Solovyeva; James Boylan; Mei Xiao; Fabrizio Serluca; Sebastian Hoersch; Jeremy Jenkins; Murthy Devarakonda; Bulent Kiziltan",
      "date": "2024-05-06",
      "year": "2024",
      "categories": [
        "q-bio.GN",
        "cs.LG"
      ],
      "source": "arxiv",
      "found_by_query": "bio_llm"
    }
  ]
}